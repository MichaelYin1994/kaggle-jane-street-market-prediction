{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "e14d75e190cad09067b6ef4ade1f47ca7ff568b5a764ea50d7cf40d3762ef09d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "from numba import njit, jit\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import iinfo, finfo, int8, int16, int32, int64, float32, float64\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import activations\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from IPython.display import HTML\n",
    "\n",
    "# 设置plotly为暗黑模式\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "plot_config = dict({'scrollZoom': True, 'displayModeBar': True, 'displaylogo': False})\n",
    "sns.set(style=\"ticks\", font_scale=1.2, palette='deep', color_codes=True)\n",
    "colors = [\"C\" + str(i) for i in range(0, 9+1)]\n",
    "\n",
    "# 默认plotly色号\n",
    "default_color_list = [\n",
    "    '#1f77b4',  # muted blue\n",
    "    '#ff7f0e',  # safety orange\n",
    "    '#2ca02c',  # cooked asparagus green\n",
    "    '#d62728',  # brick red\n",
    "    '#9467bd',  # muted purple\n",
    "    '#8c564b',  # chestnut brown\n",
    "    '#e377c2',  # raspberry yogurt pink\n",
    "    '#7f7f7f',  # middle gray\n",
    "    '#bcbd22',  # curry yellow-green\n",
    "    '#17becf'   # blue-teal\n",
    "    ]\n",
    "\n",
    "# 设定全局随机种子，并且屏蔽warnings\n",
    "GLOBAL_RANDOM_SEED = 2022\n",
    "np.random.seed(GLOBAL_RANDOM_SEED)\n",
    "tf.random.set_seed(GLOBAL_RANDOM_SEED)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] 2021-01-04 21:24:46.25 End Reading ! It took 53.74 seconds !\n[INFO] 2021-01-04 21:24:46.25 Basic data description: \n    -- train_df shape: (2390491, 138)\n    -- example_test_df shape: (15219, 133)\n    -- feat_df shape: (130, 30)\n    -- example_prediction_df shape: (15219, 2)\n"
     ]
    }
   ],
   "source": [
    "# 导入数据\n",
    "load_data_start_time = time.time()\n",
    "train_df  = pd.read_csv(\n",
    "    './data/jane-street-market-prediction/train.csv', nrows=None)\n",
    "feat_df = pd.read_csv(\n",
    "    './data/jane-street-market-prediction/features.csv')\n",
    "example_test_df = pd.read_csv(\n",
    "    './data/jane-street-market-prediction/example_test.csv')\n",
    "example_prediction_df = pd.read_csv(\n",
    "    './data/jane-street-market-prediction/example_sample_submission.csv')\n",
    "load_data_end_time = time.time()\n",
    "\n",
    "# 打印数据基本情况\n",
    "print(\"[INFO] {} End Reading ! It took {:.2f} seconds !\".format(\n",
    "    str(datetime.now())[:-4], load_data_end_time-load_data_start_time))\n",
    "print(\"[INFO] {} Basic data description: \".format(str(datetime.now())[:-4]))\n",
    "print(\"    -- train_df shape: {}\".format(\n",
    "    train_df.shape))\n",
    "print(\"    -- example_test_df shape: {}\".format(\n",
    "    example_test_df.shape))\n",
    "print(\"    -- feat_df shape: {}\".format(\n",
    "    feat_df.shape))\n",
    "print(\"    -- example_prediction_df shape: {}\".format(\n",
    "    example_prediction_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] 2021-01-04 21:24:49.06 Autoencoder data prepared !\n"
     ]
    }
   ],
   "source": [
    "# 挑选策略变化之后的数据\n",
    "train = train_df.query('date > 85').reset_index(drop=True)\n",
    "\n",
    "# 构造标签\n",
    "train = train.query('weight > 0').reset_index(drop = True)\n",
    "train['action'] =  ((train['resp_1'] > 0.00001) & \\\n",
    "                    (train['resp_2'] > 0.00001 ) & \\\n",
    "                    (train['resp_3'] > 0.00001) & \\\n",
    "                    (train['resp_4'] > 0.00001 ) &  \\\n",
    "                    (train['resp'] > 0.00001)).astype('int')\n",
    "feature_name_list = [c for c in train.columns if 'feature' in c]\n",
    "resp_name_list = [\"resp\", \"resp_1\", \"resp_2\", \"resp_3\", \"resp_4\"]\n",
    "\n",
    "# 使用均值填充缺失值\n",
    "mean_val_list = []\n",
    "for name in feature_name_list:\n",
    "    mean_val = train[name].mean()\n",
    "    train[name].fillna(mean_val, inplace=True)\n",
    "    mean_val_list.append(mean_val)\n",
    "mean_val_array = np.array(mean_val_list)\n",
    "\n",
    "# 构造自编码器的输入输出\n",
    "X = train[feature_name_list].values\n",
    "y = np.stack([(train[c] > 0.000001).astype('int') for c in resp_name_list]).T\n",
    "y_resp = train[resp_name_list].values\n",
    "\n",
    "train_dates = train[\"date\"].values\n",
    "train_weights = train[\"weight\"].values\n",
    "train_resp = train[\"resp\"].values\n",
    "\n",
    "print(\"[INFO] {} Autoencoder data prepared !\".format(\n",
    "    str(datetime.now())[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_test_data(test_df=None, pred_df=None):\n",
    "    \"\"\"测试数据生成器。用于模拟测试数据生成过程，测试模型提交正确性与效率。\"\"\"\n",
    "    n_test = len(test_df)\n",
    "\n",
    "    for i in range(n_test):\n",
    "        yield test_df.iloc[i], pred_df.iloc[i]\n",
    "\n",
    "\n",
    "@jit\n",
    "def njit_fillna(array, values):\n",
    "    \"\"\"利用即时编译（jit）对array数组的NaN值借助values进行填充。\n",
    "\n",
    "    @References:\n",
    "    ----------\n",
    "    [1] https://www.kaggle.com/gogo827jz/optimise-speed-of-filling-nan-function\n",
    "    \"\"\"\n",
    "    if np.isnan(array.sum()):\n",
    "        array = np.where(np.isnan(array), values, array)\n",
    "    return array\n",
    "\n",
    "\n",
    "def custom_metric(dates_array=None,\n",
    "                  weights_array=None,\n",
    "                  resp_array=None,\n",
    "                  action_label_array=None):\n",
    "    \"\"\"依据官方要求的Metric，计算分数。\n",
    "\n",
    "    @References:\n",
    "    ----------\n",
    "    [1] https://www.kaggle.com/c/jane-street-market-prediction/discussion/199107\n",
    "    [2] https://www.kaggle.com/c/jane-street-market-prediction/overview/evaluation\n",
    "    [3] \n",
    "    \"\"\"\n",
    "    tmp_df = pd.DataFrame({\"date\": dates_array,\n",
    "                           \"weight\": weights_array,\n",
    "                           \"resp\": resp_array,\n",
    "                           \"action\": action_label_array})\n",
    "    tmp_df[\"p\"] = tmp_df[\"weight\"]  * tmp_df[\"resp\"] * tmp_df[\"action\"]\n",
    "    # tmp_df = tmp_df.query(\"weight != 0\").reset_index(drop=True)\n",
    "    p_i_val = tmp_df.groupby([\"date\"])[\"p\"].sum().values\n",
    "\n",
    "    n_dates = len(p_i_val)\n",
    "    t = np.sum(p_i_val) / np.sqrt(np.sum(p_i_val ** 2)) * (np.sqrt(250 / n_dates))\n",
    "    return min(max(t, 0), 6) * np.sum(p_i_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://medium.com/@micwurm/using-tensorflow-lite-to-speed-up-predictions-a3954886eb98\n",
    "\n",
    "class LiteModel:\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, model_path):\n",
    "        return LiteModel(tf.lite.Interpreter(model_path=model_path))\n",
    "\n",
    "    @classmethod\n",
    "    def from_keras_model(cls, kmodel):\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(kmodel)\n",
    "        tflite_model = converter.convert()\n",
    "        return LiteModel(tf.lite.Interpreter(model_content=tflite_model))\n",
    "\n",
    "    def __init__(self, interpreter):\n",
    "        self.interpreter = interpreter\n",
    "        self.interpreter.allocate_tensors()\n",
    "        input_det = self.interpreter.get_input_details()[0]\n",
    "        output_det = self.interpreter.get_output_details()[0]\n",
    "        self.input_index = input_det[\"index\"]\n",
    "        self.output_index = output_det[\"index\"]\n",
    "        self.input_shape = input_det[\"shape\"]\n",
    "        self.output_shape = output_det[\"shape\"]\n",
    "        self.input_dtype = input_det[\"dtype\"]\n",
    "        self.output_dtype = output_det[\"dtype\"]\n",
    "\n",
    "    def predict(self, inp):\n",
    "        inp = inp.astype(self.input_dtype)\n",
    "        count = inp.shape[0]\n",
    "        out = np.zeros((count, self.output_shape[1]), dtype=self.output_dtype)\n",
    "        for i in range(count):\n",
    "            self.interpreter.set_tensor(self.input_index, inp[i:i+1])\n",
    "            self.interpreter.invoke()\n",
    "            out[i] = self.interpreter.get_tensor(self.output_index)[0]\n",
    "        return out\n",
    "\n",
    "    def predict_single(self, inp):\n",
    "        \"\"\" Like predict(), but only for a single record. The input data can be a Python list. \"\"\"\n",
    "        inp = np.array([inp], dtype=self.input_dtype)\n",
    "        self.interpreter.set_tensor(self.input_index, inp)\n",
    "        self.interpreter.invoke()\n",
    "        out = self.interpreter.get_tensor(self.output_index)\n",
    "        return out[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"针对带有Group id（组id）数据的时间序列交叉验证集合生成类。\n",
    "\n",
    "    生成针对带有Group id的数据的时序交叉验证集。其中训练与验证的\n",
    "    Group之间可以指定group_gap，用来隔离时间上的关系。这种情况下\n",
    "    group_id通常是时间id，例如天或者小时。\n",
    "\n",
    "    @Parameters:\n",
    "    ----------\n",
    "        n_splits: {int-like}, default=5\n",
    "            切分的集合数目。\n",
    "        max_train_group_size: {int-like}, default=+inf\n",
    "            训练集单个组的最大样本数据限制。\n",
    "        group_gap: {int-like}, default=None\n",
    "            依据group_id切分组时，训练组与测试组的id的gap数目。\n",
    "        max_test_group_size: {int-like}, default=+inf\n",
    "            测试集单个组的最大样本数据限制。\n",
    "\n",
    "    @References:\n",
    "    ----------\n",
    "    [1] https://www.kaggle.com/gogo827jz/jane-street-ffill-xgboost-purgedtimeseriescv\n",
    "    \"\"\"\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self, n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"生成训练组与测试组的id索引，返回组索引的生成器。\n",
    "\n",
    "        @Parameters:\n",
    "        ----------\n",
    "            X: {array-like} {n_samples, n_features}\n",
    "                训练数据，输入形状为{n_samples, n_features}。\n",
    "            y: {array-like} {n_samples, }\n",
    "                标签数据，形状为{n_samples, }。\n",
    "            groups: {array-like} {n_samples, }\n",
    "                用来依据组来划分训练集与测试集的组id，必须为连续的组id。\n",
    "\n",
    "        @Yields:\n",
    "        ----------\n",
    "            train: ndarray\n",
    "                依据group_id切分的训练组id。\n",
    "            test: ndarray\n",
    "                依据group_id切分的测试组id。\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None ！\")\n",
    "\n",
    "        # 初始化基本参数信息\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples, n_splits, group_gap = _num_samples(X), self.n_splits, self.group_gap\n",
    "        n_folds = n_splits + 1\n",
    "\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = np.argsort(ind)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "\n",
    "        # 扫描整个数据id list，构建group_dcit，{group_id: 属于该group的样本的idx}\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds, n_groups))\n",
    "\n",
    "        # group_test_size: 每个fold预留的test group的大小\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array, test_array = [], []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "            test_array  = test_array[group_gap:]\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]\n",
    "\n",
    "\n",
    "def test_purged_group_time_series_split():\n",
    "    X = train_df.query('date > 85').reset_index(drop=True)[[\"ts_id\", \"feature_0\"]].values\n",
    "    y = train_df.query('date > 85').reset_index(drop=True)[\"resp\"].values\n",
    "    groups = train_df.query('date > 85').reset_index(drop=True)[\"date\"].values\n",
    "\n",
    "    group_ts_kfolds = PurgedGroupTimeSeriesSplit(\n",
    "        n_splits=4, group_gap=31, max_test_group_size=31)\n",
    "    train_idx, valid_idx = [], []\n",
    "    for train_idx_tmp, valid_idx_tmp in group_ts_kfolds.split(X=X, y=y, groups=groups):\n",
    "        train_idx.append(train_idx_tmp)\n",
    "        valid_idx.append(valid_idx_tmp)\n",
    "\n",
    "    print([[train_df.iloc[min(item)][\"date\"].astype(int),\n",
    "            train_df.iloc[max(item)][\"date\"].astype(int)] for item in train_idx])\n",
    "    print([[train_df.iloc[min(item)][\"date\"].astype(int),\n",
    "            train_df.iloc[max(item)][\"date\"].astype(int)] for item in valid_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] 2021-01-04 21:47:25.77 Build autoencoder successed !\n"
     ]
    }
   ],
   "source": [
    "def build_tabular_autoencoder(verbose=False, is_compile=True,\n",
    "                              stddev=0.05, **kwargs):\n",
    "    \"\"\"降噪自编码器实现。针对表格形式数据的降噪自编码器，噪声等级由高斯噪声的stddev参数指定\"\"\"\n",
    "    input_dim = kwargs.pop(\"input_dim\", None)\n",
    "    n_labels = kwargs.pop(\"n_labels\", None)\n",
    "    n_regs = kwargs.pop(\"n_regs\", None)\n",
    "\n",
    "    # 构建降噪自编码器\n",
    "    layer_input = layers.Input(input_dim, dtype='float32')\n",
    "\n",
    "    layer_encoded = layers.BatchNormalization()(layer_input)\n",
    "    layer_encoded = layers.GaussianNoise(stddev=stddev)(layer_encoded)\n",
    "    layer_encoded = layers.Dense(256, activation='relu')(layer_encoded)\n",
    "\n",
    "    # 解码层1：针对输入的重构\n",
    "    layer_decoded = layers.Dropout(0.2)(layer_encoded)\n",
    "    layer_decoded = layers.Dense(input_dim, name='reconstruct_output')(layer_decoded)\n",
    "\n",
    "    # 解码层2：针对resp的重构\n",
    "    layer_output = layers.Dense(128, activation='relu')(layer_decoded)\n",
    "    layer_output = layers.BatchNormalization()(layer_output)\n",
    "    layer_output = layers.Dropout(0.2)(layer_output)\n",
    "\n",
    "    layer_output_labels = layers.Dense(n_labels, activation='sigmoid', name='layer_output_labels')(layer_output)\n",
    "    layer_output_regs = layers.Dense(n_regs, activation='tanh', name='layer_output_regs')(layer_output)\n",
    "\n",
    "    # 输出层\n",
    "    encoder_model = models.Model(inputs=layer_input, outputs=layer_decoded)\n",
    "    autoencoder_model = models.Model(inputs=layer_input, outputs=[layer_decoded, layer_output_labels, layer_output_regs])\n",
    "\n",
    "    if verbose:\n",
    "        autoencoder_model.summary()\n",
    "    if is_compile:\n",
    "        autoencoder_model.compile(loss={'reconstruct_output':'mae', 'layer_output_labels':'binary_crossentropy', 'layer_output_regs': 'mae'},\n",
    "                                  metrics={'layer_output_labels':'acc'}, \n",
    "                                  optimizer=optimizers.Adam(0.003))\n",
    "    return encoder_model, autoencoder_model\n",
    "\n",
    "encoder_model, autoencoder_model = build_tabular_autoencoder(\n",
    "    input_dim=X.shape[1], n_labels=y.shape[1], n_regs=y_resp.shape[1], stddev=0.03, verbose=False)\n",
    "\n",
    "print(\"[INFO] {} Build autoencoder successed !\".format(\n",
    "    str(datetime.now())[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 1414273 samples, validate on 157142 samples\n",
      "Epoch 1/20\n",
      "1414273/1414273 - 10s - loss: 1.9219 - reconstruct_output_loss: 0.9661 - layer_output_labels_loss: 0.7316 - layer_output_regs_loss: 0.2150 - layer_output_labels_acc: 0.5057 - val_loss: 1.4680 - val_reconstruct_output_loss: 0.6985 - val_layer_output_labels_loss: 0.6976 - val_layer_output_regs_loss: 0.0709 - val_layer_output_labels_acc: 0.5152\n",
      "Epoch 2/20\n",
      "1414273/1414273 - 10s - loss: 1.2978 - reconstruct_output_loss: 0.5531 - layer_output_labels_loss: 0.7014 - layer_output_regs_loss: 0.0418 - layer_output_labels_acc: 0.5114 - val_loss: 1.1297 - val_reconstruct_output_loss: 0.4189 - val_layer_output_labels_loss: 0.6914 - val_layer_output_regs_loss: 0.0188 - val_layer_output_labels_acc: 0.5218\n",
      "Epoch 3/20\n",
      "1414273/1414273 - 9s - loss: 1.1598 - reconstruct_output_loss: 0.4464 - layer_output_labels_loss: 0.6959 - layer_output_regs_loss: 0.0173 - layer_output_labels_acc: 0.5173 - val_loss: 1.0097 - val_reconstruct_output_loss: 0.3030 - val_layer_output_labels_loss: 0.6899 - val_layer_output_regs_loss: 0.0165 - val_layer_output_labels_acc: 0.5276\n",
      "Epoch 4/20\n",
      "1414273/1414273 - 10s - loss: 1.1200 - reconstruct_output_loss: 0.4111 - layer_output_labels_loss: 0.6932 - layer_output_regs_loss: 0.0155 - layer_output_labels_acc: 0.5211 - val_loss: 0.9501 - val_reconstruct_output_loss: 0.2470 - val_layer_output_labels_loss: 0.6898 - val_layer_output_regs_loss: 0.0131 - val_layer_output_labels_acc: 0.5300\n",
      "Epoch 5/20\n",
      "1414273/1414273 - 10s - loss: 1.1010 - reconstruct_output_loss: 0.3961 - layer_output_labels_loss: 0.6915 - layer_output_regs_loss: 0.0134 - layer_output_labels_acc: 0.5243 - val_loss: 0.9429 - val_reconstruct_output_loss: 0.2409 - val_layer_output_labels_loss: 0.6895 - val_layer_output_regs_loss: 0.0123 - val_layer_output_labels_acc: 0.5311\n",
      "Epoch 6/20\n",
      "1414273/1414273 - 10s - loss: 1.0914 - reconstruct_output_loss: 0.3876 - layer_output_labels_loss: 0.6907 - layer_output_regs_loss: 0.0130 - layer_output_labels_acc: 0.5262 - val_loss: 0.9292 - val_reconstruct_output_loss: 0.2272 - val_layer_output_labels_loss: 0.6891 - val_layer_output_regs_loss: 0.0128 - val_layer_output_labels_acc: 0.5331\n",
      "Epoch 7/20\n",
      "1414273/1414273 - 10s - loss: 1.0834 - reconstruct_output_loss: 0.3808 - layer_output_labels_loss: 0.6903 - layer_output_regs_loss: 0.0123 - layer_output_labels_acc: 0.5278 - val_loss: 0.9239 - val_reconstruct_output_loss: 0.2228 - val_layer_output_labels_loss: 0.6892 - val_layer_output_regs_loss: 0.0118 - val_layer_output_labels_acc: 0.5335\n",
      "Epoch 8/20\n",
      "1414273/1414273 - 10s - loss: 1.0781 - reconstruct_output_loss: 0.3757 - layer_output_labels_loss: 0.6899 - layer_output_regs_loss: 0.0125 - layer_output_labels_acc: 0.5283 - val_loss: 0.9207 - val_reconstruct_output_loss: 0.2195 - val_layer_output_labels_loss: 0.6889 - val_layer_output_regs_loss: 0.0121 - val_layer_output_labels_acc: 0.5331\n",
      "Epoch 9/20\n",
      "1414273/1414273 - 10s - loss: 1.0719 - reconstruct_output_loss: 0.3698 - layer_output_labels_loss: 0.6898 - layer_output_regs_loss: 0.0123 - layer_output_labels_acc: 0.5295 - val_loss: 0.9178 - val_reconstruct_output_loss: 0.2170 - val_layer_output_labels_loss: 0.6886 - val_layer_output_regs_loss: 0.0120 - val_layer_output_labels_acc: 0.5340\n",
      "Epoch 10/20\n",
      "1414273/1414273 - 10s - loss: 1.0666 - reconstruct_output_loss: 0.3647 - layer_output_labels_loss: 0.6897 - layer_output_regs_loss: 0.0121 - layer_output_labels_acc: 0.5294 - val_loss: 0.9228 - val_reconstruct_output_loss: 0.2220 - val_layer_output_labels_loss: 0.6886 - val_layer_output_regs_loss: 0.0120 - val_layer_output_labels_acc: 0.5342\n",
      "Epoch 11/20\n",
      "1414273/1414273 - 10s - loss: 1.0609 - reconstruct_output_loss: 0.3593 - layer_output_labels_loss: 0.6896 - layer_output_regs_loss: 0.0121 - layer_output_labels_acc: 0.5297 - val_loss: 0.9135 - val_reconstruct_output_loss: 0.2129 - val_layer_output_labels_loss: 0.6885 - val_layer_output_regs_loss: 0.0119 - val_layer_output_labels_acc: 0.5345\n",
      "Epoch 12/20\n",
      "1414273/1414273 - 10s - loss: 1.0544 - reconstruct_output_loss: 0.3529 - layer_output_labels_loss: 0.6894 - layer_output_regs_loss: 0.0120 - layer_output_labels_acc: 0.5302 - val_loss: 0.9101 - val_reconstruct_output_loss: 0.2094 - val_layer_output_labels_loss: 0.6885 - val_layer_output_regs_loss: 0.0121 - val_layer_output_labels_acc: 0.5353\n",
      "Epoch 13/20\n",
      "1414273/1414273 - 10s - loss: 1.0484 - reconstruct_output_loss: 0.3468 - layer_output_labels_loss: 0.6894 - layer_output_regs_loss: 0.0121 - layer_output_labels_acc: 0.5304 - val_loss: 0.9083 - val_reconstruct_output_loss: 0.2076 - val_layer_output_labels_loss: 0.6883 - val_layer_output_regs_loss: 0.0121 - val_layer_output_labels_acc: 0.5354\n",
      "Epoch 14/20\n",
      "1414273/1414273 - 10s - loss: 1.0428 - reconstruct_output_loss: 0.3416 - layer_output_labels_loss: 0.6894 - layer_output_regs_loss: 0.0118 - layer_output_labels_acc: 0.5304 - val_loss: 0.9048 - val_reconstruct_output_loss: 0.2048 - val_layer_output_labels_loss: 0.6883 - val_layer_output_regs_loss: 0.0115 - val_layer_output_labels_acc: 0.5353\n",
      "Epoch 15/20\n",
      "1414273/1414273 - 10s - loss: 1.0372 - reconstruct_output_loss: 0.3361 - layer_output_labels_loss: 0.6893 - layer_output_regs_loss: 0.0116 - layer_output_labels_acc: 0.5306 - val_loss: 0.9054 - val_reconstruct_output_loss: 0.2055 - val_layer_output_labels_loss: 0.6882 - val_layer_output_regs_loss: 0.0115 - val_layer_output_labels_acc: 0.5359\n",
      "Epoch 16/20\n",
      "1414273/1414273 - 10s - loss: 1.0319 - reconstruct_output_loss: 0.3313 - layer_output_labels_loss: 0.6892 - layer_output_regs_loss: 0.0115 - layer_output_labels_acc: 0.5310 - val_loss: 0.8994 - val_reconstruct_output_loss: 0.1995 - val_layer_output_labels_loss: 0.6880 - val_layer_output_regs_loss: 0.0117 - val_layer_output_labels_acc: 0.5368\n",
      "Epoch 17/20\n",
      "1414273/1414273 - 10s - loss: 1.0280 - reconstruct_output_loss: 0.3274 - layer_output_labels_loss: 0.6892 - layer_output_regs_loss: 0.0115 - layer_output_labels_acc: 0.5313 - val_loss: 0.8931 - val_reconstruct_output_loss: 0.1929 - val_layer_output_labels_loss: 0.6883 - val_layer_output_regs_loss: 0.0116 - val_layer_output_labels_acc: 0.5350\n",
      "Epoch 18/20\n",
      "1414273/1414273 - 10s - loss: 1.0235 - reconstruct_output_loss: 0.3229 - layer_output_labels_loss: 0.6891 - layer_output_regs_loss: 0.0115 - layer_output_labels_acc: 0.5314 - val_loss: 0.8898 - val_reconstruct_output_loss: 0.1897 - val_layer_output_labels_loss: 0.6881 - val_layer_output_regs_loss: 0.0118 - val_layer_output_labels_acc: 0.5357\n",
      "Epoch 19/20\n",
      "1414273/1414273 - 10s - loss: 1.0205 - reconstruct_output_loss: 0.3199 - layer_output_labels_loss: 0.6891 - layer_output_regs_loss: 0.0116 - layer_output_labels_acc: 0.5319 - val_loss: 0.9051 - val_reconstruct_output_loss: 0.2050 - val_layer_output_labels_loss: 0.6881 - val_layer_output_regs_loss: 0.0118 - val_layer_output_labels_acc: 0.5354\n",
      "Epoch 20/20\n",
      "1414273/1414273 - 10s - loss: 1.0172 - reconstruct_output_loss: 0.3167 - layer_output_labels_loss: 0.6891 - layer_output_regs_loss: 0.0116 - layer_output_labels_acc: 0.5314 - val_loss: 0.8855 - val_reconstruct_output_loss: 0.1857 - val_layer_output_labels_loss: 0.6879 - val_layer_output_regs_loss: 0.0117 - val_layer_output_labels_acc: 0.5370\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5c3c4b87d0>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "early_stop = callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\",\n",
    "                                     verbose=1, patience=40,\n",
    "                                     restore_best_weights=True)\n",
    "autoencoder_model.fit(x=X, y=[X, y, y_resp],\n",
    "                      batch_size=32768,\n",
    "                      epochs=20,\n",
    "                      verbose=2,\n",
    "                      validation_split=0.1,\n",
    "                      callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] 2021-01-04 21:50:47.31 Build MLP Model successed !\n"
     ]
    }
   ],
   "source": [
    "def build_model(verbose=False, is_compile=True, encoder_model=None, **kwargs):\n",
    "    \"\"\"针对二分类任务的MLP模型，使用自编码器的编码层作为预训练层。\"\"\"\n",
    "    input_dim = kwargs.pop(\"input_dim\", None)\n",
    "    output_dim = kwargs.pop(\"output_dim\", None)\n",
    "    n_labels = kwargs.pop(\"n_labels\", None)\n",
    "\n",
    "    # 构造网络结构\n",
    "    layer_input = layers.Input(input_dim, dtype='float32')\n",
    "    layer_encoded = encoder_model(layer_input)\n",
    "\n",
    "    layer_feats = layers.concatenate([layer_input, layer_encoded])\n",
    "    layer_feats = layers.BatchNormalization()(layer_feats)\n",
    "\n",
    "    # 特征抽取\n",
    "    layer_dense = layers.Dense(64, activation=\"relu\")(layer_feats)\n",
    "    layer_dense = layers.BatchNormalization()(layer_dense)\n",
    "    layer_dense = layers.Dropout(0.2)(layer_dense)\n",
    "\n",
    "    layer_dense = layers.Dense(32, activation=\"relu\")(layer_dense)\n",
    "    layer_dense = layers.BatchNormalization()(layer_dense)\n",
    "    layer_dense = layers.Dropout(0.2)(layer_dense)\n",
    "\n",
    "    # 输出层构造与模型构造\n",
    "    layer_output = layers.Dense(output_dim, activation='sigmoid', name=\"label_output\")(layer_dense)\n",
    "    model = models.Model(layer_input, layer_output)\n",
    "\n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    if is_compile:\n",
    "        model.compile(loss={'label_output':'binary_crossentropy'},\n",
    "                      metrics=[tf.keras.metrics.AUC(name='auc')],\n",
    "                      optimizer=optimizers.Adam(0.003))\n",
    "    return model\n",
    "\n",
    "def test_build_model():\n",
    "    # 构造mlp模型\n",
    "    mlp_model = build_model(verbose=False, encoder_model=encoder_model,\n",
    "                            input_dim=X.shape[1], output_dim=y.shape[1])\n",
    "\n",
    "print(\"[INFO] {} Build MLP Model successed !\".format(\n",
    "    str(datetime.now())[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] 2021-01-04 22:08:33.62 Model training start:\n",
      "=========================================\n",
      "-- folds 0, valid_acc: 0.1643, valid_roc_auc: 0.5326, valid_custom: 1170.7429\n",
      "-- folds 1, valid_acc: 0.1961, valid_roc_auc: 0.5402, valid_custom: 598.1706\n",
      "-- folds 2, valid_acc: 0.2293, valid_roc_auc: 0.5401, valid_custom: 9.5025\n",
      "-- folds 3, valid_acc: 0.2423, valid_roc_auc: 0.5405, valid_custom: 900.2462\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_auc` which is not available. Available metrics are: loss,auc\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-ee149edba7b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                   \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                   callbacks=[early_stop])\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# valid预测结果\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 训练前全局准备\n",
    "THRESHOLD = 0.5\n",
    "MODELS = []\n",
    "encoder_model.trainable = True\n",
    "early_stop = callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\",\n",
    "                                     verbose=1, patience=30,\n",
    "                                     restore_best_weights=True)\n",
    "group_ts_kfolds = PurgedGroupTimeSeriesSplit(\n",
    "        n_splits=15, group_gap=20)\n",
    "min_date = min(train_dates)\n",
    "train_dates = train_dates - min_date\n",
    "\n",
    "# 开始训练模型\n",
    "valid_acc_total, valid_roc_auc_total, valid_custom_total = [], [], []\n",
    "\n",
    "print(\"[INFO] {} Model training start:\".format(str(datetime.now())[:-4]))\n",
    "print(\"=========================================\")\n",
    "for fold, (train_idx, valid_idx) in enumerate(group_ts_kfolds.split(X=X, y=y, groups=train_dates)):\n",
    "    X_train, X_val = X[train_idx], X[valid_idx]\n",
    "    y_train, y_val = y[train_idx], y[valid_idx]\n",
    "\n",
    "    X_train_weight, X_val_weight = train_weights[train_idx], train_weights[valid_idx]\n",
    "    X_train_resp, X_val_resp = train_resp[train_idx], train_resp[valid_idx]\n",
    "    X_train_dates, X_val_dates = train_dates[train_idx], train_dates[valid_idx]\n",
    "\n",
    "    # 准备模型\n",
    "    mlp_model = build_model(verbose=False, encoder_model=encoder_model,\n",
    "                            input_dim=X_train.shape[1], output_dim=y_train.shape[1])\n",
    "\n",
    "    mlp_model.fit(x=X_train, y=y_train,\n",
    "                  validation_data=(X_val, y_val),\n",
    "                  batch_size=32768,\n",
    "                  epochs=20,\n",
    "                  verbose=0,\n",
    "                  callbacks=[early_stop])\n",
    "\n",
    "    # valid预测结果\n",
    "    valid_pred_proba = mlp_model.predict(X_val)\n",
    "    valid_pred_label_mat = np.where(\n",
    "            valid_pred_proba>=THRESHOLD, 1, 0).astype(int)\n",
    "\n",
    "    # valid data上按照官方metric进行结果评估\n",
    "    valid_pred_label = np.where(\n",
    "            np.mean(valid_pred_proba, axis=1)>=THRESHOLD, 1, 0).astype(int)\n",
    "    valid_custom_metric = custom_metric(dates_array=X_val_dates,\n",
    "                                        weights_array=X_val_weight,\n",
    "                                        action_label_array=valid_pred_label,\n",
    "                                        resp_array=X_val_resp)\n",
    "\n",
    "    # Accuracy与ROC AUC进行评估\n",
    "    valid_acc = accuracy_score(valid_pred_label_mat, y_val)\n",
    "\n",
    "    valid_roc_auc_list = []\n",
    "    for i in range(y_val.shape[1]):\n",
    "        valid_pred_tmp = valid_pred_proba[:, i].reshape(-1, 1)\n",
    "        y_val_tmp = y_val[:, i].reshape(-1, 1)\n",
    "\n",
    "        valid_roc_auc_list.append(roc_auc_score(y_val_tmp, valid_pred_tmp))\n",
    "    valid_roc_auc = np.mean(valid_roc_auc_list)\n",
    "\n",
    "    # 标准打印训练信息\n",
    "    print(\"-- folds {}, valid_acc: {:.4f}, valid_roc_auc: {:.4f}, valid_custom: {:.4f}\".format(\n",
    "            fold, valid_acc, valid_roc_auc, valid_custom_metric))\n",
    "\n",
    "    # 保存模型与关键训练指标\n",
    "    MODELS.append(mlp_model)\n",
    "    valid_acc_total.append(valid_acc)\n",
    "    valid_roc_auc_total.append(valid_roc_auc)\n",
    "    valid_custom_total.append(valid_custom_metric)\n",
    "\n",
    "# 打印总体分数指标\n",
    "print(\"-- total metric, valid_acc: {:.4f}, valid_roc_auc: {:.4f}, valid_custom: {:.4f}\".format(\n",
    "        np.mean(valid_acc_total), np.mean(valid_roc_auc_total), np.mean(valid_custom_total)))\n",
    "\n",
    "print(\"=========================================\")\n",
    "print(\"[INFO] {} Model training end\".format(str(datetime.now())[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 转换为Tensorflow Lite Model，加快推理速度\n",
    "# MODELS_TFLITE = []\n",
    "# for model in MODELS:\n",
    "#     tflite_model = LiteModel.from_keras_model(model)\n",
    "#     MODELS_TFLITE.append(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 预备提交预测结果\n",
    "# env = gen_test_data(example_test_df, example_prediction_df)\n",
    "\n",
    "# THRESHOLD = 0.5\n",
    "# for (test_df, pred_df) in tqdm(env):\n",
    "#     if test_df['weight'].item() > 0:\n",
    "#         x_test_val = test_df[feature_name_list].values\n",
    "#         x_test_val = njit_fillna(x_test_val, mean_val_array).reshape(1, -1)\n",
    "\n",
    "#         # 利用MODELS里训练好的神经网络进行训练\n",
    "#         pred = [model.predict(x_test_val) for model in MODELS]\n",
    "#         pred = np.mean(pred)\n",
    "#         pred_df.action = np.where(pred >= THRESHOLD, 1, 0).astype(int)\n",
    "#     else:\n",
    "#         pred_df.action = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}