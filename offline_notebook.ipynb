{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "e14d75e190cad09067b6ef4ade1f47ca7ff568b5a764ea50d7cf40d3762ef09d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import callbacks\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from IPython.display import HTML\n",
    "\n",
    "# 设置plotly为暗黑模式\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "plot_config = dict({'scrollZoom': True, 'displayModeBar': True, 'displaylogo': False})\n",
    "sns.set(style=\"ticks\", font_scale=1.2, palette='deep', color_codes=True)\n",
    "colors = [\"C\" + str(i) for i in range(0, 9+1)]\n",
    "\n",
    "# 默认plotly色号\n",
    "default_color_list = [\n",
    "    '#1f77b4',  # muted blue\n",
    "    '#ff7f0e',  # safety orange\n",
    "    '#2ca02c',  # cooked asparagus green\n",
    "    '#d62728',  # brick red\n",
    "    '#9467bd',  # muted purple\n",
    "    '#8c564b',  # chestnut brown\n",
    "    '#e377c2',  # raspberry yogurt pink\n",
    "    '#7f7f7f',  # middle gray\n",
    "    '#bcbd22',  # curry yellow-green\n",
    "    '#17becf'   # blue-teal\n",
    "    ]\n",
    "\n",
    "# 设定全局随机种子，并且屏蔽warnings\n",
    "GLOBAL_RANDOM_SEED = 2022\n",
    "np.random.seed(GLOBAL_RANDOM_SEED)\n",
    "tf.random.set_seed(GLOBAL_RANDOM_SEED)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 导入数据\n",
    "load_data_start_time = time.time()\n",
    "train_df  = pd.read_csv(\n",
    "    './data/jane-street-market-prediction/train.csv', nrows=None)\n",
    "feat_df = pd.read_csv(\n",
    "    './data/jane-street-market-prediction/features.csv')\n",
    "example_test_df = pd.read_csv(\n",
    "    './data/jane-street-market-prediction/example_test.csv')\n",
    "sample_prediction_df = pd.read_csv(\n",
    "    './data/jane-street-market-prediction/example_sample_submission.csv')\n",
    "load_data_end_time = time.time()\n",
    "\n",
    "# 打印数据基本情况\n",
    "print(\"[INFO] {} End Reading ! It took {:.2f} seconds !\".format(\n",
    "    str(datetime.now())[:-4], load_data_end_time-load_data_start_time))\n",
    "print(\"[INFO] {} Basic data description: \".format(str(datetime.now())[:-4]))\n",
    "print(\"    -- train_df shape: {}\".format(\n",
    "    train_df.shape))\n",
    "print(\"    -- example_test_df shape: {}\".format(\n",
    "    example_test_df.shape))\n",
    "print(\"    -- feat_df shape: {}\".format(\n",
    "    feat_df.shape))\n",
    "print(\"    -- sample_prediction_df shape: {}\".format(\n",
    "    sample_prediction_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0, 344], [0, 375], [0, 406], [0, 437]]\n[[376, 406], [407, 437], [438, 468], [469, 499]]\n"
     ]
    }
   ],
   "source": [
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"针对带有Group id（组id）数据的时间序列交叉验证集合生成类。\n",
    "\n",
    "    生成针对带有Group id的数据的时序交叉验证集。其中训练与验证的\n",
    "    Group之间可以指定group_gap，用来隔离时间上的关系。这种情况下\n",
    "    group_id通常是时间id，例如天或者小时。\n",
    "\n",
    "    @Parameters:\n",
    "    ----------\n",
    "        n_splits: {int-like}, default=5\n",
    "            切分的集合数目。\n",
    "        max_train_group_size: {int-like}, default=+inf\n",
    "            训练集单个组的最大样本数据限制。\n",
    "        group_gap: {int-like}, default=None\n",
    "            依据group_id切分组时，训练组与测试组的id的gap数目。\n",
    "        max_test_group_size: {int-like}, default=+inf\n",
    "            测试集单个组的最大样本数据限制。\n",
    "\n",
    "    @References:\n",
    "    ----------\n",
    "    [1] https://www.kaggle.com/gogo827jz/jane-street-ffill-xgboost-purgedtimeseriescv\n",
    "    \"\"\"\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self, n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"生成训练组与测试组的id索引，返回组索引的生成器。\n",
    "\n",
    "        @Parameters:\n",
    "        ----------\n",
    "            X: {array-like} {n_samples, n_features}\n",
    "                训练数据，输入形状为{n_samples, n_features}。\n",
    "            y: {array-like} {n_samples, }\n",
    "                标签数据，形状为{n_samples, }。\n",
    "            groups: {array-like} {n_samples, }\n",
    "                用来依据组来划分训练集与测试集的组id，必须为连续的组id。\n",
    "\n",
    "        @Yields:\n",
    "        ----------\n",
    "            train: ndarray\n",
    "                依据group_id切分的训练组id。\n",
    "            test: ndarray\n",
    "                依据group_id切分的测试组id。\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None ！\")\n",
    "\n",
    "        # 初始化基本参数信息\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples, n_splits, group_gap = _num_samples(X), self.n_splits, self.group_gap\n",
    "        n_folds = n_splits + 1\n",
    "\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = np.argsort(ind)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "\n",
    "        # 扫描整个数据id list，构建group_dcit，{group_id: 属于该group的样本的idx}\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds, n_groups))\n",
    "\n",
    "        # group_test_size: 每个fold预留的test group的大小\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array, test_array = [], []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "            test_array  = test_array[group_gap:]\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]\n",
    "\n",
    "\n",
    "def test_purged_group_time_series_split():\n",
    "    X = train_df[[\"ts_id\", \"feature_0\"]].values\n",
    "    y = train_df[\"resp\"].values\n",
    "    groups = train_df[\"date\"].values\n",
    "\n",
    "    group_ts_kfolds = PurgedGroupTimeSeriesSplit(\n",
    "        n_splits=4, group_gap=31, max_test_group_size=31)\n",
    "    train_idx, valid_idx = [], []\n",
    "    for train_idx_tmp, valid_idx_tmp in group_ts_kfolds.split(X=X, y=y, groups=groups):\n",
    "        train_idx.append(train_idx_tmp)\n",
    "        valid_idx.append(valid_idx_tmp)\n",
    "\n",
    "    print([[train_df.iloc[min(item)][\"date\"].astype(int),\n",
    "            train_df.iloc[max(item)][\"date\"].astype(int)] for item in train_idx])\n",
    "    print([[train_df.iloc[min(item)][\"date\"].astype(int),\n",
    "            train_df.iloc[max(item)][\"date\"].astype(int)] for item in valid_idx])\n",
    "\n",
    "test_purged_group_time_series_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tabular_autoencoder(verbose=False, is_compile=True,\n",
    "                              stddev=0.05, **kwargs):\n",
    "    \"\"\"降噪自编码器实现。针对表格形式数据的降噪自编码器，噪声等级由gaussian_noise指定\"\"\"\n",
    "    input_dim = kwargs.pop(\"input_dim\")\n",
    "    layer_input = Input(input_dim, dtype='float32')\n",
    "\n",
    "    encoded = BatchNormalization()(i)\n",
    "    encoded = GaussianNoise(stddev=stddev)(encoded)\n",
    "    encoded = Dense(640,activation='relu')(encoded)\n",
    "    decoded = Dropout(0.2)(encoded)\n",
    "    decoded = Dense(input_dim,name='decoded')(decoded)\n",
    "    x = Dense(320,activation='relu')(decoded)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(output_dim,activation='sigmoid',name='label_output')(x)\n",
    "\n",
    "    encoder = Model(inputs=i,outputs=encoded)\n",
    "    autoencoder = Model(inputs=i,outputs=[decoded,x])\n",
    "    \n",
    "    autoencoder.compile(optimizer=Adam(0.001),loss={'decoded':'mse','label_output':'binary_crossentropy'})\n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(25)[[\"date\", \"weight\", \"resp_1\", \"resp_2\", \"resp_3\", \"resp_4\", \"resp\",\n",
    "                   \"feature_0\", \"feature_129\", \"ts_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_config = dict({'scrollZoom': False, 'displayModeBar': True, 'displaylogo': False})\n",
    "fig = make_subplots(rows=2, cols=2)\n",
    "ind = 1\n",
    "for row in range(1, 2+1):\n",
    "    for col in range(1, 2+1):\n",
    "        resp_tmp = np.cumsum(train_df[\"resp_{}\".format(ind)].head(200000).values)\n",
    "        ts_id = train_df[\"ts_id\"].head(200000).values\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=ts_id, y=resp_tmp,\n",
    "                                 mode=\"lines\", line_width=2,\n",
    "                                 line_color=default_color_list[ind],\n",
    "                                 name=\"resp_{}\".format(row)), row=row, col=col)\n",
    "        fig.update_yaxes(title_text=\"resp_{}\".format(ind), ticks=\"outside\",\n",
    "                         row=row, col=col, automargin=True)\n",
    "        ind += 1\n",
    "\n",
    "# fig.update_layout(go.Layout(title=\"resp plot\", width=900, height=800,\n",
    "#                             showlegend=False))\n",
    "# fig.show(config=plot_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}