{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "e14d75e190cad09067b6ef4ade1f47ca7ff568b5a764ea50d7cf40d3762ef09d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import iinfo, finfo, int8, int16, int32, int64, float32, float64\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import activations\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from IPython.display import HTML\n",
    "\n",
    "# 设置plotly为暗黑模式\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "plot_config = dict({'scrollZoom': True, 'displayModeBar': True, 'displaylogo': False})\n",
    "sns.set(style=\"ticks\", font_scale=1.2, palette='deep', color_codes=True)\n",
    "colors = [\"C\" + str(i) for i in range(0, 9+1)]\n",
    "\n",
    "# 默认plotly色号\n",
    "default_color_list = [\n",
    "    '#1f77b4',  # muted blue\n",
    "    '#ff7f0e',  # safety orange\n",
    "    '#2ca02c',  # cooked asparagus green\n",
    "    '#d62728',  # brick red\n",
    "    '#9467bd',  # muted purple\n",
    "    '#8c564b',  # chestnut brown\n",
    "    '#e377c2',  # raspberry yogurt pink\n",
    "    '#7f7f7f',  # middle gray\n",
    "    '#bcbd22',  # curry yellow-green\n",
    "    '#17becf'   # blue-teal\n",
    "    ]\n",
    "\n",
    "# 设定全局随机种子，并且屏蔽warnings\n",
    "GLOBAL_RANDOM_SEED = 2022\n",
    "np.random.seed(GLOBAL_RANDOM_SEED)\n",
    "tf.random.set_seed(GLOBAL_RANDOM_SEED)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] 2020-12-30 20:19:57.30 End Reading ! It took 55.44 seconds !\n[INFO] 2020-12-30 20:19:57.30 Basic data description: \n    -- train_df shape: (2390491, 138)\n    -- example_test_df shape: (15219, 133)\n    -- feat_df shape: (130, 30)\n    -- sample_prediction_df shape: (15219, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 导入数据\n",
    "load_data_start_time = time.time()\n",
    "train_df  = pd.read_csv(\n",
    "    './data/jane-street-market-prediction/train.csv', nrows=None)\n",
    "feat_df = pd.read_csv(\n",
    "    './data/jane-street-market-prediction/features.csv')\n",
    "example_test_df = pd.read_csv(\n",
    "    './data/jane-street-market-prediction/example_test.csv')\n",
    "sample_prediction_df = pd.read_csv(\n",
    "    './data/jane-street-market-prediction/example_sample_submission.csv')\n",
    "load_data_end_time = time.time()\n",
    "\n",
    "# 打印数据基本情况\n",
    "print(\"[INFO] {} End Reading ! It took {:.2f} seconds !\".format(\n",
    "    str(datetime.now())[:-4], load_data_end_time-load_data_start_time))\n",
    "print(\"[INFO] {} Basic data description: \".format(str(datetime.now())[:-4]))\n",
    "print(\"    -- train_df shape: {}\".format(\n",
    "    train_df.shape))\n",
    "print(\"    -- example_test_df shape: {}\".format(\n",
    "    example_test_df.shape))\n",
    "print(\"    -- feat_df shape: {}\".format(\n",
    "    feat_df.shape))\n",
    "print(\"    -- sample_prediction_df shape: {}\".format(\n",
    "    sample_prediction_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReduceMemoryUsage():\n",
    "    \"\"\"通过pandas的DataFrame的每一列的类型转换，降低DataFrame的内存消耗。\n",
    "\n",
    "    扫描pandas的DataFrame的每一列（column），获取每一列的最大最小值。判断\n",
    "    其是否落在某一具体浮点数或者整型数范围之内，并进行强制类型转换。例如：\n",
    "    某列（float64表示）最大最小值为[l, h]区间，而l与h分别小于float32最大最小\n",
    "    表示范围[-k, +k]范围内，则可以进行类型转换为float32来表示该列元素。\n",
    "\n",
    "    @Parameters:\n",
    "    ----------\n",
    "        data_table: {pandas DataFrame-like}\n",
    "            pandas的DataFrame类型。\n",
    "        verbose: {bool-like}\n",
    "            是否打印内存精简的相关信息。\n",
    "\n",
    "    @Return:\n",
    "    ----------\n",
    "        经过内存精简的DataFrame\n",
    "\n",
    "    @References:\n",
    "    ----------\n",
    "    [1] https://docs.scipy.org/doc/numpy/reference/generated/numpy.iinfo.html\n",
    "    [2] https://wizardforcel.gitbooks.io/ts-numpy-tut/content/3.html\n",
    "    \"\"\"\n",
    "    def __init__(self, data_table=None, verbose=True):\n",
    "        self._data_table = data_table\n",
    "        self._verbose = verbose\n",
    "\n",
    "    def type_report(self, data_table):\n",
    "        \"\"\"Reporting basic characteristics of the tabular data data_table.\"\"\"\n",
    "        data_types = list(map(str, data_table.dtypes.values))\n",
    "        basic_report = pd.DataFrame(data_types, columns=[\"types\"])\n",
    "        basic_report[\"feature_name\"] = list(data_table.columns)\n",
    "        return basic_report\n",
    "\n",
    "    def reduce_memory_usage(self):\n",
    "        memory_reduced_data = self.__reduce_memory()\n",
    "        return memory_reduced_data\n",
    "\n",
    "    def __reduce_memory(self):\n",
    "        print(\"\\nReduce memory process:\")\n",
    "        print(\"-------------------------------------------\")\n",
    "        memory_before_reduced = self._data_table.memory_usage(\n",
    "            deep=True).sum() / 1024**2\n",
    "        types = self.type_report(self._data_table)\n",
    "        if self._verbose is True:\n",
    "            print(\"@Memory usage of data is {:.5f} MB.\".format(\n",
    "                memory_before_reduced))\n",
    "\n",
    "        # Scan each feature in data_table, reduce the memory usage for features\n",
    "        for ind, name in enumerate(types[\"feature_name\"].values):\n",
    "            # ToBeFixed: Unstable query.\n",
    "            feature_type = str(\n",
    "                types[types[\"feature_name\"] == name][\"types\"].iloc[0])\n",
    "\n",
    "            if (feature_type in \"object\") and (feature_type in \"datetime64[ns]\"):\n",
    "                try:\n",
    "                    feature_min = self._data_table[name].min()\n",
    "                    feature_max = self._data_table[name].max()\n",
    "\n",
    "                    if \"int\" in feature_type:\n",
    "                        if feature_min > iinfo(int8).min and feature_max < iinfo(int8).max:\n",
    "                            self._data_table[name] = self._data_table[name].astype(int8)\n",
    "                        elif feature_min > iinfo(int16).min and feature_max < iinfo(int16).max:\n",
    "                            self._data_table[name] = self._data_table[name].astype(int16)\n",
    "                        elif feature_min > iinfo(int32).min and feature_max < iinfo(int32).max:\n",
    "                            self._data_table[name] = self._data_table[name].astype(int32)\n",
    "                        else:\n",
    "                            self._data_table[name] = self._data_table[name].astype(int64)\n",
    "                    else:\n",
    "                        if feature_min > finfo(float32).min and feature_max < finfo(float32).max:\n",
    "                            self._data_table[name] = self._data_table[name].astype(float32)\n",
    "                        else:\n",
    "                            self._data_table[name] = self._data_table[name].astype(float64)\n",
    "                except Exception as error_msg:\n",
    "                    print(\"\\n--------ERROR INFORMATION---------\")\n",
    "                    print(error_msg)\n",
    "                    print(\"Error on the {}\".format(name))\n",
    "                    print(\"--------ERROR INFORMATION---------\\n\")\n",
    "            if self._verbose is True:\n",
    "                print(\"Processed {} feature({}), total is {}.\".format(\n",
    "                    ind + 1, name, len(types)))\n",
    "\n",
    "        memory_after_reduced = self._data_table.memory_usage(\n",
    "            deep=True).sum() / 1024**2\n",
    "        if self._verbose is True:\n",
    "            print(\"@Memory usage after optimization: {:.5f} MB.\".format(\n",
    "                memory_after_reduced))\n",
    "            print(\"@Decreased by {:.5f}%.\".format(\n",
    "                100 * (memory_before_reduced - memory_after_reduced) / memory_before_reduced))\n",
    "        print(\"-------------------------------------------\")\n",
    "        return self._data_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0, 227], [0, 262], [0, 295], [0, 327]]\n[[262, 295], [295, 327], [327, 363], [363, 396]]\n"
     ]
    }
   ],
   "source": [
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"针对带有Group id（组id）数据的时间序列交叉验证集合生成类。\n",
    "\n",
    "    生成针对带有Group id的数据的时序交叉验证集。其中训练与验证的\n",
    "    Group之间可以指定group_gap，用来隔离时间上的关系。这种情况下\n",
    "    group_id通常是时间id，例如天或者小时。\n",
    "\n",
    "    @Parameters:\n",
    "    ----------\n",
    "        n_splits: {int-like}, default=5\n",
    "            切分的集合数目。\n",
    "        max_train_group_size: {int-like}, default=+inf\n",
    "            训练集单个组的最大样本数据限制。\n",
    "        group_gap: {int-like}, default=None\n",
    "            依据group_id切分组时，训练组与测试组的id的gap数目。\n",
    "        max_test_group_size: {int-like}, default=+inf\n",
    "            测试集单个组的最大样本数据限制。\n",
    "\n",
    "    @References:\n",
    "    ----------\n",
    "    [1] https://www.kaggle.com/gogo827jz/jane-street-ffill-xgboost-purgedtimeseriescv\n",
    "    \"\"\"\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self, n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"生成训练组与测试组的id索引，返回组索引的生成器。\n",
    "\n",
    "        @Parameters:\n",
    "        ----------\n",
    "            X: {array-like} {n_samples, n_features}\n",
    "                训练数据，输入形状为{n_samples, n_features}。\n",
    "            y: {array-like} {n_samples, }\n",
    "                标签数据，形状为{n_samples, }。\n",
    "            groups: {array-like} {n_samples, }\n",
    "                用来依据组来划分训练集与测试集的组id，必须为连续的组id。\n",
    "\n",
    "        @Yields:\n",
    "        ----------\n",
    "            train: ndarray\n",
    "                依据group_id切分的训练组id。\n",
    "            test: ndarray\n",
    "                依据group_id切分的测试组id。\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None ！\")\n",
    "\n",
    "        # 初始化基本参数信息\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples, n_splits, group_gap = _num_samples(X), self.n_splits, self.group_gap\n",
    "        n_folds = n_splits + 1\n",
    "\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = np.argsort(ind)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "\n",
    "        # 扫描整个数据id list，构建group_dcit，{group_id: 属于该group的样本的idx}\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds, n_groups))\n",
    "\n",
    "        # group_test_size: 每个fold预留的test group的大小\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array, test_array = [], []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "            test_array  = test_array[group_gap:]\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]\n",
    "\n",
    "\n",
    "def test_purged_group_time_series_split():\n",
    "    X = train_df.query('date > 85').reset_index(drop=True)[[\"ts_id\", \"feature_0\"]].values\n",
    "    y = train_df.query('date > 85').reset_index(drop=True)[\"resp\"].values\n",
    "    groups = train_df.query('date > 85').reset_index(drop=True)[\"date\"].values\n",
    "\n",
    "    group_ts_kfolds = PurgedGroupTimeSeriesSplit(\n",
    "        n_splits=4, group_gap=31, max_test_group_size=31)\n",
    "    train_idx, valid_idx = [], []\n",
    "    for train_idx_tmp, valid_idx_tmp in group_ts_kfolds.split(X=X, y=y, groups=groups):\n",
    "        train_idx.append(train_idx_tmp)\n",
    "        valid_idx.append(valid_idx_tmp)\n",
    "\n",
    "    print([[train_df.iloc[min(item)][\"date\"].astype(int),\n",
    "            train_df.iloc[max(item)][\"date\"].astype(int)] for item in train_idx])\n",
    "    print([[train_df.iloc[min(item)][\"date\"].astype(int),\n",
    "            train_df.iloc[max(item)][\"date\"].astype(int)] for item in valid_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] 2020-12-30 16:04:18.25 Autoencoder data prepared !\n"
     ]
    }
   ],
   "source": [
    "train = train_df.query('date > 85').reset_index(drop=True)\n",
    "train.fillna(method=\"bfill\", inplace=True)\n",
    "\n",
    "# 构造标签\n",
    "train = train.query('weight > 0').reset_index(drop = True)\n",
    "train['action'] =  ((train['resp_1'] > 0.00001) & \\\n",
    "                    (train['resp_2'] > 0.00001 ) & \\\n",
    "                    (train['resp_3'] > 0.00001) & \\\n",
    "                    (train['resp_4'] > 0.00001 ) &  \\\n",
    "                    (train['resp'] > 0.00001)).astype('int')\n",
    "feature_name_list = [c for c in train.columns if 'feature' in c]\n",
    "resp_name_list = [\"resp\", \"resp_1\", \"resp_2\", \"resp_3\", \"resp_4\"]\n",
    "\n",
    "# 构造自编码器的输入输出\n",
    "X = train[feature_name_list].values\n",
    "y = np.stack([(train[c] > 0.000001).astype('int') for c in resp_name_list]).T\n",
    "group_ids = train[\"date\"].values\n",
    "print(\"[INFO] {} Autoencoder data prepared !\".format(\n",
    "    str(datetime.now())[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] 2020-12-30 15:17:25.41 Build autoencoder successed !\n"
     ]
    }
   ],
   "source": [
    "def build_tabular_autoencoder(verbose=False, is_compile=True,\n",
    "                              stddev=0.05, **kwargs):\n",
    "    \"\"\"降噪自编码器实现。针对表格形式数据的降噪自编码器，噪声等级由高斯噪声的stddev参数指定\"\"\"\n",
    "    input_dim = kwargs.pop(\"input_dim\", None)\n",
    "    n_labels = kwargs.pop(\"n_labels\", None)\n",
    "\n",
    "    # 构建降噪自编码器\n",
    "    layer_input = layers.Input(input_dim, dtype='float32')\n",
    "\n",
    "    layer_encoded = layers.BatchNormalization()(layer_input)\n",
    "    layer_encoded = layers.GaussianNoise(stddev=stddev)(layer_encoded)\n",
    "    layer_encoded = layers.Dense(640, activation='relu')(layer_encoded)\n",
    "\n",
    "    # 解码层1：针对输入的重构\n",
    "    layer_decoded = layers.Dropout(0.2)(layer_encoded)\n",
    "    layer_decoded = layers.Dense(input_dim, name='reconstruct_output')(layer_decoded)\n",
    "\n",
    "    # 解码层2：针对resp的重构\n",
    "    layer_output = layers.Dense(320, activation='relu')(layer_decoded)\n",
    "    layer_output = layers.BatchNormalization()(layer_output)\n",
    "    layer_output = layers.Dropout(0.2)(layer_output)\n",
    "    layer_output = layers.Dense(n_labels, activation='sigmoid', \n",
    "                                name='label_output')(layer_output)\n",
    "\n",
    "    # 输出层\n",
    "    encoder_model = models.Model(inputs=layer_input, outputs=layer_decoded)\n",
    "    autoencoder_model = models.Model(inputs=layer_input, outputs=[layer_decoded, layer_output])\n",
    "\n",
    "    if verbose:\n",
    "        autoencoder_model.summary()\n",
    "    if is_compile:\n",
    "        autoencoder_model.compile(loss={'reconstruct_output':'mae', 'label_output':'binary_crossentropy'},\n",
    "                                  metrics={'label_output':'acc'}, \n",
    "                                  optimizer=optimizers.Adam(0.006))\n",
    "    return encoder_model, autoencoder_model\n",
    "\n",
    "encoder_model, autoencoder_model = build_tabular_autoencoder(\n",
    "    input_dim=X.shape[1], n_labels=y.shape[1], verbose=False)\n",
    "print(\"[INFO] {} Build autoencoder successed !\".format(\n",
    "    str(datetime.now())[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 1414273 samples, validate on 157142 samples\n",
      "Epoch 1/40\n",
      "1414273/1414273 - 20s - loss: 1.4524 - reconstruct_output_loss: 0.6946 - label_output_loss: 0.7512 - label_output_acc: 0.5079 - val_loss: 1.3237 - val_reconstruct_output_loss: 0.6246 - val_label_output_loss: 0.6983 - val_label_output_acc: 0.5250\n",
      "Epoch 2/40\n",
      "1414273/1414273 - 19s - loss: 1.0493 - reconstruct_output_loss: 0.3514 - label_output_loss: 0.6974 - label_output_acc: 0.5173 - val_loss: 1.0311 - val_reconstruct_output_loss: 0.3408 - val_label_output_loss: 0.6898 - val_label_output_acc: 0.5288\n",
      "Epoch 3/40\n",
      "1414273/1414273 - 20s - loss: 1.0005 - reconstruct_output_loss: 0.3084 - label_output_loss: 0.6921 - label_output_acc: 0.5230 - val_loss: 0.9522 - val_reconstruct_output_loss: 0.2628 - val_label_output_loss: 0.6890 - val_label_output_acc: 0.5303\n",
      "Epoch 4/40\n",
      "1414273/1414273 - 20s - loss: 0.9866 - reconstruct_output_loss: 0.2960 - label_output_loss: 0.6905 - label_output_acc: 0.5261 - val_loss: 0.8564 - val_reconstruct_output_loss: 0.1674 - val_label_output_loss: 0.6888 - val_label_output_acc: 0.5311\n",
      "Epoch 5/40\n",
      "1414273/1414273 - 20s - loss: 0.9820 - reconstruct_output_loss: 0.2919 - label_output_loss: 0.6900 - label_output_acc: 0.5276 - val_loss: 0.8565 - val_reconstruct_output_loss: 0.1677 - val_label_output_loss: 0.6887 - val_label_output_acc: 0.5324\n",
      "Epoch 6/40\n",
      "1414273/1414273 - 21s - loss: 0.9752 - reconstruct_output_loss: 0.2856 - label_output_loss: 0.6897 - label_output_acc: 0.5289 - val_loss: 0.8467 - val_reconstruct_output_loss: 0.1582 - val_label_output_loss: 0.6884 - val_label_output_acc: 0.5344\n",
      "Epoch 7/40\n",
      "1414273/1414273 - 21s - loss: 0.9701 - reconstruct_output_loss: 0.2805 - label_output_loss: 0.6895 - label_output_acc: 0.5294 - val_loss: 0.8461 - val_reconstruct_output_loss: 0.1570 - val_label_output_loss: 0.6890 - val_label_output_acc: 0.5314\n",
      "Epoch 8/40\n",
      "1414273/1414273 - 21s - loss: 0.9665 - reconstruct_output_loss: 0.2770 - label_output_loss: 0.6895 - label_output_acc: 0.5296 - val_loss: 0.8403 - val_reconstruct_output_loss: 0.1514 - val_label_output_loss: 0.6887 - val_label_output_acc: 0.5330\n",
      "Epoch 9/40\n",
      "1414273/1414273 - 20s - loss: 0.9650 - reconstruct_output_loss: 0.2756 - label_output_loss: 0.6894 - label_output_acc: 0.5300 - val_loss: 0.8511 - val_reconstruct_output_loss: 0.1622 - val_label_output_loss: 0.6887 - val_label_output_acc: 0.5320\n",
      "Epoch 10/40\n",
      "1414273/1414273 - 20s - loss: 0.9605 - reconstruct_output_loss: 0.2711 - label_output_loss: 0.6893 - label_output_acc: 0.5305 - val_loss: 0.8555 - val_reconstruct_output_loss: 0.1670 - val_label_output_loss: 0.6883 - val_label_output_acc: 0.5321\n",
      "Epoch 11/40\n",
      "1414273/1414273 - 20s - loss: 0.9563 - reconstruct_output_loss: 0.2671 - label_output_loss: 0.6893 - label_output_acc: 0.5304 - val_loss: 0.8449 - val_reconstruct_output_loss: 0.1561 - val_label_output_loss: 0.6885 - val_label_output_acc: 0.5328\n",
      "Epoch 12/40\n",
      "1414273/1414273 - 20s - loss: 0.9542 - reconstruct_output_loss: 0.2652 - label_output_loss: 0.6890 - label_output_acc: 0.5312 - val_loss: 0.8375 - val_reconstruct_output_loss: 0.1487 - val_label_output_loss: 0.6885 - val_label_output_acc: 0.5323\n",
      "Epoch 13/40\n",
      "1414273/1414273 - 20s - loss: 0.9501 - reconstruct_output_loss: 0.2609 - label_output_loss: 0.6891 - label_output_acc: 0.5314 - val_loss: 0.8442 - val_reconstruct_output_loss: 0.1555 - val_label_output_loss: 0.6885 - val_label_output_acc: 0.5336\n",
      "Epoch 14/40\n",
      "1414273/1414273 - 20s - loss: 0.9487 - reconstruct_output_loss: 0.2598 - label_output_loss: 0.6890 - label_output_acc: 0.5319 - val_loss: 0.8573 - val_reconstruct_output_loss: 0.1688 - val_label_output_loss: 0.6882 - val_label_output_acc: 0.5329\n",
      "Epoch 15/40\n",
      "1414273/1414273 - 20s - loss: 0.9454 - reconstruct_output_loss: 0.2565 - label_output_loss: 0.6889 - label_output_acc: 0.5319 - val_loss: 0.8380 - val_reconstruct_output_loss: 0.1494 - val_label_output_loss: 0.6885 - val_label_output_acc: 0.5333\n",
      "Epoch 16/40\n",
      "1414273/1414273 - 20s - loss: 0.9408 - reconstruct_output_loss: 0.2521 - label_output_loss: 0.6889 - label_output_acc: 0.5319 - val_loss: 0.8452 - val_reconstruct_output_loss: 0.1567 - val_label_output_loss: 0.6882 - val_label_output_acc: 0.5336\n",
      "Epoch 17/40\n",
      "1414273/1414273 - 20s - loss: 0.9409 - reconstruct_output_loss: 0.2521 - label_output_loss: 0.6889 - label_output_acc: 0.5323 - val_loss: 0.8318 - val_reconstruct_output_loss: 0.1428 - val_label_output_loss: 0.6889 - val_label_output_acc: 0.5324\n",
      "Epoch 18/40\n",
      "1414273/1414273 - 20s - loss: 0.9394 - reconstruct_output_loss: 0.2507 - label_output_loss: 0.6888 - label_output_acc: 0.5322 - val_loss: 0.8297 - val_reconstruct_output_loss: 0.1417 - val_label_output_loss: 0.6879 - val_label_output_acc: 0.5348\n",
      "Epoch 19/40\n",
      "1414273/1414273 - 20s - loss: 0.9359 - reconstruct_output_loss: 0.2474 - label_output_loss: 0.6887 - label_output_acc: 0.5330 - val_loss: 0.8487 - val_reconstruct_output_loss: 0.1599 - val_label_output_loss: 0.6885 - val_label_output_acc: 0.5323\n",
      "Epoch 20/40\n",
      "1414273/1414273 - 20s - loss: 0.9357 - reconstruct_output_loss: 0.2472 - label_output_loss: 0.6886 - label_output_acc: 0.5333 - val_loss: 0.8275 - val_reconstruct_output_loss: 0.1391 - val_label_output_loss: 0.6882 - val_label_output_acc: 0.5341\n",
      "Epoch 21/40\n",
      "1414273/1414273 - 21s - loss: 0.9325 - reconstruct_output_loss: 0.2440 - label_output_loss: 0.6887 - label_output_acc: 0.5334 - val_loss: 0.8263 - val_reconstruct_output_loss: 0.1375 - val_label_output_loss: 0.6886 - val_label_output_acc: 0.5321\n",
      "Epoch 22/40\n",
      "1414273/1414273 - 21s - loss: 0.9313 - reconstruct_output_loss: 0.2429 - label_output_loss: 0.6884 - label_output_acc: 0.5337 - val_loss: 0.8266 - val_reconstruct_output_loss: 0.1381 - val_label_output_loss: 0.6884 - val_label_output_acc: 0.5330\n",
      "Epoch 23/40\n",
      "1414273/1414273 - 20s - loss: 0.9294 - reconstruct_output_loss: 0.2411 - label_output_loss: 0.6885 - label_output_acc: 0.5338 - val_loss: 0.8302 - val_reconstruct_output_loss: 0.1419 - val_label_output_loss: 0.6883 - val_label_output_acc: 0.5334\n",
      "Epoch 24/40\n",
      "1414273/1414273 - 20s - loss: 0.9260 - reconstruct_output_loss: 0.2379 - label_output_loss: 0.6884 - label_output_acc: 0.5339 - val_loss: 0.8165 - val_reconstruct_output_loss: 0.1283 - val_label_output_loss: 0.6881 - val_label_output_acc: 0.5343\n",
      "Epoch 25/40\n",
      "1414273/1414273 - 20s - loss: 0.9266 - reconstruct_output_loss: 0.2384 - label_output_loss: 0.6883 - label_output_acc: 0.5341 - val_loss: 0.8300 - val_reconstruct_output_loss: 0.1410 - val_label_output_loss: 0.6888 - val_label_output_acc: 0.5318\n",
      "Epoch 26/40\n",
      "1414273/1414273 - 20s - loss: 0.9260 - reconstruct_output_loss: 0.2377 - label_output_loss: 0.6882 - label_output_acc: 0.5341 - val_loss: 0.8178 - val_reconstruct_output_loss: 0.1294 - val_label_output_loss: 0.6881 - val_label_output_acc: 0.5342\n",
      "Epoch 27/40\n",
      "1414273/1414273 - 20s - loss: 0.9248 - reconstruct_output_loss: 0.2366 - label_output_loss: 0.6882 - label_output_acc: 0.5346 - val_loss: 0.8322 - val_reconstruct_output_loss: 0.1437 - val_label_output_loss: 0.6884 - val_label_output_acc: 0.5326\n",
      "Epoch 28/40\n",
      "1414273/1414273 - 20s - loss: 0.9231 - reconstruct_output_loss: 0.2350 - label_output_loss: 0.6881 - label_output_acc: 0.5344 - val_loss: 0.8355 - val_reconstruct_output_loss: 0.1472 - val_label_output_loss: 0.6880 - val_label_output_acc: 0.5346\n",
      "Epoch 29/40\n",
      "1414273/1414273 - 20s - loss: 0.9228 - reconstruct_output_loss: 0.2347 - label_output_loss: 0.6881 - label_output_acc: 0.5344 - val_loss: 0.8203 - val_reconstruct_output_loss: 0.1321 - val_label_output_loss: 0.6880 - val_label_output_acc: 0.5348\n",
      "Epoch 30/40\n",
      "1414273/1414273 - 21s - loss: 0.9217 - reconstruct_output_loss: 0.2339 - label_output_loss: 0.6881 - label_output_acc: 0.5347 - val_loss: 0.8261 - val_reconstruct_output_loss: 0.1376 - val_label_output_loss: 0.6883 - val_label_output_acc: 0.5332\n",
      "Epoch 31/40\n",
      "1414273/1414273 - 21s - loss: 0.9203 - reconstruct_output_loss: 0.2323 - label_output_loss: 0.6880 - label_output_acc: 0.5351 - val_loss: 0.8566 - val_reconstruct_output_loss: 0.1681 - val_label_output_loss: 0.6883 - val_label_output_acc: 0.5338\n",
      "Epoch 32/40\n",
      "1414273/1414273 - 20s - loss: 0.9195 - reconstruct_output_loss: 0.2316 - label_output_loss: 0.6881 - label_output_acc: 0.5352 - val_loss: 0.8145 - val_reconstruct_output_loss: 0.1263 - val_label_output_loss: 0.6881 - val_label_output_acc: 0.5346\n",
      "Epoch 33/40\n",
      "1414273/1414273 - 20s - loss: 0.9175 - reconstruct_output_loss: 0.2296 - label_output_loss: 0.6880 - label_output_acc: 0.5355 - val_loss: 0.8211 - val_reconstruct_output_loss: 0.1326 - val_label_output_loss: 0.6882 - val_label_output_acc: 0.5329\n",
      "Epoch 34/40\n",
      "1414273/1414273 - 20s - loss: 0.9169 - reconstruct_output_loss: 0.2290 - label_output_loss: 0.6880 - label_output_acc: 0.5354 - val_loss: 0.8277 - val_reconstruct_output_loss: 0.1394 - val_label_output_loss: 0.6881 - val_label_output_acc: 0.5336\n",
      "Epoch 35/40\n",
      "1414273/1414273 - 20s - loss: 0.9181 - reconstruct_output_loss: 0.2303 - label_output_loss: 0.6879 - label_output_acc: 0.5357 - val_loss: 0.8200 - val_reconstruct_output_loss: 0.1314 - val_label_output_loss: 0.6883 - val_label_output_acc: 0.5337\n",
      "Epoch 36/40\n",
      "1414273/1414273 - 20s - loss: 0.9161 - reconstruct_output_loss: 0.2287 - label_output_loss: 0.6878 - label_output_acc: 0.5359 - val_loss: 0.8282 - val_reconstruct_output_loss: 0.1399 - val_label_output_loss: 0.6881 - val_label_output_acc: 0.5339\n",
      "Epoch 37/40\n",
      "1414273/1414273 - 20s - loss: 0.9154 - reconstruct_output_loss: 0.2279 - label_output_loss: 0.6878 - label_output_acc: 0.5361 - val_loss: 0.8151 - val_reconstruct_output_loss: 0.1267 - val_label_output_loss: 0.6883 - val_label_output_acc: 0.5349\n",
      "Epoch 38/40\n",
      "1414273/1414273 - 20s - loss: 0.9154 - reconstruct_output_loss: 0.2279 - label_output_loss: 0.6877 - label_output_acc: 0.5366 - val_loss: 0.8151 - val_reconstruct_output_loss: 0.1273 - val_label_output_loss: 0.6876 - val_label_output_acc: 0.5358\n",
      "Epoch 39/40\n",
      "1414273/1414273 - 21s - loss: 0.9140 - reconstruct_output_loss: 0.2264 - label_output_loss: 0.6877 - label_output_acc: 0.5361 - val_loss: 0.8187 - val_reconstruct_output_loss: 0.1301 - val_label_output_loss: 0.6883 - val_label_output_acc: 0.5334\n",
      "Epoch 40/40\n",
      "1414273/1414273 - 20s - loss: 0.9143 - reconstruct_output_loss: 0.2267 - label_output_loss: 0.6876 - label_output_acc: 0.5361 - val_loss: 0.8236 - val_reconstruct_output_loss: 0.1351 - val_label_output_loss: 0.6883 - val_label_output_acc: 0.5326\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb7903e1690>"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "early_stop = callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\",\n",
    "                                     verbose=1, patience=40,\n",
    "                                     restore_best_weights=True)\n",
    "autoencoder_model.fit(x=X, y=[X, y],\n",
    "                      batch_size=32768,\n",
    "                      epochs=40,\n",
    "                      verbose=2,\n",
    "                      validation_split=0.1,\n",
    "                      callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(verbose=False, is_compile=True, encoder_model=None, **kwargs):\n",
    "    \"\"\"针对二分类任务的MLP模型，使用自编码器的编码层作为预训练层。\"\"\"\n",
    "    input_dim = kwargs.pop(\"input_dim\", None)\n",
    "    output_dim = kwargs.pop(\"output_dim\", None)\n",
    "    n_labels = kwargs.pop(\"n_labels\", None)\n",
    "\n",
    "    # 构造网络结构\n",
    "    layer_input = layers.Input(input_dim, dtype='float32')\n",
    "    layer_encoded = encoder_model(layer_input)\n",
    "\n",
    "    layer_feats = layers.concatenate([layer_input, layer_encoded])\n",
    "    layer_feats = layers.BatchNormalization()(layer_feats)\n",
    "    layer_feats = layers.Lambda(tf.keras.activations.selu)(layer_feats)\n",
    "\n",
    "    # 特征抽取\n",
    "    layer_dense = layers.Dense(128, activation=\"relu\")(layer_feats)\n",
    "    layer_dense = layers.BatchNormalization()(layer_dense)\n",
    "    layer_dense = layers.Dropout(0.3)(layer_dense)\n",
    "\n",
    "    layer_dense = layers.Dense(32, activation=\"relu\")(layer_dense)\n",
    "    layer_dense = layers.BatchNormalization()(layer_dense)\n",
    "    layer_dense = layers.Dropout(0.3)(layer_dense)\n",
    "\n",
    "    # 输出层构造与模型构造\n",
    "    layer_output = layers.Dense(output_dim, activation='sigmoid', name=\"label_output\")(layer_dense)\n",
    "    model = models.Model(layer_input, layer_output)\n",
    "\n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    if is_compile:\n",
    "        model.compile(loss={'label_output':'binary_crossentropy'},\n",
    "                      metrics=[tf.keras.metrics.AUC(name='auc')],\n",
    "                      optimizer=optimizers.Adam(0.006))\n",
    "    return model\n",
    "\n",
    "print(\"[INFO] {} Build MLP Model successed !\".format(\n",
    "    str(datetime.now())[:-4]))\n",
    "\n",
    "# 构造mlp模型\n",
    "# mlp_model = build_model(verbose=False, encoder_model=encoder_model,\n",
    "#                         input_dim=X.shape[1], output_dim=y.shape[1])\n",
    "# print(\"[INFO] {} Build MLP successed !\".format(\n",
    "#     str(datetime.now())[:-4]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 666858 samples, validate on 119439 samples\n",
      "Epoch 1/5\n",
      "666858/666858 - 9s - loss: 0.7650 - auc: 0.5078 - val_loss: 0.8711 - val_auc: 0.5156\n",
      "Epoch 2/5\n",
      "666858/666858 - 9s - loss: 0.7102 - auc: 0.5126 - val_loss: 0.7210 - val_auc: 0.5258\n",
      "Epoch 3/5\n",
      "666858/666858 - 9s - loss: 0.6989 - auc: 0.5198 - val_loss: 0.6969 - val_auc: 0.5315\n",
      "Epoch 4/5\n",
      "666858/666858 - 9s - loss: 0.6944 - auc: 0.5270 - val_loss: 0.6915 - val_auc: 0.5378\n",
      "Epoch 5/5\n",
      "666858/666858 - 9s - loss: 0.6922 - auc: 0.5331 - val_loss: 0.6904 - val_auc: 0.5408\n",
      "Train on 783335 samples, validate on 130051 samples\n",
      "Epoch 1/5\n",
      "783335/783335 - 11s - loss: 0.7458 - auc: 0.5083 - val_loss: 0.7381 - val_auc: 0.5181\n",
      "Epoch 2/5\n",
      "783335/783335 - 10s - loss: 0.7039 - auc: 0.5147 - val_loss: 0.6975 - val_auc: 0.5230\n",
      "Epoch 3/5\n",
      "783335/783335 - 11s - loss: 0.6957 - auc: 0.5239 - val_loss: 0.6927 - val_auc: 0.5288\n",
      "Epoch 4/5\n",
      "783335/783335 - 11s - loss: 0.6927 - auc: 0.5318 - val_loss: 0.6915 - val_auc: 0.5327\n",
      "Epoch 5/5\n",
      "783335/783335 - 11s - loss: 0.6914 - auc: 0.5363 - val_loss: 0.6910 - val_auc: 0.5339\n",
      "Train on 902805 samples, validate on 127712 samples\n",
      "Epoch 1/5\n",
      "902805/902805 - 12s - loss: 0.7565 - auc: 0.5081 - val_loss: 0.7858 - val_auc: 0.5128\n",
      "Epoch 2/5\n",
      "902805/902805 - 11s - loss: 0.7046 - auc: 0.5131 - val_loss: 0.7011 - val_auc: 0.5217\n",
      "Epoch 3/5\n",
      "902805/902805 - 11s - loss: 0.6954 - auc: 0.5239 - val_loss: 0.6933 - val_auc: 0.5280\n",
      "Epoch 4/5\n",
      "902805/902805 - 11s - loss: 0.6924 - auc: 0.5317 - val_loss: 0.6920 - val_auc: 0.5298\n",
      "Epoch 5/5\n",
      "902805/902805 - 11s - loss: 0.6910 - auc: 0.5371 - val_loss: 0.6916 - val_auc: 0.5313\n",
      "Train on 1032887 samples, validate on 126825 samples\n",
      "Epoch 1/5\n",
      "1032887/1032887 - 14s - loss: 0.7391 - auc: 0.5086 - val_loss: 0.7199 - val_auc: 0.5294\n",
      "Epoch 2/5\n",
      "1032887/1032887 - 13s - loss: 0.6997 - auc: 0.5202 - val_loss: 0.6918 - val_auc: 0.5403\n",
      "Epoch 3/5\n",
      "1032887/1032887 - 13s - loss: 0.6935 - auc: 0.5313 - val_loss: 0.6897 - val_auc: 0.5432\n",
      "Epoch 4/5\n",
      "1032887/1032887 - 13s - loss: 0.6913 - auc: 0.5368 - val_loss: 0.6895 - val_auc: 0.5443\n",
      "Epoch 5/5\n",
      "1032887/1032887 - 13s - loss: 0.6904 - auc: 0.5405 - val_loss: 0.6894 - val_auc: 0.5450\n",
      "Train on 1160630 samples, validate on 145264 samples\n",
      "Epoch 1/5\n",
      "1160630/1160630 - 16s - loss: 0.7369 - auc: 0.5059 - val_loss: 0.7157 - val_auc: 0.5199\n",
      "Epoch 2/5\n",
      "1160630/1160630 - 16s - loss: 0.6996 - auc: 0.5151 - val_loss: 0.6931 - val_auc: 0.5274\n",
      "Epoch 3/5\n",
      "1160630/1160630 - 16s - loss: 0.6936 - auc: 0.5257 - val_loss: 0.6908 - val_auc: 0.5346\n",
      "Epoch 4/5\n",
      "1160630/1160630 - 16s - loss: 0.6916 - auc: 0.5322 - val_loss: 0.6903 - val_auc: 0.5385\n",
      "Epoch 5/5\n",
      "1160630/1160630 - 16s - loss: 0.6908 - auc: 0.5365 - val_loss: 0.6898 - val_auc: 0.5416\n",
      "Train on 1287486 samples, validate on 138603 samples\n",
      "Epoch 1/5\n",
      "1287486/1287486 - 18s - loss: 0.7455 - auc: 0.5082 - val_loss: 0.7116 - val_auc: 0.5294\n",
      "Epoch 2/5\n",
      "1287486/1287486 - 17s - loss: 0.6994 - auc: 0.5212 - val_loss: 0.6899 - val_auc: 0.5432\n",
      "Epoch 3/5\n",
      "1287486/1287486 - 17s - loss: 0.6932 - auc: 0.5307 - val_loss: 0.6891 - val_auc: 0.5458\n",
      "Epoch 4/5\n",
      "1287486/1287486 - 17s - loss: 0.6913 - auc: 0.5355 - val_loss: 0.6891 - val_auc: 0.5455\n",
      "Epoch 5/5\n",
      "1287486/1287486 - 17s - loss: 0.6906 - auc: 0.5380 - val_loss: 0.6890 - val_auc: 0.5473\n"
     ]
    }
   ],
   "source": [
    "# 训练前全局准备\n",
    "MODELS = []\n",
    "encoder_model.trainable = False\n",
    "early_stop = callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\",\n",
    "                                     verbose=1, patience=30,\n",
    "                                     restore_best_weights=True)\n",
    "group_ts_kfolds = PurgedGroupTimeSeriesSplit(\n",
    "        n_splits=6, group_gap=31, max_test_group_size=31)\n",
    "group_ids = group_ids - min(group_ids)\n",
    "\n",
    "# 开始训练模型\n",
    "for train_idx, valid_idx in group_ts_kfolds.split(X=X, y=y, groups=group_ids):\n",
    "    X_train, X_val = X[train_idx], X[valid_idx]\n",
    "    y_train, y_val = y[train_idx], y[valid_idx]\n",
    "\n",
    "    # 准备模型\n",
    "    mlp_model = build_model(verbose=False, encoder_model=encoder_model,\n",
    "                            input_dim=X_train.shape[1], output_dim=y_train.shape[1])\n",
    "\n",
    "    mlp_model.fit(x=X_train, y=y_train,\n",
    "                  validation_data=(X_val, y_val),\n",
    "                  batch_size=32768,\n",
    "                  epochs=5,\n",
    "                  verbose=2,\n",
    "                  callbacks=[early_stop])\n",
    "\n",
    "    # 保存模型\n",
    "    MODELS.append(mlp_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}