{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "from numba import njit, jit\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import iinfo, finfo, int8, int16, int32, int64, float32, float64\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import activations\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from IPython.display import HTML\n",
    "\n",
    "# 设置plotly为暗黑模式\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "plot_config = dict({'scrollZoom': True, 'displayModeBar': True, 'displaylogo': False})\n",
    "sns.set(style=\"ticks\", font_scale=1.2, palette='deep', color_codes=True)\n",
    "colors = [\"C\" + str(i) for i in range(0, 9+1)]\n",
    "\n",
    "# 默认plotly色号\n",
    "default_color_list = [\n",
    "    '#1f77b4',  # muted blue\n",
    "    '#ff7f0e',  # safety orange\n",
    "    '#2ca02c',  # cooked asparagus green\n",
    "    '#d62728',  # brick red\n",
    "    '#9467bd',  # muted purple\n",
    "    '#8c564b',  # chestnut brown\n",
    "    '#e377c2',  # raspberry yogurt pink\n",
    "    '#7f7f7f',  # middle gray\n",
    "    '#bcbd22',  # curry yellow-green\n",
    "    '#17becf'   # blue-teal\n",
    "    ]\n",
    "\n",
    "# 设定全局随机种子，并且屏蔽warnings\n",
    "GLOBAL_RANDOM_SEED = 2022\n",
    "np.random.seed(GLOBAL_RANDOM_SEED)\n",
    "tf.random.set_seed(GLOBAL_RANDOM_SEED)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] 2021-01-04 20:17:13.00 End Reading ! It took 57.14 seconds !\n[INFO] 2021-01-04 20:17:13.00 Basic data description: \n    -- train_df shape: (2390491, 138)\n    -- example_test_df shape: (15219, 133)\n    -- feat_df shape: (130, 30)\n    -- example_prediction_df shape: (15219, 2)\n"
     ]
    }
   ],
   "source": [
    "# 导入数据\n",
    "load_data_start_time = time.time()\n",
    "train_df  = pd.read_csv(\n",
    "    './data/jane-street-market-prediction/train.csv', nrows=None)\n",
    "feat_df = pd.read_csv(\n",
    "    './data/jane-street-market-prediction/features.csv')\n",
    "example_test_df = pd.read_csv(\n",
    "    './data/jane-street-market-prediction/example_test.csv')\n",
    "example_prediction_df = pd.read_csv(\n",
    "    './data/jane-street-market-prediction/example_sample_submission.csv')\n",
    "load_data_end_time = time.time()\n",
    "\n",
    "# 打印数据基本情况\n",
    "print(\"[INFO] {} End Reading ! It took {:.2f} seconds !\".format(\n",
    "    str(datetime.now())[:-4], load_data_end_time-load_data_start_time))\n",
    "print(\"[INFO] {} Basic data description: \".format(str(datetime.now())[:-4]))\n",
    "print(\"    -- train_df shape: {}\".format(\n",
    "    train_df.shape))\n",
    "print(\"    -- example_test_df shape: {}\".format(\n",
    "    example_test_df.shape))\n",
    "print(\"    -- feat_df shape: {}\".format(\n",
    "    feat_df.shape))\n",
    "print(\"    -- example_prediction_df shape: {}\".format(\n",
    "    example_prediction_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] 2021-01-04 20:22:24.10 Autoencoder data prepared !\n"
     ]
    }
   ],
   "source": [
    "# 挑选策略变化之后的数据\n",
    "train = train_df.query('date > 85').reset_index(drop=True)\n",
    "train = train.query('weight > 0').reset_index(drop = True)\n",
    "\n",
    "feature_name_list = [c for c in train.columns if 'feature' in c]\n",
    "resp_name_list = [\"resp\", \"resp_1\", \"resp_2\", \"resp_3\", \"resp_4\"]\n",
    "\n",
    "# 使用均值填充缺失值\n",
    "mean_val_list = []\n",
    "for name in feature_name_list:\n",
    "    mean_val = train[name].mean()\n",
    "    train[name].fillna(mean_val, inplace=True)\n",
    "    mean_val_list.append(mean_val)\n",
    "mean_val_array = np.array(mean_val_list)\n",
    "\n",
    "# 构造自编码器的输入输出\n",
    "X = train[feature_name_list].values\n",
    "y = train[resp_name_list].values\n",
    "\n",
    "train_dates = train[\"date\"].values\n",
    "train_weights = train[\"weight\"].values\n",
    "train_resp = train[\"resp\"].values\n",
    "train_resp_all = train[resp_name_list].values\n",
    "\n",
    "print(\"[INFO] {} Autoencoder data prepared !\".format(\n",
    "    str(datetime.now())[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_test_data(test_df=None, pred_df=None):\n",
    "    \"\"\"测试数据生成器。用于模拟测试数据生成过程，测试模型提交正确性与效率。\"\"\"\n",
    "    n_test = len(test_df)\n",
    "\n",
    "    for i in range(n_test):\n",
    "        yield test_df.iloc[i], pred_df.iloc[i]\n",
    "\n",
    "\n",
    "@jit\n",
    "def njit_fillna(array, values):\n",
    "    \"\"\"利用即时编译（jit）对array数组的NaN值借助values进行填充。\n",
    "\n",
    "    @References:\n",
    "    ----------\n",
    "    [1] https://www.kaggle.com/gogo827jz/optimise-speed-of-filling-nan-function\n",
    "    \"\"\"\n",
    "    if np.isnan(array.sum()):\n",
    "        array = np.where(np.isnan(array), values, array)\n",
    "    return array\n",
    "\n",
    "\n",
    "def custom_metric(dates_array=None,\n",
    "                  weights_array=None,\n",
    "                  resp_array=None,\n",
    "                  action_label_array=None):\n",
    "    \"\"\"依据官方要求的Metric，计算分数。\n",
    "\n",
    "    @References:\n",
    "    ----------\n",
    "    [1] https://www.kaggle.com/c/jane-street-market-prediction/discussion/199107\n",
    "    [2] https://www.kaggle.com/c/jane-street-market-prediction/overview/evaluation\n",
    "    [3] \n",
    "    \"\"\"\n",
    "    tmp_df = pd.DataFrame({\"date\": dates_array,\n",
    "                           \"weight\": weights_array,\n",
    "                           \"resp\": resp_array,\n",
    "                           \"action\": action_label_array})\n",
    "    tmp_df[\"p\"] = tmp_df[\"weight\"]  * tmp_df[\"resp\"] * tmp_df[\"action\"]\n",
    "    # tmp_df = tmp_df.query(\"weight != 0\").reset_index(drop=True)\n",
    "    p_i_val = tmp_df.groupby([\"date\"])[\"p\"].sum().values\n",
    "\n",
    "    n_dates = len(p_i_val)\n",
    "    t = np.sum(p_i_val) / np.sqrt(np.sum(p_i_val ** 2)) * (np.sqrt(250 / n_dates))\n",
    "    return min(max(t, 0), 6) * np.sum(p_i_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"针对带有Group id（组id）数据的时间序列交叉验证集合生成类。\n",
    "\n",
    "    生成针对带有Group id的数据的时序交叉验证集。其中训练与验证的\n",
    "    Group之间可以指定group_gap，用来隔离时间上的关系。这种情况下\n",
    "    group_id通常是时间id，例如天或者小时。\n",
    "\n",
    "    @Parameters:\n",
    "    ----------\n",
    "        n_splits: {int-like}, default=5\n",
    "            切分的集合数目。\n",
    "        max_train_group_size: {int-like}, default=+inf\n",
    "            训练集单个组的最大样本数据限制。\n",
    "        group_gap: {int-like}, default=None\n",
    "            依据group_id切分组时，训练组与测试组的id的gap数目。\n",
    "        max_test_group_size: {int-like}, default=+inf\n",
    "            测试集单个组的最大样本数据限制。\n",
    "\n",
    "    @References:\n",
    "    ----------\n",
    "    [1] https://www.kaggle.com/gogo827jz/jane-street-ffill-xgboost-purgedtimeseriescv\n",
    "    \"\"\"\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self, n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"生成训练组与测试组的id索引，返回组索引的生成器。\n",
    "\n",
    "        @Parameters:\n",
    "        ----------\n",
    "            X: {array-like} {n_samples, n_features}\n",
    "                训练数据，输入形状为{n_samples, n_features}。\n",
    "            y: {array-like} {n_samples, }\n",
    "                标签数据，形状为{n_samples, }。\n",
    "            groups: {array-like} {n_samples, }\n",
    "                用来依据组来划分训练集与测试集的组id，必须为连续的组id。\n",
    "\n",
    "        @Yields:\n",
    "        ----------\n",
    "            train: ndarray\n",
    "                依据group_id切分的训练组id。\n",
    "            test: ndarray\n",
    "                依据group_id切分的测试组id。\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None ！\")\n",
    "\n",
    "        # 初始化基本参数信息\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples, n_splits, group_gap = _num_samples(X), self.n_splits, self.group_gap\n",
    "        n_folds = n_splits + 1\n",
    "\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = np.argsort(ind)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "\n",
    "        # 扫描整个数据id list，构建group_dcit，{group_id: 属于该group的样本的idx}\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds, n_groups))\n",
    "\n",
    "        # group_test_size: 每个fold预留的test group的大小\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array, test_array = [], []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "            test_array  = test_array[group_gap:]\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]\n",
    "\n",
    "\n",
    "def test_purged_group_time_series_split():\n",
    "    X = train_df.query('date > 85').reset_index(drop=True)[[\"ts_id\", \"feature_0\"]].values\n",
    "    y = train_df.query('date > 85').reset_index(drop=True)[\"resp\"].values\n",
    "    groups = train_df.query('date > 85').reset_index(drop=True)[\"date\"].values\n",
    "\n",
    "    group_ts_kfolds = PurgedGroupTimeSeriesSplit(\n",
    "        n_splits=4, group_gap=31, max_test_group_size=31)\n",
    "    train_idx, valid_idx = [], []\n",
    "    for train_idx_tmp, valid_idx_tmp in group_ts_kfolds.split(X=X, y=y, groups=groups):\n",
    "        train_idx.append(train_idx_tmp)\n",
    "        valid_idx.append(valid_idx_tmp)\n",
    "\n",
    "    print([[train_df.iloc[min(item)][\"date\"].astype(int),\n",
    "            train_df.iloc[max(item)][\"date\"].astype(int)] for item in train_idx])\n",
    "    print([[train_df.iloc[min(item)][\"date\"].astype(int),\n",
    "            train_df.iloc[max(item)][\"date\"].astype(int)] for item in valid_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] 2021-01-04 20:22:29.14 Build autoencoder successed !\n"
     ]
    }
   ],
   "source": [
    "def build_tabular_autoencoder(verbose=False, is_compile=True,\n",
    "                              stddev=0.05, **kwargs):\n",
    "    \"\"\"降噪自编码器实现。针对表格形式数据的降噪自编码器，噪声等级由高斯噪声的stddev参数指定\"\"\"\n",
    "    input_dim = kwargs.pop(\"input_dim\", None)\n",
    "    n_outputs = kwargs.pop(\"n_outputs\", None)\n",
    "\n",
    "    # 构建降噪自编码器\n",
    "    layer_input = layers.Input(input_dim, dtype='float32')\n",
    "\n",
    "    layer_encoded = layers.BatchNormalization()(layer_input)\n",
    "    layer_encoded = layers.GaussianNoise(stddev=stddev)(layer_encoded)\n",
    "    layer_encoded = layers.Dense(256, activation='relu')(layer_encoded)\n",
    "\n",
    "    # 解码层1：针对输入的重构\n",
    "    layer_decoded = layers.Dropout(0.2)(layer_encoded)\n",
    "    layer_decoded = layers.Dense(input_dim, name='reconstruct_output')(layer_decoded)\n",
    "\n",
    "    # 解码层2：针对resp的重构\n",
    "    layer_output = layers.Dense(128, activation='relu')(layer_decoded)\n",
    "    layer_output = layers.BatchNormalization()(layer_output)\n",
    "    layer_output = layers.Dropout(0.2)(layer_output)\n",
    "    layer_output = layers.Dense(n_outputs, activation='sigmoid', \n",
    "                                name='label_output')(layer_output)\n",
    "\n",
    "    # 输出层\n",
    "    encoder_model = models.Model(inputs=layer_input, outputs=layer_decoded)\n",
    "    autoencoder_model = models.Model(inputs=layer_input, outputs=[layer_decoded, layer_output])\n",
    "\n",
    "    if verbose:\n",
    "        autoencoder_model.summary()\n",
    "    if is_compile:\n",
    "        autoencoder_model.compile(loss={'reconstruct_output':'mae', 'label_output':'mae'}, optimizer=optimizers.Adam(0.003))\n",
    "    return encoder_model, autoencoder_model\n",
    "\n",
    "encoder_model, autoencoder_model = build_tabular_autoencoder(\n",
    "    input_dim=X.shape[1], n_outputs=y.shape[1], stddev=0.2, verbose=False)\n",
    "\n",
    "print(\"[INFO] {} Build autoencoder successed !\".format(\n",
    "    str(datetime.now())[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 1414273 samples, validate on 157142 samples\n",
      "Epoch 1/15\n",
      "1414273/1414273 - 10s - loss: 0.3973 - reconstruct_output_loss: 0.3861 - label_output_loss: 0.0111 - val_loss: 0.2360 - val_reconstruct_output_loss: 0.2246 - val_label_output_loss: 0.0113\n",
      "Epoch 2/15\n",
      "1414273/1414273 - 10s - loss: 0.3896 - reconstruct_output_loss: 0.3786 - label_output_loss: 0.0111 - val_loss: 0.2318 - val_reconstruct_output_loss: 0.2204 - val_label_output_loss: 0.0113\n",
      "Epoch 3/15\n",
      "1414273/1414273 - 10s - loss: 0.3825 - reconstruct_output_loss: 0.3716 - label_output_loss: 0.0111 - val_loss: 0.2293 - val_reconstruct_output_loss: 0.2179 - val_label_output_loss: 0.0113\n",
      "Epoch 4/15\n",
      "1414273/1414273 - 10s - loss: 0.3766 - reconstruct_output_loss: 0.3655 - label_output_loss: 0.0110 - val_loss: 0.2324 - val_reconstruct_output_loss: 0.2209 - val_label_output_loss: 0.0113\n",
      "Epoch 5/15\n",
      "1414273/1414273 - 10s - loss: 0.3693 - reconstruct_output_loss: 0.3583 - label_output_loss: 0.0110 - val_loss: 0.2218 - val_reconstruct_output_loss: 0.2103 - val_label_output_loss: 0.0113\n",
      "Epoch 6/15\n",
      "1414273/1414273 - 10s - loss: 0.3633 - reconstruct_output_loss: 0.3523 - label_output_loss: 0.0111 - val_loss: 0.2186 - val_reconstruct_output_loss: 0.2072 - val_label_output_loss: 0.0113\n",
      "Epoch 7/15\n",
      "1414273/1414273 - 10s - loss: 0.3567 - reconstruct_output_loss: 0.3456 - label_output_loss: 0.0110 - val_loss: 0.2233 - val_reconstruct_output_loss: 0.2118 - val_label_output_loss: 0.0113\n",
      "Epoch 8/15\n",
      "1414273/1414273 - 9s - loss: 0.3521 - reconstruct_output_loss: 0.3411 - label_output_loss: 0.0110 - val_loss: 0.2114 - val_reconstruct_output_loss: 0.1999 - val_label_output_loss: 0.0113\n",
      "Epoch 9/15\n",
      "1414273/1414273 - 10s - loss: 0.3477 - reconstruct_output_loss: 0.3367 - label_output_loss: 0.0110 - val_loss: 0.2177 - val_reconstruct_output_loss: 0.2062 - val_label_output_loss: 0.0113\n",
      "Epoch 10/15\n",
      "1414273/1414273 - 9s - loss: 0.3448 - reconstruct_output_loss: 0.3338 - label_output_loss: 0.0110 - val_loss: 0.2226 - val_reconstruct_output_loss: 0.2111 - val_label_output_loss: 0.0113\n",
      "Epoch 11/15\n",
      "1414273/1414273 - 10s - loss: 0.3405 - reconstruct_output_loss: 0.3295 - label_output_loss: 0.0110 - val_loss: 0.2083 - val_reconstruct_output_loss: 0.1968 - val_label_output_loss: 0.0113\n",
      "Epoch 12/15\n",
      "1414273/1414273 - 11s - loss: 0.3367 - reconstruct_output_loss: 0.3257 - label_output_loss: 0.0110 - val_loss: 0.2098 - val_reconstruct_output_loss: 0.1983 - val_label_output_loss: 0.0113\n",
      "Epoch 13/15\n",
      "1414273/1414273 - 10s - loss: 0.3336 - reconstruct_output_loss: 0.3225 - label_output_loss: 0.0110 - val_loss: 0.2020 - val_reconstruct_output_loss: 0.1905 - val_label_output_loss: 0.0113\n",
      "Epoch 14/15\n",
      "1414273/1414273 - 10s - loss: 0.3312 - reconstruct_output_loss: 0.3202 - label_output_loss: 0.0110 - val_loss: 0.2144 - val_reconstruct_output_loss: 0.2030 - val_label_output_loss: 0.0113\n",
      "Epoch 15/15\n",
      "1414273/1414273 - 10s - loss: 0.3292 - reconstruct_output_loss: 0.3181 - label_output_loss: 0.0110 - val_loss: 0.2076 - val_reconstruct_output_loss: 0.1961 - val_label_output_loss: 0.0113\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe6ba7e0450>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "early_stop = callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\",\n",
    "                                     verbose=1, patience=40,\n",
    "                                     restore_best_weights=True)\n",
    "autoencoder_model.fit(x=X, y=[X, y],\n",
    "                      batch_size=32768,\n",
    "                      epochs=15,\n",
    "                      verbose=2,\n",
    "                      validation_split=0.1,\n",
    "                      callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] 2021-01-04 21:18:40.89 Build MLP Model successed !\n"
     ]
    }
   ],
   "source": [
    "def utility_score_loss(y_true, y_pred):\n",
    "    \"\"\"带有正则项的MAE LOSS，参考自项目[1]。\n",
    "\n",
    "    @References:\n",
    "    ----------\n",
    "    [1] https://www.kaggle.com/satorushibata/lightgbm-classifier-pca-logit-on-utility-score#Plot-&-Summaries\n",
    "    \"\"\"\n",
    "    regularization_val = 0.01\n",
    "\n",
    "    residual = (y_true - y_pred)\n",
    "    sign_matching = (y_true * y_pred) >= 0\n",
    "    utility_mse_loss = tf.where(sign_matching,\n",
    "                                tf.square(residual),\n",
    "                                tf.square(residual) - y_true * y_pred * regularization_val)\n",
    "    utility_mse_loss = tf.reduce_mean(utility_mse_loss, axis=-1)\n",
    "    return utility_mse_loss\n",
    "\n",
    "\n",
    "def build_model(verbose=False, is_compile=True, encoder_model=None, **kwargs):\n",
    "    \"\"\"针对二分类任务的MLP模型，使用自编码器的编码层作为预训练层。\"\"\"\n",
    "    input_dim = kwargs.pop(\"input_dim\", None)\n",
    "    output_dim = kwargs.pop(\"output_dim\", None)\n",
    "    n_labels = kwargs.pop(\"n_labels\", None)\n",
    "\n",
    "    # 构造网络结构\n",
    "    layer_input = layers.Input(input_dim, dtype='float32')\n",
    "    layer_encoded = encoder_model(layer_input)\n",
    "\n",
    "    layer_feats = layers.concatenate([layer_input, layer_encoded])\n",
    "    layer_feats = layers.BatchNormalization()(layer_feats)\n",
    "\n",
    "    # 特征抽取\n",
    "    layer_dense = layers.Dense(64, activation=\"relu\")(layer_feats)\n",
    "    layer_dense = layers.BatchNormalization()(layer_dense)\n",
    "    layer_dense = layers.Dropout(0.2)(layer_dense)\n",
    "\n",
    "    layer_dense = layers.Dense(32, activation=\"relu\")(layer_dense)\n",
    "    layer_dense = layers.BatchNormalization()(layer_dense)\n",
    "    layer_dense = layers.Dropout(0.2)(layer_dense)\n",
    "\n",
    "    # 输出层构造与模型构造\n",
    "    layer_output = layers.Dense(output_dim, activation='sigmoid', name=\"label_output\")(layer_dense)\n",
    "    model = models.Model(layer_input, layer_output)\n",
    "\n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    if is_compile:\n",
    "        model.compile(loss=utility_score_loss,\n",
    "                      metrics=[\"mae\"],\n",
    "                      optimizer=optimizers.Adam(0.003))\n",
    "    return model\n",
    "\n",
    "def test_build_model():\n",
    "    # 构造mlp模型\n",
    "    mlp_model = build_model(verbose=False, encoder_model=encoder_model,\n",
    "                            input_dim=X.shape[1], output_dim=y.shape[1])\n",
    "\n",
    "def test_utility_score_loss():\n",
    "    X_true = X[:20, :5]\n",
    "    X_pred = X[50:70, :5]\n",
    "\n",
    "    print(utility_score_loss(X_true, X_pred))\n",
    "    print(utility_score_loss(X_true, X_pred).shape)\n",
    "\n",
    "print(\"[INFO] {} Build MLP Model successed !\".format(\n",
    "    str(datetime.now())[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] 2021-01-04 21:14:42.60 Model training start:\n",
      "=========================================\n",
      "-- folds 0, valid_custom: 43.4213\n",
      "-- folds 1, valid_custom: -0.0000\n",
      "-- folds 2, valid_custom: -0.0000\n",
      "-- folds 3, valid_custom: 4.6847\n",
      "-- folds 4, valid_custom: -0.0000\n",
      "-- folds 5, valid_custom: -0.0000\n",
      "-- total metric, valid_custom: 8.0177\n",
      "=========================================\n",
      "[INFO] 2021-01-04 21:17:16.03 Model training end\n"
     ]
    }
   ],
   "source": [
    "# 训练前全局准备\n",
    "MODELS = []\n",
    "encoder_model.trainable = False\n",
    "early_stop = callbacks.EarlyStopping(monitor=\"val_mae\", mode=\"min\",\n",
    "                                     verbose=1, patience=30,\n",
    "                                     restore_best_weights=True)\n",
    "group_ts_kfolds = PurgedGroupTimeSeriesSplit(\n",
    "        n_splits=6, group_gap=20)\n",
    "min_date = min(train_dates)\n",
    "train_dates = train_dates - min_date\n",
    "\n",
    "# 开始训练模型\n",
    "valid_acc_total, valid_roc_auc_total, valid_custom_total = [], [], []\n",
    "\n",
    "print(\"[INFO] {} Model training start:\".format(str(datetime.now())[:-4]))\n",
    "print(\"=========================================\")\n",
    "for fold, (train_idx, valid_idx) in enumerate(group_ts_kfolds.split(X=X, y=y, groups=train_dates)):\n",
    "    X_train, X_val = X[train_idx], X[valid_idx]\n",
    "    y_train, y_val = y[train_idx], y[valid_idx]\n",
    "\n",
    "    X_train_weight, X_val_weight = train_weights[train_idx], train_weights[valid_idx]\n",
    "    X_train_resp, X_val_resp = train_resp[train_idx], train_resp[valid_idx]\n",
    "    X_train_dates, X_val_dates = train_dates[train_idx], train_dates[valid_idx]\n",
    "\n",
    "    # 准备模型\n",
    "    mlp_model = build_model(verbose=False, encoder_model=encoder_model,\n",
    "                            input_dim=X_train.shape[1], output_dim=y_train.shape[1])\n",
    "\n",
    "    mlp_model.fit(x=X_train, y=y_train,\n",
    "                  validation_data=(X_val, y_val),\n",
    "                  batch_size=32768,\n",
    "                  epochs=20,\n",
    "                  verbose=0,\n",
    "                  callbacks=[early_stop])\n",
    "\n",
    "    # valid预测结果\n",
    "    valid_pred_res = mlp_model.predict(X_val)\n",
    "    valid_pred_label_mat = np.where(\n",
    "            valid_pred_res>=0, 1, 0).astype(int)\n",
    "\n",
    "    # valid data上按照官方metric进行结果评估\n",
    "    valid_pred_label = np.where(\n",
    "            np.mean(valid_pred_res, axis=1)>=0, 1, 0).astype(int)\n",
    "    valid_custom_metric = custom_metric(dates_array=X_val_dates,\n",
    "                                        weights_array=X_val_weight,\n",
    "                                        action_label_array=valid_pred_label,\n",
    "                                        resp_array=X_val_resp)\n",
    "\n",
    "    # Accuracy与ROC AUC进行评估\n",
    "    # valid_acc = accuracy_score(valid_pred_label_mat, y_val)\n",
    "\n",
    "    # valid_roc_auc_list = []\n",
    "    # for i in range(y_val.shape[1]):\n",
    "    #     valid_pred_tmp = valid_pred_proba[:, i].reshape(-1, 1)\n",
    "    #     y_val_tmp = y_val[:, i].reshape(-1, 1)\n",
    "\n",
    "    #     valid_roc_auc_list.append(roc_auc_score(y_val_tmp, valid_pred_tmp))\n",
    "    # valid_roc_auc = np.mean(valid_roc_auc_list)\n",
    "\n",
    "    # 标准打印训练信息\n",
    "    print(\"-- folds {}, valid_custom: {:.4f}\".format(\n",
    "            fold, valid_custom_metric))\n",
    "\n",
    "    # 保存模型与关键训练指标\n",
    "    MODELS.append(mlp_model)\n",
    "    valid_custom_total.append(valid_custom_metric)\n",
    "\n",
    "# 打印总体分数指标\n",
    "print(\"-- total metric, valid_custom: {:.4f}\".format(np.mean(valid_custom_total)))\n",
    "\n",
    "print(\"=========================================\")\n",
    "print(\"[INFO] {} Model training end\".format(str(datetime.now())[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}