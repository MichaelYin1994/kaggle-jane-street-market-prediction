{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "import collections\n",
    "from six import with_metaclass\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from datetime import datetime\n",
    "\n",
    "from numba import njit, jit\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import iinfo, finfo, int8, int16, int32, int64, float32, float64\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import activations\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import permutation_test_score\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from IPython.display import HTML\n",
    "\n",
    "# 设置plotly为暗黑模式\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "plot_config = dict({'scrollZoom': True, 'displayModeBar': True, 'displaylogo': False})\n",
    "sns.set(style=\"ticks\", font_scale=1.2, palette='deep', color_codes=True)\n",
    "colors = [\"C\" + str(i) for i in range(0, 9+1)]\n",
    "\n",
    "# 默认plotly色号\n",
    "default_color_list = [\n",
    "    '#1f77b4',  # muted blue\n",
    "    '#ff7f0e',  # safety orange\n",
    "    '#2ca02c',  # cooked asparagus green\n",
    "    '#d62728',  # brick red\n",
    "    '#9467bd',  # muted purple\n",
    "    '#8c564b',  # chestnut brown\n",
    "    '#e377c2',  # raspberry yogurt pink\n",
    "    '#7f7f7f',  # middle gray\n",
    "    '#bcbd22',  # curry yellow-green\n",
    "    '#17becf'   # blue-teal\n",
    "    ]\n",
    "\n",
    "# 设定全局随机种子，并且屏蔽warnings\n",
    "GLOBAL_RANDOM_SEED = 2022\n",
    "np.random.seed(GLOBAL_RANDOM_SEED)\n",
    "tf.random.set_seed(GLOBAL_RANDOM_SEED)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "# 检查GPU设备情况\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # 限制只能使用第一块GPU（通过GPU的List的id指定）\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] 2021-01-14 11:44:44.32 End Reading ! It took 71.04 seconds !\n[INFO] 2021-01-14 11:44:44.32 Basic data description: \n    -- train_df shape: (2390491, 138)\n    -- example_test_df shape: (15219, 133)\n    -- feat_df shape: (130, 30)\n    -- example_prediction_df shape: (15219, 2)\n"
     ]
    }
   ],
   "source": [
    "# 导入数据\n",
    "load_data_start_time = time.time()\n",
    "train_df  = pd.read_csv(\n",
    "    './data/jane-street-market-prediction/train.csv', nrows=None)\n",
    "feat_df = pd.read_csv(\n",
    "    './data/jane-street-market-prediction/features.csv')\n",
    "example_test_df = pd.read_csv(\n",
    "    './data/jane-street-market-prediction/example_test.csv')\n",
    "example_prediction_df = pd.read_csv(\n",
    "    './data/jane-street-market-prediction/example_sample_submission.csv')\n",
    "load_data_end_time = time.time()\n",
    "\n",
    "# 打印数据基本情况\n",
    "print(\"[INFO] {} End Reading ! It took {:.2f} seconds !\".format(\n",
    "    str(datetime.now())[:-4], load_data_end_time-load_data_start_time))\n",
    "print(\"[INFO] {} Basic data description: \".format(str(datetime.now())[:-4]))\n",
    "print(\"    -- train_df shape: {}\".format(\n",
    "    train_df.shape))\n",
    "print(\"    -- example_test_df shape: {}\".format(\n",
    "    example_test_df.shape))\n",
    "print(\"    -- feat_df shape: {}\".format(\n",
    "    feat_df.shape))\n",
    "print(\"    -- example_prediction_df shape: {}\".format(\n",
    "    example_prediction_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_test_data(test_df=None, pred_df=None):\n",
    "    \"\"\"测试数据生成器。用于模拟测试数据生成过程，测试模型提交正确性与效率。\"\"\"\n",
    "    n_test = len(test_df)\n",
    "\n",
    "    for i in range(n_test):\n",
    "        yield test_df.iloc[i], pred_df.iloc[i]\n",
    "\n",
    "\n",
    "@jit\n",
    "def njit_fillna(array, values):\n",
    "    \"\"\"利用即时编译（jit）对array数组的NaN值借助values进行填充。\n",
    "\n",
    "    @References:\n",
    "    ----------\n",
    "    [1] https://www.kaggle.com/gogo827jz/optimise-speed-of-filling-nan-function\n",
    "    \"\"\"\n",
    "    if np.isnan(array.sum()):\n",
    "        array = np.where(np.isnan(array), values, array)\n",
    "    return array\n",
    "\n",
    "\n",
    "def custom_metric(dates_array=None,\n",
    "                  weights_array=None,\n",
    "                  resp_array=None,\n",
    "                  action_label_array=None):\n",
    "    \"\"\"依据官方要求的Metric，计算分数。\n",
    "\n",
    "    @References:\n",
    "    ----------\n",
    "    [1] https://www.kaggle.com/c/jane-street-market-prediction/discussion/199107\n",
    "    [2] https://www.kaggle.com/c/jane-street-market-prediction/overview/evaluation\n",
    "    [3] \n",
    "    \"\"\"\n",
    "    tmp_df = pd.DataFrame({\"date\": dates_array,\n",
    "                           \"weight\": weights_array,\n",
    "                           \"resp\": resp_array,\n",
    "                           \"action\": action_label_array})\n",
    "    tmp_df[\"p\"] = tmp_df[\"weight\"]  * tmp_df[\"resp\"] * tmp_df[\"action\"]\n",
    "    # tmp_df = tmp_df.query(\"weight != 0\").reset_index(drop=True)\n",
    "    p_i_val = tmp_df.groupby([\"date\"])[\"p\"].sum().values\n",
    "\n",
    "    n_dates = len(p_i_val)\n",
    "    t = np.sum(p_i_val) / np.sqrt(np.sum(p_i_val ** 2)) * (np.sqrt(250 / n_dates))\n",
    "    return min(max(t, 0), 6) * np.sum(p_i_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"针对带有Group id（组id）数据的时间序列交叉验证集合生成类。\n",
    "\n",
    "    生成针对带有Group id的数据的时序交叉验证集。其中训练与验证的\n",
    "    Group之间可以指定group_gap，用来隔离时间上的关系。这种情况下\n",
    "    group_id通常是时间id，例如天或者小时。\n",
    "\n",
    "    @Parameters:\n",
    "    ----------\n",
    "        n_splits: {int-like}, default=5\n",
    "            切分的集合数目。\n",
    "        max_train_group_size: {int-like}, default=+inf\n",
    "            训练集单个组的最大样本数据限制。\n",
    "        group_gap: {int-like}, default=None\n",
    "            依据group_id切分组时，训练组与测试组的id的gap数目。\n",
    "        max_test_group_size: {int-like}, default=+inf\n",
    "            测试集单个组的最大样本数据限制。\n",
    "\n",
    "    @References:\n",
    "    ----------\n",
    "    [1] https://www.kaggle.com/gogo827jz/jane-street-ffill-xgboost-purgedtimeseriescv\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self, n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"生成训练组与测试组的id索引，返回组索引的生成器。\n",
    "\n",
    "        @Parameters:\n",
    "        ----------\n",
    "            X: {array-like} {n_samples, n_features}\n",
    "                训练数据，输入形状为{n_samples, n_features}。\n",
    "            y: {array-like} {n_samples, }\n",
    "                标签数据，形状为{n_samples, }。\n",
    "            groups: {array-like} {n_samples, }\n",
    "                用来依据组来划分训练集与测试集的组id，必须为连续的组id。\n",
    "\n",
    "        @Yields:\n",
    "        ----------\n",
    "            train: ndarray\n",
    "                依据group_id切分的训练组id。\n",
    "            test: ndarray\n",
    "                依据group_id切分的测试组id。\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None ！\")\n",
    "        for i in range(1, len(groups)):\n",
    "            if groups[i] < groups[i-1]:\n",
    "                raise ValueError(\"groups must be a monotone increasing sequence !\")\n",
    "\n",
    "        # 初始化基本参数信息\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples, n_splits, group_gap = X.shape[0], self.n_splits, self.group_gap\n",
    "        n_folds = n_splits + 1\n",
    "\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "\n",
    "        # 使得groups的id取值从0顺序开始（假定groups是递增的）\n",
    "        groups_reid, groupid2reid, index_tmp = [], {}, -1\n",
    "        for _, item in enumerate(groups):\n",
    "            if item not in groupid2reid:\n",
    "                index_tmp += 1\n",
    "                groupid2reid[item] = index_tmp\n",
    "            groups_reid.append(index_tmp)\n",
    "\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups_reid, return_index=True)\n",
    "        unique_groups = np.argsort(ind)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "\n",
    "        # 扫描整个数据id list，构建group_dcit，{group_id: 属于该group的样本的idx}\n",
    "        for idx in np.arange(n_samples):\n",
    "            if groups_reid[idx] in group_dict:\n",
    "                group_dict[groups_reid[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups_reid[idx]] = [idx]\n",
    "\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds, n_groups))\n",
    "\n",
    "        # group_test_size: 每个fold预留的test group的大小\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array, test_array = [], []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                train_array = np.sort(np.unique(\n",
    "                    np.concatenate((train_array, train_array_tmp)),\n",
    "                    axis=None), axis=None)\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                    np.concatenate((test_array, test_array_tmp)),\n",
    "                    axis=None), axis=None)\n",
    "            test_array  = test_array[group_gap:]\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]\n",
    "\n",
    "\n",
    "def test_purged_group_time_series_split():\n",
    "    X = train_df.query('date > 85').reset_index(drop=True)[[\"ts_id\", \"feature_0\"]].values\n",
    "    y = train_df.query('date > 85').reset_index(drop=True)[\"resp\"].values\n",
    "    groups = train_df.query('date > 85').reset_index(drop=True)[\"date\"].values\n",
    "\n",
    "    group_ts_kfolds = PurgedGroupTimeSeriesSplit(\n",
    "        n_splits=7, group_gap=20, max_train_group_size=80, max_test_group_size=60)\n",
    "    train_idx, valid_idx = [], []\n",
    "    for train_idx_tmp, valid_idx_tmp in group_ts_kfolds.split(X=X, y=y, groups=groups):\n",
    "        train_idx.append(train_idx_tmp)\n",
    "        valid_idx.append(valid_idx_tmp)\n",
    "\n",
    "        print(\"train range: {}, valid range: {}\".format(\n",
    "            [min(groups[train_idx_tmp]), max(groups[train_idx_tmp])],\n",
    "            [min(groups[valid_idx_tmp]), max(groups[valid_idx_tmp])]\n",
    "        ))\n",
    "\n",
    "# test_purged_group_time_series_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiteModel:\n",
    "    \"\"\"将模型转换为Tensorflow Lite模型，提升推理速度。目前仅支持Keras模型转换。\n",
    "\n",
    "    @Attributes:\n",
    "    ----------\n",
    "    interpreter: {Tensorflow lite transformed object}\n",
    "        利用tf.lite.interpreter转换后的Keras模型。\n",
    "\n",
    "    @References:\n",
    "    ----------\n",
    "    [1] https://medium.com/@micwurm/using-tensorflow-lite-to-speed-up-predictions-a3954886eb98\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, model_path):\n",
    "        \"\"\"类方法。用于model_path下的模型，一般为*.h5模型。\"\"\"\n",
    "        return LiteModel(tf.lite.Interpreter(model_path=model_path))\n",
    "\n",
    "    @classmethod\n",
    "    def from_keras_model(cls, kmodel):\n",
    "        \"\"\"类方法。用于直接转换keras模型。不用实例化类可直接调用该方法，返回\n",
    "        被转换为tf.lite形式的Keras模型。\n",
    "\n",
    "        @Attributes:\n",
    "        ----------\n",
    "        kmodel: {tf.keras model}\n",
    "            待转换的Keras模型。\n",
    "\n",
    "        @Returens:\n",
    "        ----------\n",
    "        经过转换的Keras模型。\n",
    "        \"\"\"\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(kmodel)\n",
    "        tflite_model = converter.convert()\n",
    "        return LiteModel(tf.lite.Interpreter(model_content=tflite_model))\n",
    "\n",
    "    def __init__(self, interpreter):\n",
    "        \"\"\"为经过tf.lite.interpreter转换的模型构建构造输入输出的关键参数。\n",
    "\n",
    "        @TODO(zhuoyin94@163.com):\n",
    "        ----------\n",
    "        [1] 可添加关键字，指定converter选择采用INT8量化还是混合精度量化。\n",
    "        [2] 可添加关键字，指定converter选择量化的方式：低延迟还是高推理速度？\n",
    "        \"\"\"\n",
    "        self.interpreter = interpreter\n",
    "        self.interpreter.allocate_tensors()\n",
    "\n",
    "        input_det = self.interpreter.get_input_details()[0]\n",
    "        output_det = self.interpreter.get_output_details()[0]\n",
    "        self.input_index = input_det[\"index\"]\n",
    "        self.output_index = output_det[\"index\"]\n",
    "        self.input_shape = input_det[\"shape\"]\n",
    "        self.output_shape = output_det[\"shape\"]\n",
    "        self.input_dtype = input_det[\"dtype\"]\n",
    "        self.output_dtype = output_det[\"dtype\"]\n",
    "\n",
    "    def predict(self, inp):\n",
    "        inp = inp.astype(self.input_dtype)\n",
    "        count = inp.shape[0]\n",
    "        out = np.zeros((count, self.output_shape[1]), dtype=self.output_dtype)\n",
    "        for i in range(count):\n",
    "            self.interpreter.set_tensor(self.input_index, inp[i:i+1])\n",
    "            self.interpreter.invoke()\n",
    "            out[i] = self.interpreter.get_tensor(self.output_index)[0]\n",
    "        return out\n",
    "\n",
    "    def predict_single(self, inp):\n",
    "        \"\"\" Like predict(), but only for a single record. The input data can be a Python list. \"\"\"\n",
    "        inp = np.array([inp], dtype=self.input_dtype)\n",
    "        self.interpreter.set_tensor(self.input_index, inp)\n",
    "        self.interpreter.invoke()\n",
    "        out = self.interpreter.get_tensor(self.output_index)\n",
    "        return out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] 2021-01-14 11:44:47.86 Data prepared !\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "数据预处理部分。包括标签生成、数据统计值获取。\n",
    "\"\"\"\n",
    "# 挑选策略变化之后的数据\n",
    "train = train_df.query('date > 85').reset_index(drop=True)\n",
    "train = train.query('weight > 0').reset_index(drop = True)\n",
    "\n",
    "target_threshold = 0.00001\n",
    "\n",
    "# 构造标签\n",
    "train['action'] =  ((train['resp_1'] > target_threshold) & \\\n",
    "                    (train['resp_2'] > target_threshold) & \\\n",
    "                    (train['resp_3'] > target_threshold) & \\\n",
    "                    (train['resp_4'] > target_threshold) &  \\\n",
    "                    (train['resp'] > target_threshold)).astype('int')\n",
    "feature_name_list = [c for c in train.columns if 'feature' in c]\n",
    "resp_name_list = [\"resp\", \"resp_1\", \"resp_2\", \"resp_3\", \"resp_4\"]\n",
    "\n",
    "# 使用均值填充缺失值\n",
    "mean_val_list = []\n",
    "for name in feature_name_list:\n",
    "    mean_val = train[name].mean()\n",
    "    train[name].fillna(mean_val, inplace=True)\n",
    "    mean_val_list.append(mean_val)\n",
    "mean_val_array = np.array(mean_val_list)\n",
    "\n",
    "# 构造后续神经网络模型的输入输出\n",
    "X = train[feature_name_list].values.astype(\"float32\")\n",
    "y = np.hstack([(train[c] > target_threshold).astype(\n",
    "    'int').values.reshape(-1, 1) for c in resp_name_list])\n",
    "\n",
    "train_dates = train[\"date\"].values\n",
    "train_weights = train[\"weight\"].values\n",
    "train_resp = train[\"resp\"].values\n",
    "\n",
    "print(\"[INFO] {} Data prepared !\".format(\n",
    "    str(datetime.now())[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] 2021-01-14 11:44:47.96 Tools prepared !\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "特征工程辅助工具。\n",
    "\"\"\"\n",
    "def build_tabular_autoencoder(verbose=False, is_compile=True,\n",
    "                              stddev=0.05, **kwargs):\n",
    "    \"\"\"降噪自编码器实现。针对表格形式数据的降噪自编码器，噪声等级由高斯噪声的stddev参数指定\"\"\"\n",
    "    input_dim = kwargs.pop(\"input_dim\", None)\n",
    "    n_labels = kwargs.pop(\"n_labels\", None)\n",
    "\n",
    "    # 构建降噪自编码器\n",
    "    layer_input = layers.Input(input_dim, dtype='float32')\n",
    "\n",
    "    layer_encoded = layers.BatchNormalization()(layer_input)\n",
    "    layer_encoded = layers.GaussianNoise(stddev=stddev)(layer_encoded)\n",
    "    layer_encoded = layers.Dense(256, activation='relu')(layer_encoded)\n",
    "\n",
    "    # 解码层1：针对输入的重构\n",
    "    layer_decoded = layers.Dropout(0.3)(layer_encoded)\n",
    "    layer_decoded = layers.Dense(input_dim, name='reconstruct_output')(layer_decoded)\n",
    "\n",
    "    # 解码层2：针对resp的重构\n",
    "    layer_output = layers.Dense(128, activation='relu')(layer_decoded)\n",
    "    layer_output = layers.BatchNormalization()(layer_output)\n",
    "    layer_output = layers.Dropout(0.3)(layer_output)\n",
    "\n",
    "    layer_output_labels = layers.Dense(n_labels, activation='sigmoid', name='layer_output_labels')(layer_output)\n",
    "\n",
    "    # 输出层\n",
    "    encoder_model = models.Model(inputs=layer_input, outputs=layer_decoded)\n",
    "    autoencoder_model = models.Model(inputs=layer_input, outputs=[layer_decoded, layer_output_labels])\n",
    "\n",
    "    if verbose:\n",
    "        autoencoder_model.summary()\n",
    "    if is_compile:\n",
    "        autoencoder_model.compile(loss={'reconstruct_output': 'mse', 'layer_output_labels':'binary_crossentropy'},\n",
    "                                  metrics={'layer_output_labels':'acc'}, \n",
    "                                  optimizer=optimizers.Adam(0.001))\n",
    "    return encoder_model, autoencoder_model\n",
    "\n",
    "\n",
    "def feat_pca(feat_array=None, n_dims=30):\n",
    "    \"\"\"利用PCA，将feat_array降维至n_dims维度。\"\"\"\n",
    "    if feat_array.shape[1] <= n_dims:\n",
    "        raise ValueError(\"n_dims must smaller than the dim of feat_array !\")\n",
    "\n",
    "    # 归一化feat_array\n",
    "    X_sc = StandardScaler()\n",
    "    X_sc.fit(feat_array)\n",
    "    feat_array = X_sc.transform(feat_array)\n",
    "\n",
    "    # 降维\n",
    "    pca = PCA(n_components=n_dims)\n",
    "    pca.fit(feat_array)\n",
    "    feat_array_pca = pca.transform(feat_array)\n",
    "\n",
    "    return X_sc, pca, feat_array_pca\n",
    "\n",
    "\n",
    "@njit\n",
    "def njit_search_best_thresold_acc(y_pred_proba, y_true):\n",
    "    \"\"\"通过阈值搜索最优的准确率切分阈值.\"\"\"\n",
    "    best_acc, best_threshold = 0, 0\n",
    "    for threshold in range(4500, 5800):\n",
    "        thresold_tmp = threshold / 10000\n",
    "        y_pred_label = np.where(y_pred_proba > thresold_tmp, 1, 0)\n",
    "        score_tmp = np.sum(np.where(y_true == y_pred_label, 1, 0)) / len(y_true)\n",
    "\n",
    "        if score_tmp > best_acc:\n",
    "            best_acc = score_tmp\n",
    "            best_threshold = thresold_tmp\n",
    "    return best_acc, best_threshold\n",
    "\n",
    "\n",
    "@njit\n",
    "def njit_custom_metric(dates_array=None,\n",
    "                       weights_array=None,\n",
    "                       resp_array=None,\n",
    "                       action_label_array=None):\n",
    "    \"\"\"利用njit装饰器与numpy来计算Kaggle官方要求的Metric。\n",
    "\n",
    "    @References:\n",
    "    ----------\n",
    "    [1] https://www.kaggle.com/c/jane-street-market-prediction/discussion/199107\n",
    "    [2] https://www.kaggle.com/c/jane-street-market-prediction/overview/evaluation\n",
    "    \"\"\"\n",
    "    p_array = weights_array * resp_array * action_label_array\n",
    "\n",
    "    n_unique_dates = np.max(dates_array) - np.min(dates_array) + 1\n",
    "    dates_array = dates_array - np.min(dates_array)\n",
    "\n",
    "    p_i_val = np.zeros((n_unique_dates, ))\n",
    "    for ind, item in enumerate(dates_array):\n",
    "        p_i_val[item] += p_array[ind]\n",
    "\n",
    "    t = np.sum(p_i_val) / np.sqrt(np.sum(p_i_val ** 2)) * (np.sqrt(250 / n_unique_dates))\n",
    "    return min(max(t, 0), 6) * np.sum(p_i_val)\n",
    "\n",
    "\n",
    "@njit\n",
    "def njit_search_best_thresold_custom(y_pred_proba=None,\n",
    "                                     dates_array=None,\n",
    "                                     weights_array=None,\n",
    "                                     resp_array=None):\n",
    "    \"\"\"通过阈值搜索最优的kaggle官方评分的切分阈值.\"\"\"\n",
    "    best_acc, best_threshold = 0, 0\n",
    "    for threshold in range(4500, 5800):\n",
    "        thresold_tmp = threshold / 10000\n",
    "        y_pred_label = np.where(y_pred_proba > thresold_tmp, 1, 0)\n",
    "        score_tmp = njit_custom_metric(dates_array=dates_array,\n",
    "                                       weights_array=weights_array,\n",
    "                                       resp_array=resp_array,\n",
    "                                       action_label_array=y_pred_label)\n",
    "\n",
    "        if score_tmp > best_acc:\n",
    "            best_acc = score_tmp\n",
    "            best_threshold = thresold_tmp\n",
    "    return best_acc, best_threshold\n",
    "\n",
    "\n",
    "class RollingBase(with_metaclass(ABCMeta, object)):\n",
    "    \"\"\"对于流数据（stream data）的统计量高效抽取的基类。\n",
    "\n",
    "    通过双端队列存储第i时刻前k个时刻的数组值，并以O(1)时间复杂度计算队内的需求指标。\n",
    "    每当队满的时候，队尾元素出队，并同时修改累加统计值。队内指标的计算通过push方法来实时更新。\n",
    "\n",
    "    @Attributes:\n",
    "    ----------\n",
    "    n_elements_in_deque: {int-like}\n",
    "        当前队内元素的个数。\n",
    "    n_feats: {int-like}\n",
    "        队内元素的特征维度。\n",
    "    max_deque_size: {int-like}\n",
    "        双端队列的大小。\n",
    "    deque: {collections deque object}\n",
    "        collections模块的双端队列实现。\n",
    "\n",
    "    @References:\n",
    "    ----------\n",
    "    [1] https://www.kaggle.com/lucasmorin/running-algos-fe-for-fast-inference\n",
    "    \"\"\"\n",
    "    def __init__(self, max_deque_size=10, n_feats=1, **kwargs):\n",
    "        if n_feats is None:\n",
    "            self.n_feats = 1\n",
    "        else:\n",
    "            self.n_feats = n_feats\n",
    "        self.n_elements_in_deque = 0\n",
    "        self.max_deque_size = max_deque_size\n",
    "        self.window_cum_sum = np.zeros(self.n_feats)\n",
    "        self.deque = collections.deque(maxlen=max_deque_size + 1)\n",
    "\n",
    "    @abstractmethod\n",
    "    def push(self, x):\n",
    "        \"\"\"将输入元素x入队，同时根据是否队满执行出队操作，并更新统计指标。\"\"\"\n",
    "        pass\n",
    "\n",
    "    def clear(self):\n",
    "        self.n_elements_in_deque = 0\n",
    "        self.deque.clear()  # 清空队内元素\n",
    "\n",
    "    def get_n_elements_in_deque(self):\n",
    "        return self.n_elements_in_deque\n",
    "\n",
    "\n",
    "class RollingStats(RollingBase):\n",
    "    \"\"\"对steam类型的数据执行滑动窗口平均。\n",
    "\n",
    "    通过双端队列存储第i时刻前k个时刻的数组值，并以O(1)时间复杂度计算队内的平均值。\n",
    "    每当队满的时候，队尾元素出队，并同时修改累加统计值。\n",
    "\n",
    "    @Attributes:\n",
    "    ----------\n",
    "    window_mean: {array-like}\n",
    "        队列内元素的平均值。\n",
    "    window_std: {array-like}\n",
    "        队列内元素的方差。\n",
    "    window_skew: {array-like}\n",
    "        队内元素的偏度。\n",
    "    window_kurtosis: {array-like}\n",
    "        队内元素的峰度。\n",
    "    **kwargs:\n",
    "        见基类说明。\n",
    "\n",
    "    @References:\n",
    "    ----------\n",
    "    [1] https://www.kaggle.com/lucasmorin/running-algos-fe-for-fast-inference\n",
    "    [2] https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n",
    "    [3] Rakthanmanon, Thanawin, et al. \"Searching and mining trillions of time series subsequences under dynamic time warping.\" Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining. 2012.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.window_mean = np.zeros(self.n_feats)\n",
    "        self.window_std = np.zeros(self.n_feats)\n",
    "        self.window_skew = np.zeros(self.n_feats)\n",
    "        self.window_kurtosis = np.zeros(self.n_feats)\n",
    "        self.window_cum_sum_square = np.zeros(self.n_feats)\n",
    "\n",
    "    def push(self, x):\n",
    "        \"\"\"入队一个元素x，并提取统计指标。\"\"\"\n",
    "        self.deque.append(x)\n",
    "        self.window_cum_sum += x\n",
    "        self.window_cum_sum_square += np.square(x)\n",
    "\n",
    "        # 指标抽取：均值，方差，峰度，偏度\n",
    "        # TODO(zhuoyin94@163.com)：使用One-pass算法计算峰度与偏度\n",
    "        if self.n_elements_in_deque < self.max_deque_size:\n",
    "            self.n_elements_in_deque += 1\n",
    "\n",
    "            self.window_mean = self.window_cum_sum / self.n_elements_in_deque\n",
    "            self.window_std = self.window_cum_sum_square / self.n_elements_in_deque - np.square(self.window_mean)\n",
    "        else:\n",
    "            poped_element = self.deque.popleft()\n",
    "            self.window_cum_sum -= poped_element\n",
    "            self.window_cum_sum_square -= np.square(poped_element)\n",
    "\n",
    "            self.window_mean = self.window_cum_sum / self.n_elements_in_deque\n",
    "            self.window_std = self.window_cum_sum_square / self.n_elements_in_deque - np.square(self.window_mean)\n",
    "\n",
    "    def get_mean(self):\n",
    "        \"\"\"获取队内元素的平均值。\"\"\"\n",
    "        if self.n_elements_in_deque:\n",
    "            return self.window_mean\n",
    "        else:\n",
    "            return np.zeros(self.n_feats)\n",
    "\n",
    "    def get_std(self):\n",
    "        \"\"\"获取队内元素的方差。\"\"\"\n",
    "        if self.n_elements_in_deque:\n",
    "            return self.window_std\n",
    "        else:\n",
    "            return np.zeros(self.n_feats)\n",
    "\n",
    "\n",
    "print(\"[INFO] {} Tools prepared !\".format(\n",
    "    str(datetime.now())[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "特征抽取工具的集中单元测试。\n",
    "\"\"\"\n",
    "def test_rolling_correctness():\n",
    "    tmp_data = np.array([[-1, -1],\n",
    "                         [1, 1],\n",
    "                         [2, 2],\n",
    "                         [3, 3],\n",
    "                         [4, 4]])\n",
    "\n",
    "    rolling_average_2 = RollingStats(max_deque_size=2, n_feats=tmp_data.shape[1])\n",
    "    for i in range(tmp_data.shape[0]):\n",
    "        rolling_average_2.push(tmp_data[i])\n",
    "        print(rolling_average_2.get_mean())\n",
    "        print(rolling_average_2.get_std())\n",
    "        print(\"---\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "    rolling_average_3 = RollingStats(max_deque_size=4, n_feats=tmp_data.shape[1])\n",
    "    for i in range(tmp_data.shape[0]):\n",
    "        rolling_average_3.push(tmp_data[i])\n",
    "        print(rolling_average_3.get_mean())\n",
    "        print(rolling_average_3.get_std())\n",
    "        print(\"---\")\n",
    "\n",
    "    tmp_data = np.array([-1, 1, 2, 3, 4, 5])\n",
    "    print(\"\\n\")\n",
    "    rolling_average_3 = RollingStats(max_deque_size=3, n_feats=None)\n",
    "    for i in range(tmp_data.shape[0]):\n",
    "        rolling_average_3.push(tmp_data[i])\n",
    "        print(rolling_average_3.get_mean())\n",
    "        print(rolling_average_3.get_std())\n",
    "        print(\"---\")\n",
    "\n",
    "\n",
    "def test_rolling_performance():\n",
    "    tmp_data = X.copy()\n",
    "\n",
    "    rolling_average_4 = RollingStats(max_deque_size=4, n_feats=tmp_data.shape[1])\n",
    "    print(\"\\nRolling window size: 4\")\n",
    "    for i in tqdm(range(len(X))):\n",
    "        rolling_average_4.push(tmp_data[i])   \n",
    "\n",
    "    rolling_average_10000 = RollingStats(max_deque_size=10000, n_feats=tmp_data.shape[1])\n",
    "    print(\"\\nRolling window size: 10000\")\n",
    "    for i in tqdm(range(len(X))):\n",
    "        rolling_average_10000.push(tmp_data[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] 2021-01-14 11:44:48.08 MLP Model prepared !\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "模型构建与模型训练部分。\n",
    "\"\"\"\n",
    "def build_model(verbose=False, is_compile=True, encoder_model=None, **kwargs):\n",
    "    \"\"\"针对二分类任务的MLP模型，使用自编码器的编码层作为预训练层。\"\"\"\n",
    "    input_dim = kwargs.pop(\"input_dim\", None)\n",
    "    output_dim = kwargs.pop(\"output_dim\", None)\n",
    "    n_labels = kwargs.pop(\"n_labels\", None)\n",
    "\n",
    "    # 构造网络结构\n",
    "    layer_input = layers.Input(input_dim, dtype='float32')\n",
    "    layer_encoded = encoder_model(layer_input)\n",
    "\n",
    "    layer_feats = layers.concatenate([layer_input, layer_encoded])\n",
    "    layer_feats = layers.BatchNormalization()(layer_feats)\n",
    "\n",
    "    # 特征抽取\n",
    "    layer_dense = layers.Dense(64, activation=\"relu\")(layer_feats)\n",
    "    layer_dense = layers.BatchNormalization()(layer_dense)\n",
    "    layer_dense = layers.Dropout(0.3)(layer_dense)\n",
    "\n",
    "    layer_dense = layers.Dense(32, activation=\"relu\")(layer_dense)\n",
    "    layer_dense = layers.BatchNormalization()(layer_dense)\n",
    "    layer_dense = layers.Dropout(0.3)(layer_dense)\n",
    "\n",
    "    # 输出层构造与模型构造\n",
    "    layer_output = layers.Dense(output_dim, activation='sigmoid', name=\"label_output\")(layer_dense)\n",
    "    model = models.Model(layer_input, layer_output)\n",
    "\n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    if is_compile:\n",
    "        model.compile(loss={'label_output':'binary_crossentropy'},\n",
    "                      metrics=[tf.keras.metrics.AUC(name='auc')],\n",
    "                      optimizer=optimizers.Adam(0.003))\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_1d_cnn(verbose=False, is_compile=True, encoder_model=None, **kwargs):\n",
    "    pass\n",
    "    x = tf.keras.layers.Dense(4096, activation='relu')(x)\n",
    "    x = tf.keras.layers.Reshape((256, 16))(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=16,\n",
    "                      kernel_size=5,\n",
    "                      strides=1,\n",
    "                      activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "\n",
    "def test_build_model():\n",
    "    # 构造mlp模型\n",
    "    mlp_model = build_model(verbose=False, encoder_model=encoder_model,\n",
    "                            input_dim=X.shape[1], output_dim=y.shape[1])\n",
    "\n",
    "print(\"[INFO] {} MLP Model prepared !\".format(\n",
    "    str(datetime.now())[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1571415/1571415 [01:59<00:00, 13196.98it/s]\n",
      "[INFO] 2021-01-14 11:49:27.20 Feature extraction completed !\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "特征工程的pipline。\n",
    "\"\"\"\n",
    "\n",
    "X_feats = []\n",
    "\n",
    "# STEP 1: 时间序列特征\n",
    "# -----------------------\n",
    "ts_feats_extractor = [RollingStats(max_deque_size=300, n_feats=len(feature_name_list)),\n",
    "                      RollingStats(max_deque_size=1000, n_feats=len(feature_name_list)),\n",
    "                      RollingStats(max_deque_size=5000, n_feats=len(feature_name_list))]\n",
    "\n",
    "for i in tqdm(range(X.shape[0])):\n",
    "    x_tmp = X[i]\n",
    "\n",
    "    x_feats_tmp = []\n",
    "    for _, extractor in enumerate(ts_feats_extractor):\n",
    "        extractor.push(x_tmp)\n",
    "        x_feats_tmp.append(extractor.get_mean())\n",
    "        x_feats_tmp.append(extractor.get_std())\n",
    "    X_feats.append(np.concatenate(x_feats_tmp).astype(\"float16\"))\n",
    "\n",
    "del ts_feats_extractor\n",
    "gc.collect()\n",
    "\n",
    "# 选择相关性最高的特征，并将时序特征与实际特征进行组合\n",
    "def yield_batch_idx(total_size=1023, batsch_size=20):\n",
    "    for i in range(0, total_size, batch_size):\n",
    "        yield [i, min(total_size-1, i+batch_size)]\n",
    "\n",
    "batch_size = 20\n",
    "keep_top_k = 15\n",
    "\n",
    "corr_scores = []\n",
    "for start, end in yield_batch_idx(len(X_feats[0]), 20):\n",
    "    curr_feats = np.vstack([item[start:end] for item in X_feats]).astype(\"float16\")\n",
    "\n",
    "    for i in range(curr_feats.shape[1]):\n",
    "        corr_scores.append([np.corrcoef(curr_feats[:, i], y[:, 0]).ravel()[1], i])\n",
    "\n",
    "selected_feat_ids = [item[1] for item in sorted(corr_scores, key=lambda x: abs(x[0]))][-keep_top_k:]\n",
    "X_feats = [item[selected_feat_ids] for item in X_feats]\n",
    "X_feats = np.hstack([X_feats, X]).astype(\"float32\")\n",
    "\n",
    "print(\"[INFO] {} Feature extraction completed !\".format(\n",
    "    str(datetime.now())[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] 2021-01-14 11:49:27.27 Model training start:\n",
      "=========================================\n",
      "-- folds 1(7)(train_range: 86->137, valid_range: 143->193), valid_acc: 0.5172, valid_roc_auc: 0.5175, valid_custom: 1235.5659\n",
      "-- folds 2(7)(train_range: 86->188, valid_range: 194->244), valid_acc: 0.5196, valid_roc_auc: 0.5198, valid_custom: 1247.4478\n",
      "-- folds 3(7)(train_range: 86->239, valid_range: 245->295), valid_acc: 0.5173, valid_roc_auc: 0.5174, valid_custom: 505.0948\n",
      "-- folds 4(7)(train_range: 86->290, valid_range: 296->346), valid_acc: 0.5194, valid_roc_auc: 0.5199, valid_custom: 907.0717\n",
      "-- folds 5(7)(train_range: 86->341, valid_range: 347->397), valid_acc: 0.5154, valid_roc_auc: 0.5161, valid_custom: 924.3744\n",
      "-- folds 6(7)(train_range: 86->392, valid_range: 398->448), valid_acc: 0.5191, valid_roc_auc: 0.5162, valid_custom: 473.3621\n",
      "-- folds 7(7)(train_range: 86->443, valid_range: 449->499), valid_acc: 0.5218, valid_roc_auc: 0.5218, valid_custom: 2092.8594\n",
      "-- total metric, valid_acc: 0.5185, valid_roc_auc: 0.5184, valid_custom: 1055.1109\n",
      "=========================================\n",
      "[INFO] 2021-01-14 12:05:11.58 Model training end.\n"
     ]
    }
   ],
   "source": [
    "# 训练前全局参数准备\n",
    "N_SPLITS = 7\n",
    "MODELS = []\n",
    "early_stop = callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\",\n",
    "                                     verbose=0, patience=15,\n",
    "                                     restore_best_weights=True)\n",
    "group_ts_kfolds = PurgedGroupTimeSeriesSplit(\n",
    "        n_splits=N_SPLITS, group_gap=5, max_test_group_size=60)\n",
    "\n",
    "# 开始训练模型\n",
    "valid_acc_total, valid_roc_auc_total, valid_custom_total = [], [], []\n",
    "\n",
    "print(\"[INFO] {} Model training start:\".format(str(datetime.now())[:-4]))\n",
    "print(\"=========================================\")\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "for fold, (train_idx, valid_idx) in enumerate(group_ts_kfolds.split(X=X_feats, y=y, groups=train_dates)):\n",
    "    #####################################################\n",
    "    # Cross validation的数据准备\n",
    "    X_train, X_val = X_feats[train_idx], X_feats[valid_idx]\n",
    "    y_train, y_val = y[train_idx], y[valid_idx]\n",
    "\n",
    "    X_train_weight, X_val_weight = train_weights[train_idx], train_weights[valid_idx]\n",
    "    X_train_resp, X_val_resp = train_resp[train_idx], train_resp[valid_idx]\n",
    "    X_train_dates, X_val_dates = train_dates[train_idx], train_dates[valid_idx]\n",
    "\n",
    "    #####################################################\n",
    "    # STEP 1: 进行特征工程\n",
    "    std_scaler, pca_transformer, X_train_pca = feat_pca(feat_array=X_train, n_dims=30)\n",
    "\n",
    "    X_val_pca = std_scaler.transform(X_val)\n",
    "    X_val_pca = pca_transformer.transform(X_val)\n",
    "\n",
    "    X_train = np.hstack([X_train, X_train_pca])\n",
    "    X_val = np.hstack([X_val, X_val_pca])\n",
    "\n",
    "    #####################################################\n",
    "    # STEP 2: 在训练数据上进行预训练\n",
    "    encoder_model, pretrain_model = build_tabular_autoencoder(\n",
    "            input_dim=X_train.shape[1], n_labels=y_train.shape[1],\n",
    "            stddev=0.03, verbose=False)\n",
    "    pretrain_model.fit(x=X_train, y=[X_train, y_train],\n",
    "                       batch_size=32758,\n",
    "                       epochs=20,\n",
    "                       verbose=0)\n",
    "    encoder_model.trainable = True\n",
    "\n",
    "    #####################################################\n",
    "    # STEP 3: 在训练数据上进行训练\n",
    "    mlp_model = build_model(verbose=False, encoder_model=encoder_model,\n",
    "                            input_dim=X_train.shape[1], output_dim=y_train.shape[1])\n",
    "\n",
    "    mlp_model.fit(x=X_train, y=y_train,\n",
    "                  validation_data=(X_val, y_val),\n",
    "                  batch_size=32758,\n",
    "                  epochs=100,\n",
    "                  verbose=0,\n",
    "                  callbacks=[early_stop])\n",
    "\n",
    "    # 转换模型为Tensorflow Lite模型，节约内存与推理时间\n",
    "    mlp_model = LiteModel.from_keras_model(mlp_model)\n",
    "\n",
    "    #####################################################\n",
    "    # STEP 4: 寻找最优valid的阈值\n",
    "    valid_pred_proba = np.mean(mlp_model.predict(X_val), axis=1)\n",
    "    best_custom, THRESHOLD = njit_search_best_thresold_custom(\n",
    "            y_pred_proba=valid_pred_proba,\n",
    "            dates_array=X_val_dates,\n",
    "            weights_array=X_val_weight,\n",
    "            resp_array=X_val_resp)\n",
    "\n",
    "    #####################################################\n",
    "    # STEP 5: valid data上按照官方metric进行结果评估\n",
    "    valid_pred_label = np.where(\n",
    "            valid_pred_proba>=THRESHOLD, 1, 0).astype(int)\n",
    "    valid_custom_metric = custom_metric(dates_array=X_val_dates,\n",
    "                                        weights_array=X_val_weight,\n",
    "                                        action_label_array=valid_pred_label,\n",
    "                                        resp_array=X_val_resp)\n",
    "    valid_acc = accuracy_score(y_val[:, 0].reshape(-1, 1),\n",
    "                               valid_pred_label.reshape(-1, 1))\n",
    "    valid_roc_auc = roc_auc_score(y_val[:, 0].reshape(-1, 1),\n",
    "                                  valid_pred_label.reshape(-1, 1))\n",
    "\n",
    "    # 标准打印训练信息\n",
    "    print(\"-- folds {}({})(train_range: {}->{}, valid_range: {}->{}), valid_acc: {:.4f}, valid_roc_auc: {:.4f}, valid_custom: {:.4f}\".format(\n",
    "            fold+1, N_SPLITS, min(X_train_dates), max(X_train_dates), min(X_val_dates), max(X_val_dates), valid_acc, valid_roc_auc, valid_custom_metric))\n",
    "\n",
    "    #####################################################\n",
    "    # STEP 6: 保存模型与关键训练指标\n",
    "    MODELS.append([THRESHOLD, std_scaler, pca_transformer, mlp_model])\n",
    "    valid_acc_total.append(valid_acc)\n",
    "    valid_roc_auc_total.append(valid_roc_auc)\n",
    "    valid_custom_total.append(valid_custom_metric)\n",
    "\n",
    "    # 强制内存回收\n",
    "    del X_train, X_val, y_train, y_val, X_train_pca, X_val_pca\n",
    "    del X_train_weight, X_val_weight, X_train_resp, X_val_resp, X_train_dates, X_val_dates\n",
    "    gc.collect()\n",
    "\n",
    "# 打印总体分数指标\n",
    "print(\"-- total metric, valid_acc: {:.4f}, valid_roc_auc: {:.4f}, valid_custom: {:.4f}\".format(\n",
    "        np.mean(valid_acc_total), np.mean(valid_roc_auc_total), np.mean(valid_custom_total)))\n",
    "\n",
    "print(\"=========================================\")\n",
    "print(\"[INFO] {} Model training end.\".format(str(datetime.now())[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "15219it [00:43, 352.28it/s]0.5131187908635476 0.4868812091364524\n",
      "0.4981943025871587 0.5018056974128413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "预测标签按CV结果依据概率加权策略：根据CV结果，以CV的custom metric的结果对预测概率进行加权。\n",
    "\"\"\"\n",
    "# 预备提交预测结果\n",
    "count_0, count_1 = 0, 0\n",
    "\n",
    "if True:\n",
    "    # 初始化环境\n",
    "    env = gen_test_data(example_test_df, example_prediction_df)\n",
    "    valid_custom_total_norm = np.array(valid_custom_total) / np.sum(valid_custom_total)\n",
    "\n",
    "    ts_feats_extractor = [RollingStats(max_deque_size=300, n_feats=len(feature_name_list)),\n",
    "                          RollingStats(max_deque_size=1000, n_feats=len(feature_name_list)),\n",
    "                          RollingStats(max_deque_size=5000, n_feats=len(feature_name_list)),\n",
    "                          RollingStats(max_deque_size=15000, n_feats=len(feature_name_list)),\n",
    "                          RollingStats(max_deque_size=30000, n_feats=len(feature_name_list))]\n",
    "\n",
    "    # 开始预测\n",
    "    THRESHOLD = 0.512\n",
    "    for (test_df, pred_df) in tqdm(env):\n",
    "        if test_df['weight'].item() > 0:\n",
    "            x_test_val = test_df[feature_name_list].values\n",
    "            x_test_val = njit_fillna(x_test_val, mean_val_array)\n",
    "\n",
    "            # 特征工程pipeline\n",
    "            # --------------\n",
    "            x_test_feats = []\n",
    "            for _, extractor in enumerate(ts_feats_extractor):\n",
    "                extractor.push(x_test_val)\n",
    "                x_test_feats.append(extractor.get_mean())\n",
    "                x_test_feats.append(extractor.get_std())\n",
    "            x_test_feats = np.concatenate(x_test_feats)[selected_feat_ids]\n",
    "            x_test_val = np.concatenate([x_test_feats, x_test_val]).reshape(1, -1)\n",
    "\n",
    "            # 利用MODELS里训练好的神经网络进行训练\n",
    "            pred_proba_list = []\n",
    "            for i in range(len(MODELS)):\n",
    "                x_test_val_pca = MODELS[i][1].transform(x_test_val)\n",
    "                x_test_val_pca = MODELS[i][2].transform(x_test_val_pca)\n",
    "                x_test_val_pca = np.hstack([x_test_val, x_test_val_pca])\n",
    "\n",
    "                pred_proba = MODELS[i][3].predict(x_test_val_pca)\n",
    "                pred_proba_list.append(np.mean(pred_proba))\n",
    "\n",
    "            pred = np.average(pred_proba_list, weights=valid_custom_total_norm)\n",
    "            pred_label = int(np.where(pred >= THRESHOLD, 1, 0))\n",
    "\n",
    "            if pred_label == 0:\n",
    "                count_0 += 1\n",
    "            else:\n",
    "                count_1 += 1\n",
    "\n",
    "            pred_df.action = pred_label\n",
    "        else:\n",
    "            pred_df.action = 0\n",
    "\n",
    "    print(count_0/(count_0+count_1), count_1/(count_0+count_1))\n",
    "    print(np.sum(y[:, 0] == 0)/(np.sum(y[:, 0] == 0) + np.sum(y[:, 0] == 1)),\n",
    "        np.sum(y[:, 0] == 1)/(np.sum(y[:, 0] == 0) + np.sum(y[:, 0] == 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "预测标签众数生成策略：选择预测的标签的众数作为样本标签。\n",
    "\"\"\"\n",
    "# 预备提交预测结果\n",
    "IS_PREDICT = False\n",
    "\n",
    "if IS_PREDICT:\n",
    "    # 初始化环境\n",
    "    env = gen_test_data(example_test_df, example_prediction_df)\n",
    "\n",
    "    # 预测\n",
    "    for (test_df, pred_df) in tqdm(env):\n",
    "        if test_df['weight'].item() > 0:\n",
    "            x_test_val = test_df[feature_name_list].values\n",
    "            x_test_val = njit_fillna(x_test_val, mean_val_array).reshape(1, -1)\n",
    "\n",
    "            # 利用MODELS里训练好的神经网络进行训练\n",
    "            pred_label_list = []\n",
    "            for i in range(len(MODELS)):\n",
    "                x_test_val_pca = MODELS[i][1].transform(x_test_val)\n",
    "                x_test_val_pca = MODELS[i][2].transform(x_test_val_pca)\n",
    "                x_test_val_pca = np.hstack([x_test_val, x_test_val_pca])\n",
    "\n",
    "                pred_proba = MODELS[i][3].predict(x_test_val_pca)\n",
    "                pred = np.mean(pred_proba)\n",
    "                pred_label_list.append(int(np.where(pred >= MODELS[i][0], 1, 0)))\n",
    "\n",
    "            # 选取预测最频繁的样本作为标签\n",
    "            # https://stackoverflow.com/questions/6252280/find-the-most-frequent-number-in-a-numpy-array\n",
    "            pred_df.action = max(map(lambda val: (pred_label_list.count(val), val), set(pred_label_list)))[1]\n",
    "        else:\n",
    "            pred_df.action = 0"
   ]
  }
 ]
}