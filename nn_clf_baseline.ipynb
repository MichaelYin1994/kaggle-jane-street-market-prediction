{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "from numba import njit, jit\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import iinfo, finfo, int8, int16, int32, int64, float32, float64\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import activations\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from IPython.display import HTML\n",
    "\n",
    "# 设置plotly为暗黑模式\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "plot_config = dict({'scrollZoom': True, 'displayModeBar': True, 'displaylogo': False})\n",
    "sns.set(style=\"ticks\", font_scale=1.2, palette='deep', color_codes=True)\n",
    "colors = [\"C\" + str(i) for i in range(0, 9+1)]\n",
    "\n",
    "# 默认plotly色号\n",
    "default_color_list = [\n",
    "    '#1f77b4',  # muted blue\n",
    "    '#ff7f0e',  # safety orange\n",
    "    '#2ca02c',  # cooked asparagus green\n",
    "    '#d62728',  # brick red\n",
    "    '#9467bd',  # muted purple\n",
    "    '#8c564b',  # chestnut brown\n",
    "    '#e377c2',  # raspberry yogurt pink\n",
    "    '#7f7f7f',  # middle gray\n",
    "    '#bcbd22',  # curry yellow-green\n",
    "    '#17becf'   # blue-teal\n",
    "    ]\n",
    "\n",
    "# 设定全局随机种子，并且屏蔽warnings\n",
    "GLOBAL_RANDOM_SEED = 2022\n",
    "np.random.seed(GLOBAL_RANDOM_SEED)\n",
    "tf.random.set_seed(GLOBAL_RANDOM_SEED)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] 2021-01-07 18:57:14.89 End Reading ! It took 54.35 seconds !\n[INFO] 2021-01-07 18:57:14.89 Basic data description: \n    -- train_df shape: (2390491, 138)\n    -- example_test_df shape: (15219, 133)\n    -- feat_df shape: (130, 30)\n    -- example_prediction_df shape: (15219, 2)\n"
     ]
    }
   ],
   "source": [
    "# 导入数据\n",
    "load_data_start_time = time.time()\n",
    "train_df  = pd.read_csv(\n",
    "    './data/jane-street-market-prediction/train.csv', nrows=None)\n",
    "feat_df = pd.read_csv(\n",
    "    './data/jane-street-market-prediction/features.csv')\n",
    "example_test_df = pd.read_csv(\n",
    "    './data/jane-street-market-prediction/example_test.csv')\n",
    "example_prediction_df = pd.read_csv(\n",
    "    './data/jane-street-market-prediction/example_sample_submission.csv')\n",
    "load_data_end_time = time.time()\n",
    "\n",
    "# 打印数据基本情况\n",
    "print(\"[INFO] {} End Reading ! It took {:.2f} seconds !\".format(\n",
    "    str(datetime.now())[:-4], load_data_end_time-load_data_start_time))\n",
    "print(\"[INFO] {} Basic data description: \".format(str(datetime.now())[:-4]))\n",
    "print(\"    -- train_df shape: {}\".format(\n",
    "    train_df.shape))\n",
    "print(\"    -- example_test_df shape: {}\".format(\n",
    "    example_test_df.shape))\n",
    "print(\"    -- feat_df shape: {}\".format(\n",
    "    feat_df.shape))\n",
    "print(\"    -- example_prediction_df shape: {}\".format(\n",
    "    example_prediction_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_test_data(test_df=None, pred_df=None):\n",
    "    \"\"\"测试数据生成器。用于模拟测试数据生成过程，测试模型提交正确性与效率。\"\"\"\n",
    "    n_test = len(test_df)\n",
    "\n",
    "    for i in range(n_test):\n",
    "        yield test_df.iloc[i], pred_df.iloc[i]\n",
    "\n",
    "\n",
    "@jit\n",
    "def njit_fillna(array, values):\n",
    "    \"\"\"利用即时编译（jit）对array数组的NaN值借助values进行填充。\n",
    "\n",
    "    @References:\n",
    "    ----------\n",
    "    [1] https://www.kaggle.com/gogo827jz/optimise-speed-of-filling-nan-function\n",
    "    \"\"\"\n",
    "    if np.isnan(array.sum()):\n",
    "        array = np.where(np.isnan(array), values, array)\n",
    "    return array\n",
    "\n",
    "\n",
    "def custom_metric(dates_array=None,\n",
    "                  weights_array=None,\n",
    "                  resp_array=None,\n",
    "                  action_label_array=None):\n",
    "    \"\"\"依据官方要求的Metric，计算分数。\n",
    "\n",
    "    @References:\n",
    "    ----------\n",
    "    [1] https://www.kaggle.com/c/jane-street-market-prediction/discussion/199107\n",
    "    [2] https://www.kaggle.com/c/jane-street-market-prediction/overview/evaluation\n",
    "    [3] \n",
    "    \"\"\"\n",
    "    tmp_df = pd.DataFrame({\"date\": dates_array,\n",
    "                           \"weight\": weights_array,\n",
    "                           \"resp\": resp_array,\n",
    "                           \"action\": action_label_array})\n",
    "    tmp_df[\"p\"] = tmp_df[\"weight\"]  * tmp_df[\"resp\"] * tmp_df[\"action\"]\n",
    "    # tmp_df = tmp_df.query(\"weight != 0\").reset_index(drop=True)\n",
    "    p_i_val = tmp_df.groupby([\"date\"])[\"p\"].sum().values\n",
    "\n",
    "    n_dates = len(p_i_val)\n",
    "    t = np.sum(p_i_val) / np.sqrt(np.sum(p_i_val ** 2)) * (np.sqrt(250 / n_dates))\n",
    "    return min(max(t, 0), 6) * np.sum(p_i_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"针对带有Group id（组id）数据的时间序列交叉验证集合生成类。\n",
    "\n",
    "    生成针对带有Group id的数据的时序交叉验证集。其中训练与验证的\n",
    "    Group之间可以指定group_gap，用来隔离时间上的关系。这种情况下\n",
    "    group_id通常是时间id，例如天或者小时。\n",
    "\n",
    "    @Parameters:\n",
    "    ----------\n",
    "        n_splits: {int-like}, default=5\n",
    "            切分的集合数目。\n",
    "        max_train_group_size: {int-like}, default=+inf\n",
    "            训练集单个组的最大样本数据限制。\n",
    "        group_gap: {int-like}, default=None\n",
    "            依据group_id切分组时，训练组与测试组的id的gap数目。\n",
    "        max_test_group_size: {int-like}, default=+inf\n",
    "            测试集单个组的最大样本数据限制。\n",
    "\n",
    "    @References:\n",
    "    ----------\n",
    "    [1] https://www.kaggle.com/gogo827jz/jane-street-ffill-xgboost-purgedtimeseriescv\n",
    "    \"\"\"\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self, n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"生成训练组与测试组的id索引，返回组索引的生成器。\n",
    "\n",
    "        @Parameters:\n",
    "        ----------\n",
    "            X: {array-like} {n_samples, n_features}\n",
    "                训练数据，输入形状为{n_samples, n_features}。\n",
    "            y: {array-like} {n_samples, }\n",
    "                标签数据，形状为{n_samples, }。\n",
    "            groups: {array-like} {n_samples, }\n",
    "                用来依据组来划分训练集与测试集的组id，必须为连续的组id。\n",
    "\n",
    "        @Yields:\n",
    "        ----------\n",
    "            train: ndarray\n",
    "                依据group_id切分的训练组id。\n",
    "            test: ndarray\n",
    "                依据group_id切分的测试组id。\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None ！\")\n",
    "\n",
    "        # 初始化基本参数信息\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples, n_splits, group_gap = _num_samples(X), self.n_splits, self.group_gap\n",
    "        n_folds = n_splits + 1\n",
    "\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = np.argsort(ind)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "\n",
    "        # 扫描整个数据id list，构建group_dcit，{group_id: 属于该group的样本的idx}\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds, n_groups))\n",
    "\n",
    "        # group_test_size: 每个fold预留的test group的大小\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array, test_array = [], []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "            test_array  = test_array[group_gap:]\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]\n",
    "\n",
    "\n",
    "def test_purged_group_time_series_split():\n",
    "    X = train_df.query('date > 85').reset_index(drop=True)[[\"ts_id\", \"feature_0\"]].values\n",
    "    y = train_df.query('date > 85').reset_index(drop=True)[\"resp\"].values\n",
    "    groups = train_df.query('date > 85').reset_index(drop=True)[\"date\"].values\n",
    "\n",
    "    group_ts_kfolds = PurgedGroupTimeSeriesSplit(\n",
    "        n_splits=4, group_gap=31, max_test_group_size=31)\n",
    "    train_idx, valid_idx = [], []\n",
    "    for train_idx_tmp, valid_idx_tmp in group_ts_kfolds.split(X=X, y=y, groups=groups):\n",
    "        train_idx.append(train_idx_tmp)\n",
    "        valid_idx.append(valid_idx_tmp)\n",
    "\n",
    "    print([[train_df.iloc[min(item)][\"date\"].astype(int),\n",
    "            train_df.iloc[max(item)][\"date\"].astype(int)] for item in train_idx])\n",
    "    print([[train_df.iloc[min(item)][\"date\"].astype(int),\n",
    "            train_df.iloc[max(item)][\"date\"].astype(int)] for item in valid_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://medium.com/@micwurm/using-tensorflow-lite-to-speed-up-predictions-a3954886eb98\n",
    "class LiteModel:\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, model_path):\n",
    "        return LiteModel(tf.lite.Interpreter(model_path=model_path))\n",
    "\n",
    "    @classmethod\n",
    "    def from_keras_model(cls, kmodel):\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(kmodel)\n",
    "        tflite_model = converter.convert()\n",
    "        return LiteModel(tf.lite.Interpreter(model_content=tflite_model))\n",
    "\n",
    "    def __init__(self, interpreter):\n",
    "        self.interpreter = interpreter\n",
    "        self.interpreter.allocate_tensors()\n",
    "        input_det = self.interpreter.get_input_details()[0]\n",
    "        output_det = self.interpreter.get_output_details()[0]\n",
    "        self.input_index = input_det[\"index\"]\n",
    "        self.output_index = output_det[\"index\"]\n",
    "        self.input_shape = input_det[\"shape\"]\n",
    "        self.output_shape = output_det[\"shape\"]\n",
    "        self.input_dtype = input_det[\"dtype\"]\n",
    "        self.output_dtype = output_det[\"dtype\"]\n",
    "\n",
    "    def predict(self, inp):\n",
    "        inp = inp.astype(self.input_dtype)\n",
    "        count = inp.shape[0]\n",
    "        out = np.zeros((count, self.output_shape[1]), dtype=self.output_dtype)\n",
    "        for i in range(count):\n",
    "            self.interpreter.set_tensor(self.input_index, inp[i:i+1])\n",
    "            self.interpreter.invoke()\n",
    "            out[i] = self.interpreter.get_tensor(self.output_index)[0]\n",
    "        return out\n",
    "\n",
    "    def predict_single(self, inp):\n",
    "        \"\"\" Like predict(), but only for a single record. The input data can be a Python list. \"\"\"\n",
    "        inp = np.array([inp], dtype=self.input_dtype)\n",
    "        self.interpreter.set_tensor(self.input_index, inp)\n",
    "        self.interpreter.invoke()\n",
    "        out = self.interpreter.get_tensor(self.output_index)\n",
    "        return out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] 2021-01-07 19:10:39.48 Data prepared !\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "数据预处理部分。包括标签生成、数据统计值获取。\n",
    "\"\"\"\n",
    "# 挑选策略变化之后的数据\n",
    "train = train_df.query('date > 85').reset_index(drop=True)\n",
    "train = train.query('weight > 0').reset_index(drop = True)\n",
    "\n",
    "target_threshold = 0.00001\n",
    "# 构造标签\n",
    "train['action'] =  ((train['resp_1'] > target_threshold) & \\\n",
    "                    (train['resp_2'] > target_threshold) & \\\n",
    "                    (train['resp_3'] > target_threshold) & \\\n",
    "                    (train['resp_4'] > target_threshold) &  \\\n",
    "                    (train['resp'] > target_threshold)).astype('int')\n",
    "feature_name_list = [c for c in train.columns if 'feature' in c]\n",
    "resp_name_list = [\"resp\", \"resp_1\", \"resp_2\", \"resp_3\", \"resp_4\"]\n",
    "\n",
    "# 使用均值填充缺失值\n",
    "mean_val_list = []\n",
    "for name in feature_name_list:\n",
    "    mean_val = train[name].mean()\n",
    "    train[name].fillna(mean_val, inplace=True)\n",
    "    mean_val_list.append(mean_val)\n",
    "mean_val_array = np.array(mean_val_list)\n",
    "\n",
    "# 构造后续神经网络模型的输入输出\n",
    "X = train[feature_name_list].values\n",
    "y = np.stack([(train[c] > target_threshold).astype('int') for c in resp_name_list]).T\n",
    "y_resp = train[resp_name_list].values\n",
    "\n",
    "train_dates = train[\"date\"].values\n",
    "train_weights = train[\"weight\"].values\n",
    "train_resp = train[\"resp\"].values\n",
    "\n",
    "print(\"[INFO] {} Data prepared !\".format(\n",
    "    str(datetime.now())[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] 2021-01-07 20:11:50.40 Tools prepared !\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "特征工程辅助工具。\n",
    "\"\"\"\n",
    "def build_tabular_autoencoder(verbose=False, is_compile=True,\n",
    "                              stddev=0.05, **kwargs):\n",
    "    \"\"\"降噪自编码器实现。针对表格形式数据的降噪自编码器，噪声等级由高斯噪声的stddev参数指定\"\"\"\n",
    "    input_dim = kwargs.pop(\"input_dim\", None)\n",
    "    n_labels = kwargs.pop(\"n_labels\", None)\n",
    "\n",
    "    # 构建降噪自编码器\n",
    "    layer_input = layers.Input(input_dim, dtype='float32')\n",
    "\n",
    "    layer_encoded = layers.BatchNormalization()(layer_input)\n",
    "    layer_encoded = layers.GaussianNoise(stddev=stddev)(layer_encoded)\n",
    "    layer_encoded = layers.Dense(256, activation='relu')(layer_encoded)\n",
    "\n",
    "    # 解码层1：针对输入的重构\n",
    "    layer_decoded = layers.Dropout(0.2)(layer_encoded)\n",
    "    layer_decoded = layers.Dense(input_dim, name='reconstruct_output')(layer_decoded)\n",
    "\n",
    "    # 解码层2：针对resp的重构\n",
    "    layer_output = layers.Dense(128, activation='relu')(layer_decoded)\n",
    "    layer_output = layers.BatchNormalization()(layer_output)\n",
    "    layer_output = layers.Dropout(0.2)(layer_output)\n",
    "\n",
    "    layer_output_labels = layers.Dense(n_labels, activation='sigmoid', name='layer_output_labels')(layer_output)\n",
    "\n",
    "    # 输出层\n",
    "    encoder_model = models.Model(inputs=layer_input, outputs=layer_decoded)\n",
    "    autoencoder_model = models.Model(inputs=layer_input, outputs=[layer_decoded, layer_output_labels])\n",
    "\n",
    "    if verbose:\n",
    "        autoencoder_model.summary()\n",
    "    if is_compile:\n",
    "        autoencoder_model.compile(loss={'reconstruct_output': 'mse', 'layer_output_labels':'binary_crossentropy'},\n",
    "                                  metrics={'layer_output_labels':'acc'}, \n",
    "                                  optimizer=optimizers.Adam(0.001))\n",
    "    return encoder_model, autoencoder_model\n",
    "\n",
    "\n",
    "def feat_pca(feat_array=None, n_dims=30):\n",
    "    \"\"\"利用PCA，将feat_array降维至n_dims维度。\"\"\"\n",
    "    if feat_array.shape[1] <= n_dims:\n",
    "        raise ValueError(\"n_dims must smaller than the dim of feat_array !\")\n",
    "\n",
    "    # 归一化feat_array\n",
    "    X_sc = StandardScaler()\n",
    "    X_sc.fit(feat_array)\n",
    "    feat_array = X_sc.transform(feat_array)\n",
    "\n",
    "    # 降维\n",
    "    pca = PCA(n_components=n_dims)\n",
    "    pca.fit(feat_array)\n",
    "    feat_array_pca = pca.transform(feat_array)\n",
    "\n",
    "    return X_sc, pca, feat_array_pca\n",
    "\n",
    "\n",
    "@njit\n",
    "def njit_search_best_thresold_acc(y_pred_proba, y_true):\n",
    "    \"\"\"通过阈值搜索最优的准确率切分阈值.\"\"\"\n",
    "    best_acc, best_threshold = 0, 0\n",
    "    for threshold in range(4500, 5800):\n",
    "        thresold_tmp = threshold / 10000\n",
    "        y_pred_label = np.where(y_pred_proba > thresold_tmp, 1, 0)\n",
    "        score_tmp = np.sum(np.where(y_true == y_pred_label, 1, 0)) / len(y_true)\n",
    "\n",
    "        if score_tmp > best_acc:\n",
    "            best_acc = score_tmp\n",
    "            best_threshold = thresold_tmp\n",
    "    return best_acc, best_threshold\n",
    "\n",
    "\n",
    "@njit\n",
    "def njit_custom_metric(dates_array=None,\n",
    "                       weights_array=None,\n",
    "                       resp_array=None,\n",
    "                       action_label_array=None):\n",
    "    \"\"\"利用njit装饰器与numpy来计算Kaggle官方要求的Metric。\n",
    "\n",
    "    @References:\n",
    "    ----------\n",
    "    [1] https://www.kaggle.com/c/jane-street-market-prediction/discussion/199107\n",
    "    [2] https://www.kaggle.com/c/jane-street-market-prediction/overview/evaluation\n",
    "    \"\"\"\n",
    "    p_array = weights_array * resp_array * action_label_array\n",
    "\n",
    "    n_unique_dates = np.max(dates_array) - np.min(dates_array) + 1\n",
    "    dates_array = dates_array - np.min(dates_array)\n",
    "\n",
    "    p_i_val = np.zeros((n_unique_dates, ))\n",
    "    for ind, item in enumerate(dates_array):\n",
    "        p_i_val[item] += p_array[ind]\n",
    "\n",
    "    t = np.sum(p_i_val) / np.sqrt(np.sum(p_i_val ** 2)) * (np.sqrt(250 / n_unique_dates))\n",
    "    return min(max(t, 0), 6) * np.sum(p_i_val)\n",
    "\n",
    "\n",
    "@njit\n",
    "def njit_search_best_thresold_custom(y_pred_proba=None,\n",
    "                                     dates_array=None,\n",
    "                                     weights_array=None,\n",
    "                                     resp_array=None):\n",
    "    \"\"\"通过阈值搜索最优的kaggle官方评分的切分阈值.\"\"\"\n",
    "    best_acc, best_threshold = 0, 0\n",
    "    for threshold in range(4500, 5800):\n",
    "        thresold_tmp = threshold / 10000\n",
    "        y_pred_label = np.where(y_pred_proba > thresold_tmp, 1, 0)\n",
    "        score_tmp = njit_custom_metric(dates_array=dates_array,\n",
    "                                       weights_array=weights_array,\n",
    "                                       resp_array=resp_array,\n",
    "                                       action_label_array=y_pred_label)\n",
    "\n",
    "        if score_tmp > best_acc:\n",
    "            best_acc = score_tmp\n",
    "            best_threshold = thresold_tmp\n",
    "    return best_acc, best_threshold\n",
    "\n",
    "\n",
    "print(\"[INFO] {} Tools prepared !\".format(\n",
    "    str(datetime.now())[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] 2021-01-07 20:11:51.62 MLP Model prepared !\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "模型构建与模型训练部分。\n",
    "\"\"\"\n",
    "def build_model(verbose=False, is_compile=True, encoder_model=None, **kwargs):\n",
    "    \"\"\"针对二分类任务的MLP模型，使用自编码器的编码层作为预训练层。\"\"\"\n",
    "    input_dim = kwargs.pop(\"input_dim\", None)\n",
    "    output_dim = kwargs.pop(\"output_dim\", None)\n",
    "    n_labels = kwargs.pop(\"n_labels\", None)\n",
    "\n",
    "    # 构造网络结构\n",
    "    layer_input = layers.Input(input_dim, dtype='float32')\n",
    "    layer_encoded = encoder_model(layer_input)\n",
    "\n",
    "    layer_feats = layers.concatenate([layer_input, layer_encoded])\n",
    "    layer_feats = layers.BatchNormalization()(layer_feats)\n",
    "\n",
    "    # 特征抽取\n",
    "    layer_dense = layers.Dense(64, activation=\"relu\")(layer_feats)\n",
    "    layer_dense = layers.BatchNormalization()(layer_dense)\n",
    "    layer_dense = layers.Dropout(0.2)(layer_dense)\n",
    "\n",
    "    layer_dense = layers.Dense(32, activation=\"relu\")(layer_dense)\n",
    "    layer_dense = layers.BatchNormalization()(layer_dense)\n",
    "    layer_dense = layers.Dropout(0.2)(layer_dense)\n",
    "\n",
    "    # 输出层构造与模型构造\n",
    "    layer_output = layers.Dense(output_dim, activation='sigmoid', name=\"label_output\")(layer_dense)\n",
    "    model = models.Model(layer_input, layer_output)\n",
    "\n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    if is_compile:\n",
    "        model.compile(loss={'label_output':'binary_crossentropy'},\n",
    "                      metrics=[tf.keras.metrics.AUC(name='auc')],\n",
    "                      optimizer=optimizers.Adam(0.003))\n",
    "    return model\n",
    "\n",
    "def test_build_model():\n",
    "    # 构造mlp模型\n",
    "    mlp_model = build_model(verbose=False, encoder_model=encoder_model,\n",
    "                            input_dim=X.shape[1], output_dim=y.shape[1])\n",
    "\n",
    "print(\"[INFO] {} MLP Model prepared !\".format(\n",
    "    str(datetime.now())[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] 2021-01-07 20:27:36.68 Model training start:\n",
      "=========================================\n",
      "-- folds 0(2)(train_range: 86->203, valid_range: 224->361), valid_acc: 0.5106, valid_roc_auc: 0.5128, valid_custom: 1530.1200\n",
      "-- folds 1(2)(train_range: 86->341, valid_range: 362->499), valid_acc: 0.5156, valid_roc_auc: 0.5158, valid_custom: 1586.8889\n",
      "-- total metric, valid_acc: 0.5131, valid_roc_auc: 0.5143, valid_custom: 1558.5044\n",
      "=========================================\n",
      "[INFO] 2021-01-07 20:31:16.19 Model training end.\n"
     ]
    }
   ],
   "source": [
    "# 训练前全局参数准备\n",
    "N_SPLITS = 2\n",
    "MODELS = []\n",
    "early_stop = callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\",\n",
    "                                     verbose=0, patience=15,\n",
    "                                     restore_best_weights=True)\n",
    "group_ts_kfolds = PurgedGroupTimeSeriesSplit(\n",
    "        n_splits=N_SPLITS, group_gap=20)\n",
    "min_date = min(train_dates)\n",
    "train_dates_offset = train_dates - min_date\n",
    "\n",
    "# 开始训练模型\n",
    "valid_acc_total, valid_roc_auc_total, valid_custom_total = [], [], []\n",
    "\n",
    "print(\"[INFO] {} Model training start:\".format(str(datetime.now())[:-4]))\n",
    "print(\"=========================================\")\n",
    "for fold, (train_idx, valid_idx) in enumerate(group_ts_kfolds.split(X=X, y=y, groups=train_dates_offset)):\n",
    "\n",
    "    #####################################################\n",
    "    # Cross validation的数据准备\n",
    "    X_train, X_val = X[train_idx], X[valid_idx]\n",
    "    y_train, y_val = y[train_idx], y[valid_idx]\n",
    "\n",
    "    X_train_weight, X_val_weight = train_weights[train_idx], train_weights[valid_idx]\n",
    "    X_train_resp, X_val_resp = train_resp[train_idx], train_resp[valid_idx]\n",
    "    X_train_dates, X_val_dates = train_dates_offset[train_idx], train_dates_offset[valid_idx]\n",
    "\n",
    "    #####################################################\n",
    "    # STEP 1: 进行特征工程\n",
    "    std_scaler, pca_transformer, X_train_pca = feat_pca(feat_array=X_train, n_dims=30)\n",
    "\n",
    "    X_val_pca = std_scaler.transform(X_val)\n",
    "    X_val_pca = pca_transformer.transform(X_val)\n",
    "\n",
    "    X_train = np.hstack([X_train, X_train_pca])\n",
    "    X_val = np.hstack([X_val, X_val_pca])\n",
    "\n",
    "    #####################################################\n",
    "    # STEP 2: 在训练数据上进行预训练\n",
    "    encoder_model, pretrain_model = build_tabular_autoencoder(\n",
    "            input_dim=X_train.shape[1], n_labels=y_train.shape[1],\n",
    "            stddev=0.06, verbose=False)\n",
    "    pretrain_model.fit(x=X_train, y=[X_train, y_train],\n",
    "                       batch_size=32768,\n",
    "                       epochs=5,\n",
    "                       verbose=0)\n",
    "    encoder_model.trainable = False\n",
    "\n",
    "    #####################################################\n",
    "    # STEP 3: 在训练数据上进行训练\n",
    "    mlp_model = build_model(verbose=False, encoder_model=encoder_model,\n",
    "                            input_dim=X_train.shape[1], output_dim=y_train.shape[1])\n",
    "\n",
    "    mlp_model.fit(x=X_train, y=y_train,\n",
    "                  validation_data=(X_val, y_val),\n",
    "                  batch_size=32768,\n",
    "                  epochs=5,\n",
    "                  verbose=0,\n",
    "                  callbacks=[early_stop])\n",
    "\n",
    "    #####################################################\n",
    "    # STEP 4: 寻找最优valid的阈值\n",
    "    valid_pred_proba = np.mean(mlp_model.predict(X_val), axis=1)\n",
    "    best_custom, THRESHOLD = njit_search_best_thresold_custom(\n",
    "            y_pred_proba=valid_pred_proba,\n",
    "            dates_array=X_val_dates,\n",
    "            weights_array=X_val_weight,\n",
    "            resp_array=X_val_resp)\n",
    "\n",
    "    #####################################################\n",
    "    # STEP 5: valid data上按照官方metric进行结果评估\n",
    "    valid_pred_label = np.where(\n",
    "            valid_pred_proba>=THRESHOLD, 1, 0).astype(int)\n",
    "    valid_custom_metric = custom_metric(dates_array=X_val_dates,\n",
    "                                        weights_array=X_val_weight,\n",
    "                                        action_label_array=valid_pred_label,\n",
    "                                        resp_array=X_val_resp)\n",
    "    valid_acc = accuracy_score(y_val[:, 0].reshape(-1, 1),\n",
    "                               valid_pred_label.reshape(-1, 1))\n",
    "    valid_roc_auc = roc_auc_score(y_val[:, 0].reshape(-1, 1),\n",
    "                                  valid_pred_label.reshape(-1, 1))\n",
    "\n",
    "    # 标准打印训练信息\n",
    "    print(\"-- folds {}({})(train_range: {}->{}, valid_range: {}->{}), valid_acc: {:.4f}, valid_roc_auc: {:.4f}, valid_custom: {:.4f}\".format(\n",
    "            fold+1, N_SPLITS, min(X_train_dates)+min_date, max(X_train_dates)+min_date, min(X_val_dates)+min_date, max(X_val_dates)+min_date, valid_acc, valid_roc_auc, valid_custom_metric))\n",
    "\n",
    "    #####################################################\n",
    "    # STEP 4: 保存模型与关键训练指标\n",
    "    MODELS.append([THRESHOLD, std_scaler, pca_transformer, mlp_model])\n",
    "    valid_acc_total.append(valid_acc)\n",
    "    valid_roc_auc_total.append(valid_roc_auc)\n",
    "    valid_custom_total.append(valid_custom_metric)\n",
    "\n",
    "    # 强制内存回收\n",
    "    del X_train, X_val, y_train, y_val, X_train_pca, X_val_pca\n",
    "    del X_train_weight, X_val_weight, X_train_resp, X_val_resp, X_train_dates, X_val_dates\n",
    "    gc.collect()\n",
    "\n",
    "# 打印总体分数指标\n",
    "print(\"-- total metric, valid_acc: {:.4f}, valid_roc_auc: {:.4f}, valid_custom: {:.4f}\".format(\n",
    "        np.mean(valid_acc_total), np.mean(valid_roc_auc_total), np.mean(valid_custom_total)))\n",
    "\n",
    "print(\"=========================================\")\n",
    "print(\"[INFO] {} Model training end.\".format(str(datetime.now())[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 是否转换Tensorflow Lite模型来加快推理速度\n",
    "IS_TRANSFORM = True\n",
    "\n",
    "if IS_TRANSFORM:\n",
    "    for i in range(len(MODELS)):\n",
    "        tflite_model = LiteModel.from_keras_model(MODELS[i][-1])\n",
    "        MODELS[i][-1] = tflite_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "15219it [00:14, 1021.11it/s]/n\n",
      "0.4959667147830517 0.5040332852169482\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "预测标签按CV结果依据概率加权策略：根据CV结果，以CV的custom metric的结果对预测概率进行加权。\n",
    "\"\"\"\n",
    "# 预备提交预测结果\n",
    "IS_PREDICT = True\n",
    "\n",
    "count_0, count_1 = 0, 0\n",
    "if IS_PREDICT:\n",
    "    # 初始化环境\n",
    "    env = gen_test_data(example_test_df, example_prediction_df)\n",
    "    valid_custom_total_norm = np.array(valid_custom_total) / np.sum(valid_custom_total)\n",
    "\n",
    "    # 预测\n",
    "    THRESHOLD = 0.498\n",
    "    for (test_df, pred_df) in tqdm(env):\n",
    "        if test_df['weight'].item() > 0:\n",
    "            x_test_val = test_df[feature_name_list].values\n",
    "            x_test_val = njit_fillna(x_test_val, mean_val_array).reshape(1, -1)\n",
    "\n",
    "            # 利用MODELS里训练好的神经网络进行训练\n",
    "            pred_proba_list = []\n",
    "            for i in range(len(MODELS)):\n",
    "                x_test_val_pca = MODELS[i][1].transform(x_test_val)\n",
    "                x_test_val_pca = MODELS[i][2].transform(x_test_val_pca)\n",
    "                x_test_val_pca = np.hstack([x_test_val, x_test_val_pca])\n",
    "\n",
    "                pred_proba = MODELS[i][3].predict(x_test_val_pca)\n",
    "                pred_proba_list.append(np.mean(pred_proba))\n",
    "\n",
    "            pred = np.average(pred_proba_list, weights=valid_custom_total_norm)\n",
    "            pred_label = int(np.where(pred >= THRESHOLD, 1, 0))\n",
    "\n",
    "            if pred_label == 0:\n",
    "                count_0 += 1\n",
    "            else:\n",
    "                count_1 += 1\n",
    "\n",
    "            pred_df.action = pred_label\n",
    "        else:\n",
    "            pred_df.action = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "预测标签众数生成策略：选择预测的标签的众数作为样本标签。\n",
    "\"\"\"\n",
    "# 预备提交预测结果\n",
    "IS_PREDICT = False\n",
    "\n",
    "if IS_PREDICT:\n",
    "    # 初始化环境\n",
    "    env = gen_test_data(example_test_df, example_prediction_df)\n",
    "\n",
    "    # 预测\n",
    "    for (test_df, pred_df) in tqdm(env):\n",
    "        if test_df['weight'].item() > 0:\n",
    "            x_test_val = test_df[feature_name_list].values\n",
    "            x_test_val = njit_fillna(x_test_val, mean_val_array).reshape(1, -1)\n",
    "\n",
    "            # 利用MODELS里训练好的神经网络进行训练\n",
    "            pred_label_list = []\n",
    "            for i in range(len(MODELS)):\n",
    "                x_test_val_pca = MODELS[i][1].transform(x_test_val)\n",
    "                x_test_val_pca = MODELS[i][2].transform(x_test_val_pca)\n",
    "                x_test_val_pca = np.hstack([x_test_val, x_test_val_pca])\n",
    "\n",
    "                pred_proba = MODELS[i][3].predict(x_test_val_pca)\n",
    "                pred = np.mean(pred_proba)\n",
    "                pred_label_list.append(int(np.where(pred >= MODELS[i][0], 1, 0)))\n",
    "\n",
    "            # 选取预测最频繁的样本作为标签\n",
    "            # https://stackoverflow.com/questions/6252280/find-the-most-frequent-number-in-a-numpy-array\n",
    "            pred_df.action = max(map(lambda val: (pred_label_list.count(val), val), set(pred_label_list)))[1]\n",
    "        else:\n",
    "            pred_df.action = 0"
   ]
  }
 ]
}