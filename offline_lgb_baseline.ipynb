{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "from numba import njit, jit\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import iinfo, finfo, int8, int16, int32, int64, float32, float64\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from IPython.display import HTML\n",
    "\n",
    "# 设置plotly为暗黑模式\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "plot_config = dict({'scrollZoom': True, 'displayModeBar': True, 'displaylogo': False})\n",
    "sns.set(style=\"ticks\", font_scale=1.2, palette='deep', color_codes=True)\n",
    "colors = [\"C\" + str(i) for i in range(0, 9+1)]\n",
    "\n",
    "# 默认plotly色号\n",
    "default_color_list = [\n",
    "    '#1f77b4',  # muted blue\n",
    "    '#ff7f0e',  # safety orange\n",
    "    '#2ca02c',  # cooked asparagus green\n",
    "    '#d62728',  # brick red\n",
    "    '#9467bd',  # muted purple\n",
    "    '#8c564b',  # chestnut brown\n",
    "    '#e377c2',  # raspberry yogurt pink\n",
    "    '#7f7f7f',  # middle gray\n",
    "    '#bcbd22',  # curry yellow-green\n",
    "    '#17becf'   # blue-teal\n",
    "    ]\n",
    "\n",
    "# 设定全局随机种子，并且屏蔽warnings\n",
    "GLOBAL_RANDOM_SEED = 2022\n",
    "np.random.seed(GLOBAL_RANDOM_SEED)\n",
    "tf.random.set_seed(GLOBAL_RANDOM_SEED)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "# 检查GPU设备情况\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # 限制只能使用第一块GPU（通过GPU的List的id指定）\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] 2021-01-14 13:38:48.55 End Reading ! It took 67.97 seconds !\n[INFO] 2021-01-14 13:38:48.55 Basic data description: \n    -- train_df shape: (2390491, 138)\n    -- example_test_df shape: (15219, 133)\n    -- feat_df shape: (130, 30)\n    -- example_prediction_df shape: (15219, 2)\n"
     ]
    }
   ],
   "source": [
    "# 导入数据\n",
    "load_data_start_time = time.time()\n",
    "train_df  = pd.read_csv(\n",
    "    './data/jane-street-market-prediction/train.csv', nrows=None)\n",
    "feat_df = pd.read_csv(\n",
    "    './data/jane-street-market-prediction/features.csv')\n",
    "example_test_df = pd.read_csv(\n",
    "    './data/jane-street-market-prediction/example_test.csv')\n",
    "example_prediction_df = pd.read_csv(\n",
    "    './data/jane-street-market-prediction/example_sample_submission.csv')\n",
    "load_data_end_time = time.time()\n",
    "\n",
    "# 打印数据基本情况\n",
    "print(\"[INFO] {} End Reading ! It took {:.2f} seconds !\".format(\n",
    "    str(datetime.now())[:-4], load_data_end_time-load_data_start_time))\n",
    "print(\"[INFO] {} Basic data description: \".format(str(datetime.now())[:-4]))\n",
    "print(\"    -- train_df shape: {}\".format(\n",
    "    train_df.shape))\n",
    "print(\"    -- example_test_df shape: {}\".format(\n",
    "    example_test_df.shape))\n",
    "print(\"    -- feat_df shape: {}\".format(\n",
    "    feat_df.shape))\n",
    "print(\"    -- example_prediction_df shape: {}\".format(\n",
    "    example_prediction_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_test_data(test_df=None, pred_df=None):\n",
    "    \"\"\"测试数据生成器。用于模拟测试数据生成过程，测试模型提交正确性与效率。\"\"\"\n",
    "    n_test = len(test_df)\n",
    "\n",
    "    for i in range(n_test):\n",
    "        yield test_df.iloc[i], pred_df.iloc[i]\n",
    "\n",
    "\n",
    "@jit\n",
    "def njit_fillna(array, values):\n",
    "    \"\"\"利用即时编译（jit）对array数组的NaN值借助values进行填充。\n",
    "\n",
    "    @References:\n",
    "    ----------\n",
    "    [1] https://www.kaggle.com/gogo827jz/optimise-speed-of-filling-nan-function\n",
    "    \"\"\"\n",
    "    if np.isnan(array.sum()):\n",
    "        array = np.where(np.isnan(array), values, array)\n",
    "    return array\n",
    "\n",
    "\n",
    "def custom_metric(dates_array=None,\n",
    "                  weights_array=None,\n",
    "                  resp_array=None,\n",
    "                  action_label_array=None):\n",
    "    \"\"\"依据官方要求的Metric，计算分数。\n",
    "\n",
    "    @References:\n",
    "    ----------\n",
    "    [1] https://www.kaggle.com/c/jane-street-market-prediction/discussion/199107\n",
    "    [2] https://www.kaggle.com/c/jane-street-market-prediction/overview/evaluation\n",
    "    [3] \n",
    "    \"\"\"\n",
    "    tmp_df = pd.DataFrame({\"date\": dates_array,\n",
    "                           \"weight\": weights_array,\n",
    "                           \"resp\": resp_array,\n",
    "                           \"action\": action_label_array})\n",
    "    tmp_df[\"p\"] = tmp_df[\"weight\"]  * tmp_df[\"resp\"] * tmp_df[\"action\"]\n",
    "    # tmp_df = tmp_df.query(\"weight != 0\").reset_index(drop=True)\n",
    "    p_i_val = tmp_df.groupby([\"date\"])[\"p\"].sum().values\n",
    "\n",
    "    n_dates = len(p_i_val)\n",
    "    t = np.sum(p_i_val) / np.sqrt(np.sum(p_i_val ** 2)) * (np.sqrt(250 / n_dates))\n",
    "    return min(max(t, 0), 6) * np.sum(p_i_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] 2021-01-14 13:38:51.41 Data prepared !\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "数据预处理部分。包括标签生成、数据统计值获取。\n",
    "\"\"\"\n",
    "# 挑选策略变化之后的数据\n",
    "train = train_df.query('date > 85').reset_index(drop=True)\n",
    "train = train.query('weight > 0').reset_index(drop = True)\n",
    "\n",
    "target_threshold = 0.00001\n",
    "\n",
    "# 构造标签\n",
    "train['action'] =  ((train['resp_1'] > target_threshold) & \\\n",
    "                    (train['resp_2'] > target_threshold) & \\\n",
    "                    (train['resp_3'] > target_threshold) & \\\n",
    "                    (train['resp_4'] > target_threshold) &  \\\n",
    "                    (train['resp'] > target_threshold)).astype('int')\n",
    "feature_name_list = [c for c in train.columns if 'feature' in c]\n",
    "resp_name_list = [\"resp\", \"resp_1\", \"resp_2\", \"resp_3\", \"resp_4\"]\n",
    "\n",
    "# 使用均值填充缺失值\n",
    "mean_val_list = []\n",
    "for name in feature_name_list:\n",
    "    mean_val = train[name].mean()\n",
    "    train[name].fillna(mean_val, inplace=True)\n",
    "    mean_val_list.append(mean_val)\n",
    "mean_val_array = np.array(mean_val_list)\n",
    "\n",
    "# 构造后续神经网络模型的输入输出\n",
    "X = train[feature_name_list].values.astype(\"float32\")\n",
    "y = np.hstack([(train[c] > target_threshold).astype(\n",
    "    'int').values.reshape(-1, 1) for c in resp_name_list])\n",
    "\n",
    "train_dates = train[\"date\"].values\n",
    "train_weights = train[\"weight\"].values\n",
    "train_resp = train[\"resp\"].values\n",
    "\n",
    "print(\"[INFO] {} Data prepared !\".format(\n",
    "    str(datetime.now())[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"针对带有Group id（组id）数据的时间序列交叉验证集合生成类。\n",
    "\n",
    "    生成针对带有Group id的数据的时序交叉验证集。其中训练与验证的\n",
    "    Group之间可以指定group_gap，用来隔离时间上的关系。这种情况下\n",
    "    group_id通常是时间id，例如天或者小时。\n",
    "\n",
    "    @Parameters:\n",
    "    ----------\n",
    "        n_splits: {int-like}, default=5\n",
    "            切分的集合数目。\n",
    "        max_train_group_size: {int-like}, default=+inf\n",
    "            训练集单个组的最大样本数据限制。\n",
    "        group_gap: {int-like}, default=None\n",
    "            依据group_id切分组时，训练组与测试组的id的gap数目。\n",
    "        max_test_group_size: {int-like}, default=+inf\n",
    "            测试集单个组的最大样本数据限制。\n",
    "\n",
    "    @References:\n",
    "    ----------\n",
    "    [1] https://www.kaggle.com/gogo827jz/jane-street-ffill-xgboost-purgedtimeseriescv\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self, n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"生成训练组与测试组的id索引，返回组索引的生成器。\n",
    "\n",
    "        @Parameters:\n",
    "        ----------\n",
    "            X: {array-like} {n_samples, n_features}\n",
    "                训练数据，输入形状为{n_samples, n_features}。\n",
    "            y: {array-like} {n_samples, }\n",
    "                标签数据，形状为{n_samples, }。\n",
    "            groups: {array-like} {n_samples, }\n",
    "                用来依据组来划分训练集与测试集的组id，必须为连续的组id。\n",
    "\n",
    "        @Yields:\n",
    "        ----------\n",
    "            train: ndarray\n",
    "                依据group_id切分的训练组id。\n",
    "            test: ndarray\n",
    "                依据group_id切分的测试组id。\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None ！\")\n",
    "        for i in range(1, len(groups)):\n",
    "            if groups[i] < groups[i-1]:\n",
    "                raise ValueError(\"groups must be a monotone increasing sequence !\")\n",
    "\n",
    "        # 初始化基本参数信息\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples, n_splits, group_gap = X.shape[0], self.n_splits, self.group_gap\n",
    "        n_folds = n_splits + 1\n",
    "\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "\n",
    "        # 使得groups的id取值从0顺序开始（假定groups是递增的）\n",
    "        groups_reid, groupid2reid, index_tmp = [], {}, -1\n",
    "        for _, item in enumerate(groups):\n",
    "            if item not in groupid2reid:\n",
    "                index_tmp += 1\n",
    "                groupid2reid[item] = index_tmp\n",
    "            groups_reid.append(index_tmp)\n",
    "\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups_reid, return_index=True)\n",
    "        unique_groups = np.argsort(ind)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "\n",
    "        # 扫描整个数据id list，构建group_dcit，{group_id: 属于该group的样本的idx}\n",
    "        for idx in np.arange(n_samples):\n",
    "            if groups_reid[idx] in group_dict:\n",
    "                group_dict[groups_reid[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups_reid[idx]] = [idx]\n",
    "\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds, n_groups))\n",
    "\n",
    "        # group_test_size: 每个fold预留的test group的大小\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array, test_array = [], []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                train_array = np.sort(np.unique(\n",
    "                    np.concatenate((train_array, train_array_tmp)),\n",
    "                    axis=None), axis=None)\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                    np.concatenate((test_array, test_array_tmp)),\n",
    "                    axis=None), axis=None)\n",
    "            test_array  = test_array[group_gap:]\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]\n",
    "\n",
    "\n",
    "def test_purged_group_time_series_split():\n",
    "    X = train_df.query('date > 85').reset_index(drop=True)[[\"ts_id\", \"feature_0\"]].values\n",
    "    y = train_df.query('date > 85').reset_index(drop=True)[\"resp\"].values\n",
    "    groups = train_df.query('date > 85').reset_index(drop=True)[\"date\"].values\n",
    "\n",
    "    group_ts_kfolds = PurgedGroupTimeSeriesSplit(\n",
    "        n_splits=7, group_gap=20, max_train_group_size=80, max_test_group_size=60)\n",
    "    train_idx, valid_idx = [], []\n",
    "    for train_idx_tmp, valid_idx_tmp in group_ts_kfolds.split(X=X, y=y, groups=groups):\n",
    "        train_idx.append(train_idx_tmp)\n",
    "        valid_idx.append(valid_idx_tmp)\n",
    "\n",
    "        print(\"train range: {}, valid range: {}\".format(\n",
    "            [min(groups[train_idx_tmp]), max(groups[train_idx_tmp])],\n",
    "            [min(groups[valid_idx_tmp]), max(groups[valid_idx_tmp])]\n",
    "        ))\n",
    "\n",
    "# test_purged_group_time_series_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] 2021-01-14 13:38:51.52 Tools prepared !\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "特征工程辅助工具。\n",
    "\"\"\"\n",
    "\n",
    "def feat_pca(feat_array=None, n_dims=30):\n",
    "    \"\"\"利用PCA，将feat_array降维至n_dims维度。\"\"\"\n",
    "    if feat_array.shape[1] <= n_dims:\n",
    "        raise ValueError(\"n_dims must smaller than the dim of feat_array !\")\n",
    "\n",
    "    # 归一化feat_array\n",
    "    X_sc = StandardScaler()\n",
    "    X_sc.fit(feat_array)\n",
    "    feat_array = X_sc.transform(feat_array)\n",
    "\n",
    "    # 降维\n",
    "    pca = PCA(n_components=n_dims)\n",
    "    pca.fit(feat_array)\n",
    "    feat_array_pca = pca.transform(feat_array)\n",
    "\n",
    "    return X_sc, pca, feat_array_pca\n",
    "\n",
    "\n",
    "@njit\n",
    "def njit_search_best_thresold_acc(y_pred_proba, y_true):\n",
    "    \"\"\"通过阈值搜索最优的准确率切分阈值.\"\"\"\n",
    "    best_acc, best_threshold = 0, 0\n",
    "    for threshold in range(4500, 5800):\n",
    "        thresold_tmp = threshold / 10000\n",
    "        y_pred_label = np.where(y_pred_proba > thresold_tmp, 1, 0)\n",
    "        score_tmp = np.sum(np.where(y_true == y_pred_label, 1, 0)) / len(y_true)\n",
    "\n",
    "        if score_tmp > best_acc:\n",
    "            best_acc = score_tmp\n",
    "            best_threshold = thresold_tmp\n",
    "    return best_acc, best_threshold\n",
    "\n",
    "\n",
    "@njit\n",
    "def njit_custom_metric(dates_array=None,\n",
    "                       weights_array=None,\n",
    "                       resp_array=None,\n",
    "                       action_label_array=None):\n",
    "    \"\"\"利用njit装饰器与numpy来计算Kaggle官方要求的Metric。\n",
    "\n",
    "    @References:\n",
    "    ----------\n",
    "    [1] https://www.kaggle.com/c/jane-street-market-prediction/discussion/199107\n",
    "    [2] https://www.kaggle.com/c/jane-street-market-prediction/overview/evaluation\n",
    "    \"\"\"\n",
    "    p_array = weights_array * resp_array * action_label_array\n",
    "\n",
    "    n_unique_dates = np.max(dates_array) - np.min(dates_array) + 1\n",
    "    dates_array = dates_array - np.min(dates_array)\n",
    "\n",
    "    p_i_val = np.zeros((n_unique_dates, ))\n",
    "    for ind, item in enumerate(dates_array):\n",
    "        p_i_val[item] += p_array[ind]\n",
    "\n",
    "    t = np.sum(p_i_val) / np.sqrt(np.sum(p_i_val ** 2)) * (np.sqrt(250 / n_unique_dates))\n",
    "    return min(max(t, 0), 6) * np.sum(p_i_val)\n",
    "\n",
    "\n",
    "@njit\n",
    "def njit_search_best_thresold_custom(y_pred_proba=None,\n",
    "                                     dates_array=None,\n",
    "                                     weights_array=None,\n",
    "                                     resp_array=None):\n",
    "    \"\"\"通过阈值搜索最优的kaggle官方评分的切分阈值.\"\"\"\n",
    "    best_acc, best_threshold = 0, 0\n",
    "    for threshold in range(4500, 5800):\n",
    "        thresold_tmp = threshold / 10000\n",
    "        y_pred_label = np.where(y_pred_proba > thresold_tmp, 1, 0)\n",
    "        score_tmp = njit_custom_metric(dates_array=dates_array,\n",
    "                                       weights_array=weights_array,\n",
    "                                       resp_array=resp_array,\n",
    "                                       action_label_array=y_pred_label)\n",
    "\n",
    "        if score_tmp > best_acc:\n",
    "            best_acc = score_tmp\n",
    "            best_threshold = thresold_tmp\n",
    "    return best_acc, best_threshold\n",
    "\n",
    "\n",
    "print(\"[INFO] {} Tools prepared !\".format(\n",
    "    str(datetime.now())[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] 2021-01-14 13:38:51.57 Model training start:\n",
      "=========================================\n",
      "-- folds 1(7)(train_range: 86->137, valid_range: 143->193), valid_acc: 0.5169, valid_roc_auc: 0.5155, valid_custom: 1256.9727\n",
      "-- folds 2(7)(train_range: 86->188, valid_range: 194->244), valid_acc: 0.5168, valid_roc_auc: 0.5179, valid_custom: 1000.1100\n",
      "-- folds 3(7)(train_range: 86->239, valid_range: 245->295), valid_acc: 0.5185, valid_roc_auc: 0.5167, valid_custom: 483.0145\n",
      "-- folds 4(7)(train_range: 86->290, valid_range: 296->346), valid_acc: 0.5214, valid_roc_auc: 0.5214, valid_custom: 1223.6603\n",
      "-- folds 5(7)(train_range: 86->341, valid_range: 347->397), valid_acc: 0.5158, valid_roc_auc: 0.5157, valid_custom: 1155.8925\n",
      "-- folds 6(7)(train_range: 86->392, valid_range: 398->448), valid_acc: 0.5189, valid_roc_auc: 0.5120, valid_custom: 307.3136\n",
      "-- folds 7(7)(train_range: 86->443, valid_range: 449->499), valid_acc: 0.5228, valid_roc_auc: 0.5228, valid_custom: 1829.6784\n",
      "-- total metric, valid_acc: 0.5187, valid_roc_auc: 0.5174, valid_custom: 1036.6631\n",
      "=========================================\n",
      "[INFO] 2021-01-14 13:56:28.71 Model training end.\n"
     ]
    }
   ],
   "source": [
    "# 训练前全局参数准备\n",
    "N_SPLITS = 7\n",
    "MODELS = []\n",
    "group_ts_kfolds = PurgedGroupTimeSeriesSplit(\n",
    "        n_splits=N_SPLITS, group_gap=5, max_test_group_size=60)\n",
    "\n",
    "# 开始训练模型\n",
    "valid_acc_total, valid_roc_auc_total, valid_custom_total = [], [], []\n",
    "\n",
    "print(\"[INFO] {} Model training start:\".format(str(datetime.now())[:-4]))\n",
    "print(\"=========================================\")\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(group_ts_kfolds.split(X=X, y=y, groups=train_dates)):\n",
    "    #####################################################\n",
    "    # Cross validation的数据准备\n",
    "    X_train, X_val = X[train_idx], X[valid_idx]\n",
    "    y_train, y_val = y[train_idx], y[valid_idx]\n",
    "\n",
    "    X_train_weight, X_val_weight = train_weights[train_idx], train_weights[valid_idx]\n",
    "    X_train_resp, X_val_resp = train_resp[train_idx], train_resp[valid_idx]\n",
    "    X_train_dates, X_val_dates = train_dates[train_idx], train_dates[valid_idx]\n",
    "\n",
    "    #####################################################\n",
    "    # STEP 1: 进行特征工程\n",
    "    std_scaler, pca_transformer, X_train_pca = feat_pca(feat_array=X_train, n_dims=30)\n",
    "\n",
    "    X_val_pca = std_scaler.transform(X_val)\n",
    "    X_val_pca = pca_transformer.transform(X_val)\n",
    "\n",
    "    X_train = np.hstack([X_train, X_train_pca])\n",
    "    X_val = np.hstack([X_val, X_val_pca])\n",
    "\n",
    "    #####################################################\n",
    "    # STEP 2: 分5次拟合5个lightgbm模型（对应于5个resp值）\n",
    "    xgb_params = {\"n_estimators\": 200,\n",
    "                  \"max_depth\": 6,\n",
    "                  \"learning_rate\": 0.04,\n",
    "                  \"verbosity\": 0,\n",
    "                  \"objective\": \"binary:logistic\",\n",
    "                  \"booster\": \"gbtree\",\n",
    "                  \"colsample_bytree\": 0.95,\n",
    "                  \"colsample_bylevel\": 0.95,\n",
    "                  \"subsample\": 0.95,\n",
    "                  \"gpu_id\": 0,\n",
    "                  \"random_state\": np.random.randint(0, 10000, 1)[0],\n",
    "                  \"tree_method\": \"gpu_hist\"}\n",
    "\n",
    "    valid_pred_proba_list, models_tmp = [], []\n",
    "    for i in range(y_val.shape[1]):\n",
    "        xgb_clf = xgb.XGBClassifier(**xgb_params)\n",
    "        xgb_clf.fit(X_train, y_train[:, i], eval_set=[(X_val, y_val[:, i])],\n",
    "            early_stopping_rounds=100, verbose=False, eval_metric=\"auc\")\n",
    "\n",
    "        valid_pred_proba_list.append(xgb_clf.predict_proba(\n",
    "            X_val, ntree_limit=xgb_clf.best_iteration))\n",
    "        models_tmp.append([xgb_clf, xgb_clf.best_iteration])\n",
    "\n",
    "    valid_pred_proba = np.mean(valid_pred_proba_list, axis=0)[:, 1]\n",
    "\n",
    "    #####################################################\n",
    "    # STEP 4: 寻找最优valid的阈值\n",
    "    best_custom, THRESHOLD = njit_search_best_thresold_custom(\n",
    "            y_pred_proba=valid_pred_proba,\n",
    "            dates_array=X_val_dates,\n",
    "            weights_array=X_val_weight,\n",
    "            resp_array=X_val_resp)\n",
    "\n",
    "    #####################################################\n",
    "    # STEP 5: valid data上按照官方metric进行结果评估\n",
    "    valid_pred_label = np.where(\n",
    "            valid_pred_proba>=THRESHOLD, 1, 0).astype(int)\n",
    "    valid_custom_metric = custom_metric(dates_array=X_val_dates,\n",
    "                                        weights_array=X_val_weight,\n",
    "                                        action_label_array=valid_pred_label,\n",
    "                                        resp_array=X_val_resp)\n",
    "    valid_acc = accuracy_score(y_val[:, 0].reshape(-1, 1),\n",
    "                               valid_pred_label.reshape(-1, 1))\n",
    "    valid_roc_auc = roc_auc_score(y_val[:, 0].reshape(-1, 1),\n",
    "                                  valid_pred_label.reshape(-1, 1))\n",
    "\n",
    "    # 标准打印训练信息\n",
    "    print(\"-- folds {}({})(train_range: {}->{}, valid_range: {}->{}), valid_acc: {:.4f}, valid_roc_auc: {:.4f}, valid_custom: {:.4f}\".format(\n",
    "            fold+1, N_SPLITS, min(X_train_dates), max(X_train_dates), min(X_val_dates), max(X_val_dates), valid_acc, valid_roc_auc, valid_custom_metric))\n",
    "\n",
    "    #####################################################\n",
    "    # STEP 6: 保存模型与关键训练指标\n",
    "    MODELS.append([THRESHOLD, std_scaler, pca_transformer, models_tmp])\n",
    "    valid_acc_total.append(valid_acc)\n",
    "    valid_roc_auc_total.append(valid_roc_auc)\n",
    "    valid_custom_total.append(valid_custom_metric)\n",
    "\n",
    "    # 强制内存回收\n",
    "    del X_train, X_val, y_train, y_val, X_train_pca, X_val_pca\n",
    "    del X_train_weight, X_val_weight, X_train_resp, X_val_resp, X_train_dates, X_val_dates\n",
    "    gc.collect()\n",
    "\n",
    "    #####################################################\n",
    "    # STEP 7: 更改随机种子，强化fold与fold之间模型的randomness\n",
    "    new_seed = np.random.randint(10000, 20000, 1)[0]\n",
    "    np.random.seed(new_seed)\n",
    "    tf.random.set_seed(new_seed)\n",
    "\n",
    "# 打印总体分数指标\n",
    "print(\"-- total metric, valid_acc: {:.4f}, valid_roc_auc: {:.4f}, valid_custom: {:.4f}\".format(\n",
    "        np.mean(valid_acc_total), np.mean(valid_roc_auc_total), np.mean(valid_custom_total)))\n",
    "\n",
    "print(\"=========================================\")\n",
    "print(\"[INFO] {} Model training end.\".format(str(datetime.now())[:-4]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "50it [00:03, 13.46it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-622b39460e81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m                     \u001b[0mMODELS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"approx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     pred_proba = MODELS[i][-1][j][0].predict_proba(\n\u001b[0;32m---> 31\u001b[0;31m                         x_test_val_pca, ntree_limit=MODELS[i][-1][j][1])\n\u001b[0m\u001b[1;32m     32\u001b[0m                     \u001b[0mpred_proba_per_fold_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_proba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mpred_proba_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_proba_per_fold_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, data, ntree_limit, validate_features, base_margin)\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \"\"\"\n\u001b[1;32m   1025\u001b[0m         test_dmatrix = DMatrix(data, base_margin=base_margin,\n\u001b[0;32m-> 1026\u001b[0;31m                                missing=self.missing, nthread=self.n_jobs)\n\u001b[0m\u001b[1;32m   1027\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mntree_limit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m             \u001b[0mntree_limit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"best_ntree_limit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, enable_categorical)\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0mfeature_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m             enable_categorical=enable_categorical)\n\u001b[0m\u001b[1;32m    506\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/data.py\u001b[0m in \u001b[0;36mdispatch_data_backend\u001b[0;34m(data, missing, threads, feature_names, feature_types, enable_categorical)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_numpy_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         return _from_numpy_array(data, missing, threads, feature_names,\n\u001b[0;32m--> 531\u001b[0;31m                                  feature_types)\n\u001b[0m\u001b[1;32m    532\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_from_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/data.py\u001b[0m in \u001b[0;36m_from_numpy_array\u001b[0;34m(data, missing, nthread, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         ctypes.c_int(nthread)))\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "预测标签按CV结果依据概率加权策略：根据CV结果，以CV的custom metric的结果对预测概率进行加权。\n",
    "\"\"\"\n",
    "# 预备提交预测结果\n",
    "count_0, count_1 = 0, 0\n",
    "\n",
    "if True:\n",
    "    # 初始化环境\n",
    "    env = gen_test_data(example_test_df, example_prediction_df)\n",
    "    valid_custom_total_norm = np.array(valid_custom_total) / np.sum(valid_custom_total)\n",
    "\n",
    "    # 开始预测\n",
    "    THRESHOLD = 0.512\n",
    "    for (test_df, pred_df) in tqdm(env):\n",
    "        if test_df['weight'].item() > 0:\n",
    "            x_test_val = test_df[feature_name_list].values\n",
    "            x_test_val = njit_fillna(x_test_val, mean_val_array).reshape(1, -1)\n",
    "\n",
    "            # 利用MODELS里训练好的xgboost进行训练\n",
    "            pred_proba_list = []\n",
    "            for i in range(len(MODELS)):\n",
    "\n",
    "                pred_proba_per_fold_list = []\n",
    "                for j in range(len(MODELS[i][-1])):\n",
    "                    x_test_val_pca = MODELS[i][1].transform(x_test_val)\n",
    "                    x_test_val_pca = MODELS[i][2].transform(x_test_val_pca)\n",
    "                    x_test_val_pca = np.hstack([x_test_val, x_test_val_pca])\n",
    "\n",
    "                    pred_proba = MODELS[i][-1][j][0].predict_proba(\n",
    "                        x_test_val_pca, ntree_limit=MODELS[i][-1][j][1])\n",
    "                    pred_proba_per_fold_list.append(pred_proba)\n",
    "                pred_proba_list.append(np.mean(pred_proba_per_fold_list, axis=0).ravel()[1])\n",
    "\n",
    "            pred = np.average(pred_proba_list, weights=valid_custom_total_norm)\n",
    "            pred_label = int(np.where(pred >= THRESHOLD, 1, 0))\n",
    "\n",
    "            if pred_label == 0:\n",
    "                count_0 += 1\n",
    "            else:\n",
    "                count_1 += 1\n",
    "\n",
    "            pred_df.action = pred_label\n",
    "        else:\n",
    "            pred_df.action = 0\n",
    "\n",
    "    print(count_0/(count_0+count_1), count_1/(count_0+count_1))\n",
    "    print(np.sum(y[:, 0] == 0)/(np.sum(y[:, 0] == 0) + np.sum(y[:, 0] == 1)),\n",
    "        np.sum(y[:, 0] == 1)/(np.sum(y[:, 0] == 0) + np.sum(y[:, 0] == 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<bound method XGBModel.set_params of XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=0.95,\n",
       "              colsample_bynode=1, colsample_bytree=0.95, gamma=0, gpu_id=0,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.04, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=200, n_jobs=12, num_parallel_tree=1,\n",
       "              random_state=5550, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "              subsample=0.95, tree_method='gpu_hist', validate_parameters=1,\n",
       "              verbosity=0)>"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "MODELS[i][-1][j][0].set_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}